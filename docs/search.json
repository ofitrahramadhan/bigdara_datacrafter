[
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "",
    "section": "",
    "text": "Use this repository to host a website for your CASA0025 final project by following these stpes:\n\nclone this repository\ninstall quarto\nedit the ‘index.qmd’ file with the contents of your project\nusing terminal, navigate to the project directory and run “quarto render”\npush the changes to your github repository\non github, navigate to Settings&gt;Pages&gt;Build and Deployment. Make sure that under “Source” it says “deploy from branch”. Under “Branch”, select “Main” in the first dropdown and “Docs” under the second drop down. Then press “Save”\n\nYour website should now be available under https://{your_username}.github.io/{your_repo_name}"
  },
  {
    "objectID": "other/Data_Extraction_Corn_GEE.html",
    "href": "other/Data_Extraction_Corn_GEE.html",
    "title": "",
    "section": "",
    "text": "#pip install earthengine-api\n\nCollecting earthengine-api\n  Downloading earthengine_api-0.1.397-py3-none-any.whl.metadata (1.7 kB)\nCollecting google-cloud-storage (from earthengine-api)\n  Downloading google_cloud_storage-2.16.0-py2.py3-none-any.whl.metadata (6.1 kB)\nCollecting google-api-python-client&gt;=1.12.1 (from earthengine-api)\n  Downloading google_api_python_client-2.125.0-py2.py3-none-any.whl.metadata (6.6 kB)\nCollecting google-auth&gt;=1.4.1 (from earthengine-api)\n  Downloading google_auth-2.29.0-py2.py3-none-any.whl.metadata (4.7 kB)\nCollecting google-auth-httplib2&gt;=0.0.3 (from earthengine-api)\n  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\nCollecting httplib2&lt;1dev,&gt;=0.9.2 (from earthengine-api)\n  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from earthengine-api) (2.31.0)\nCollecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0.dev0,&gt;=1.31.5 (from google-api-python-client&gt;=1.12.1-&gt;earthengine-api)\n  Downloading google_api_core-2.18.0-py3-none-any.whl.metadata (2.7 kB)\nCollecting uritemplate&lt;5,&gt;=3.0.1 (from google-api-python-client&gt;=1.12.1-&gt;earthengine-api)\n  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\nCollecting cachetools&lt;6.0,&gt;=2.0.0 (from google-auth&gt;=1.4.1-&gt;earthengine-api)\n  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\nCollecting pyasn1-modules&gt;=0.2.1 (from google-auth&gt;=1.4.1-&gt;earthengine-api)\n  Downloading pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\nCollecting rsa&lt;5,&gt;=3.1.4 (from google-auth&gt;=1.4.1-&gt;earthengine-api)\n  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,&lt;4,&gt;=2.4.2 in /opt/conda/lib/python3.11/site-packages (from httplib2&lt;1dev,&gt;=0.9.2-&gt;earthengine-api) (3.0.9)\nCollecting google-cloud-core&lt;3.0dev,&gt;=2.3.0 (from google-cloud-storage-&gt;earthengine-api)\n  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\nCollecting google-resumable-media&gt;=2.6.0 (from google-cloud-storage-&gt;earthengine-api)\n  Downloading google_resumable_media-2.7.0-py2.py3-none-any.whl.metadata (2.2 kB)\nCollecting google-crc32c&lt;2.0dev,&gt;=1.0 (from google-cloud-storage-&gt;earthengine-api)\n  Downloading google_crc32c-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/conda/lib/python3.11/site-packages (from requests-&gt;earthengine-api) (3.2.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.11/site-packages (from requests-&gt;earthengine-api) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests-&gt;earthengine-api) (2.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests-&gt;earthengine-api) (2023.7.22)\nCollecting googleapis-common-protos&lt;2.0.dev0,&gt;=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0.dev0,&gt;=1.31.5-&gt;google-api-python-client&gt;=1.12.1-&gt;earthengine-api)\n  Downloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,&lt;5.0.0.dev0,&gt;=3.19.5 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0.dev0,&gt;=1.31.5-&gt;google-api-python-client&gt;=1.12.1-&gt;earthengine-api) (4.23.3)\nCollecting proto-plus&lt;2.0.0dev,&gt;=1.22.3 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,&lt;3.0.0.dev0,&gt;=1.31.5-&gt;google-api-python-client&gt;=1.12.1-&gt;earthengine-api)\n  Downloading proto_plus-1.23.0-py3-none-any.whl.metadata (2.2 kB)\nCollecting pyasn1&lt;0.7.0,&gt;=0.4.6 (from pyasn1-modules&gt;=0.2.1-&gt;google-auth&gt;=1.4.1-&gt;earthengine-api)\n  Downloading pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\nDownloading earthengine_api-0.1.397-py3-none-any.whl (334 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 334.9/334.9 kB 3.6 MB/s eta 0:00:00:00:01\nDownloading google_api_python_client-2.125.0-py2.py3-none-any.whl (12.5 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.5/12.5 MB 10.1 MB/s eta 0:00:0000:0100:01\nDownloading google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 189.2/189.2 kB 1.9 MB/s eta 0:00:00a 0:00:01\nDownloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\nDownloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.9/96.9 kB 1.2 MB/s eta 0:00:00a 0:00:01\nDownloading google_cloud_storage-2.16.0-py2.py3-none-any.whl (125 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125.6/125.6 kB 4.2 MB/s eta 0:00:00\nDownloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\nDownloading google_api_core-2.18.0-py3-none-any.whl (138 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.3/138.3 kB 4.1 MB/s eta 0:00:00\nDownloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\nDownloading google_crc32c-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\nDownloading google_resumable_media-2.7.0-py2.py3-none-any.whl (80 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 kB 2.6 MB/s eta 0:00:00\nDownloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.2/181.2 kB 5.5 MB/s eta 0:00:00ta 0:00:01\nDownloading rsa-4.9-py3-none-any.whl (34 kB)\nDownloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\nDownloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl (229 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.1/229.1 kB 3.2 MB/s eta 0:00:0000:01\nDownloading proto_plus-1.23.0-py3-none-any.whl (48 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.8/48.8 kB 985.2 kB/s eta 0:00:00 0:00:01\nDownloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.3/85.3 kB 1.9 MB/s eta 0:00:00ta 0:00:01\nInstalling collected packages: uritemplate, pyasn1, proto-plus, httplib2, googleapis-common-protos, google-crc32c, cachetools, rsa, pyasn1-modules, google-resumable-media, google-auth, google-auth-httplib2, google-api-core, google-cloud-core, google-api-python-client, google-cloud-storage, earthengine-api\nSuccessfully installed cachetools-5.3.3 earthengine-api-0.1.397 google-api-core-2.18.0 google-api-python-client-2.125.0 google-auth-2.29.0 google-auth-httplib2-0.2.0 google-cloud-core-2.4.1 google-cloud-storage-2.16.0 google-crc32c-1.5.0 google-resumable-media-2.7.0 googleapis-common-protos-1.63.0 httplib2-0.22.0 proto-plus-1.23.0 pyasn1-0.6.0 pyasn1-modules-0.4.0 rsa-4.9 uritemplate-4.1.1\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport ee\n\n\nee.Authenticate() \nee.Initialize()\n\n\ndef processYear(year):\n    # Load the CDL dataset for the given year\n    dataset = ee.ImageCollection('USDA/NASS/CDL')\\\n                .filter(ee.Filter.date(f'{year}-01-01', f'{year}-12-31'))\\\n                .first()\n    crop_landcover = dataset.select('cropland')\n\n    # Filter for North Dakota counties\n    counties = ee.FeatureCollection('TIGER/2016/Counties')\n    nd = counties.filter(ee.Filter.eq('STATEFP', '38'))\n    \n    # Identify corn areas in North Dakota\n    corn = crop_landcover.eq(1).Or(crop_landcover.eq(12)).Or(crop_landcover.eq(13))\n    masked_corn = crop_landcover.updateMask(corn).clipToCollection(nd)\n\n    # Calculate NDVI for corn areas using MODIS data\n    NDVI_dataset = ee.ImageCollection('MODIS/061/MOD13Q1')\\\n                    .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\n    ndvi = NDVI_dataset.select('NDVI')\n    mean_ndvi = ndvi.mean().rename('NDVI')\n    cornNDVI = mean_ndvi.updateMask(masked_corn)\n    \n    # Calculate precipitation using GRIDMET data\n    precipitation_dataset = ee.ImageCollection(\"IDAHO_EPSCOR/GRIDMET\")\\\n                             .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\\\n                             .select('pr')\n    mean_precipitation = precipitation_dataset.mean().rename('PA')\n\n    # Load Sentinel-1 C-band SAR Image Collection for the given year, select VV polarization\n    s1_dataset = ee.ImageCollection(\"COPERNICUS/S1_GRD\")\\\n                   .filter(ee.Filter.eq('instrumentMode', 'IW'))\\\n                   .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\\\n                   .filter(ee.Filter.date(f'{year}-01-01', f'{year}-12-31'))\\\n                   .select('VV')\n    mean_s1_vv = s1_dataset.mean().rename('SAR')\n\n    # Load Radiometer Global Daily 9 km Soil Moisture AM\n    smap_dataset = ee.ImageCollection(\"NASA/SMAP/SPL3SMP_E/005\")\\\n                    .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\\\n                    .select('soil_moisture_am')\n    mean_soil_moisture = smap_dataset.mean().rename('SMS_AM')\n\n    # Load MODIS Land Surface Temperature DAY\n    lstDataset = ee.ImageCollection(\"MODIS/061/MOD11A1\")\\\n                   .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\n \n    lstmean_celsius = lstDataset.select('LST_Day_1km')\\\n                                .mean()\\\n                                .multiply(0.02)\\\n                                .subtract(273.15)\\\n                                .rename('LST_DAY')\n                         \n    # Load Radiometer Global Daily 9 km Soil Moisture PM\n\n    smapDataset_pm = ee.ImageCollection(\"NASA/SMAP/SPL3SMP_E/005\")\\\n                       .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\\\n                       .select('soil_moisture_am') \n    meanSoilMoisture_pm = smapDataset_pm.mean().rename('SMS_PM')\n\n                         \n  \n    # Load MODIS Land Surface Temperature NIGHT\n    lstDataset_night = ee.ImageCollection(\"MODIS/061/MOD11A1\")\\\n                         .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\n \n    lstmean_celsius_night = lstDataset_night.select('LST_Night_1km')\\\n                                              .mean()\\\n                                              .multiply(0.02)\\\n                                              .subtract(273.15)\\\n                                              .rename('LST_NIGHT')\n                         \n    # Photosynthetically Active Radiation Daily 3-Hour \n\n    par_12 = ee.ImageCollection(\"MODIS/061/MCD18C2\")\\\n               .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\\\n               .select('GMT_1200_PAR')\n                        \n    mean_par_12 = par_12.mean().rename('PAR'); # Calculate the Photosynthetically Active Radiation at 12\n\n                         \n    # Net Evapotranspiration  \n    netevapo = ee.ImageCollection(\"MODIS/061/MOD16A2GF\")\\\n                 .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\\\n                 .select('ET')\n                    \n    mean_netevapo = netevapo.mean().rename('ET')  # Calculate the mean Soil Moisture\n\n\n    # Combine all layers\n    combinedDataset = cornNDVI.addBands(mean_precipitation).addBands(mean_s1_vv).addBands(mean_soil_moisture).addBands(lstmean_celsius).addBands(meanSoilMoisture_pm).addBands(lstmean_celsius_night).addBands(mean_par_12).addBands(mean_netevapo)\n\n    # Reduce regions and calculate mean values over the specified areas\n    combined_mean = combinedDataset.reduceRegions(\n        collection=nd,\n        reducer=ee.Reducer.mean(),\n        scale=30,\n        tileScale=4,\n    )\n\n    # Define export parameters\n    export_params = {\n        'collection': combined_mean,\n        'description': f'combined_{year}',\n        'folder': 'GEE_Folder',\n        'fileNamePrefix': f'Combined_{year}',\n        'fileFormat': 'CSV',\n        'selectors': ['NAME', 'GEOID', 'NDVI', 'PA', 'SAR', 'SMS_AM', 'LST_DAY', 'SMS_PM', 'LST_NIGHT', 'PAR', 'ET']\n    }\n\n    # Uncomment the line below to as I have got the data in my drive \n    #ee.batch.Export.table.toDrive(**export_params).start()\n\n# Example of processing each year\nfor year in range(2000, 2024):\n    processYear(year)"
  },
  {
    "objectID": "Modeling/soybean_supervised_learning.html",
    "href": "Modeling/soybean_supervised_learning.html",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "",
    "text": "This jupyter notebook is addressed for creating model that can estimate crop yield in North Dakota, United States. The detail workflow of this project can be seen in here."
  },
  {
    "objectID": "Modeling/soybean_supervised_learning.html#install-load-packages",
    "href": "Modeling/soybean_supervised_learning.html#install-load-packages",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "0. Install & Load Packages",
    "text": "0. Install & Load Packages\n\n#install packages\n!pip install tensorflow\n\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.16.1)\nRequirement already satisfied: absl-py&gt;=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.0)\nRequirement already satisfied: astunparse&gt;=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers&gt;=23.5.26 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,&gt;=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta&gt;=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py&gt;=3.10.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.10.0)\nRequirement already satisfied: libclang&gt;=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.3.2)\nRequirement already satisfied: opt-einsum&gt;=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (23.1)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,&lt;5.0.0dev,&gt;=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.23.3)\nRequirement already satisfied: requests&lt;3,&gt;=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.31.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (68.1.2)\nRequirement already satisfied: six&gt;=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor&gt;=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions&gt;=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.7.1)\nRequirement already satisfied: wrapt&gt;=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.15.0)\nRequirement already satisfied: grpcio&lt;2.0,&gt;=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.62.1)\nRequirement already satisfied: tensorboard&lt;2.17,&gt;=2.16 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.16.2)\nRequirement already satisfied: keras&gt;=3.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.1.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem&gt;=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.36.0)\nRequirement already satisfied: numpy&lt;2.0.0,&gt;=1.23.5 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.24.4)\nRequirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse&gt;=1.6.0-&gt;tensorflow) (0.41.2)\nRequirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from keras&gt;=3.0.0-&gt;tensorflow) (13.5.3)\nRequirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras&gt;=3.0.0-&gt;tensorflow) (0.0.7)\nRequirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras&gt;=3.0.0-&gt;tensorflow) (0.11.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/conda/lib/python3.11/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (3.2.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.11/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (2.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (2023.7.22)\nRequirement already satisfied: markdown&gt;=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard&lt;2.17,&gt;=2.16-&gt;tensorflow) (3.6)\nRequirement already satisfied: tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard&lt;2.17,&gt;=2.16-&gt;tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug&gt;=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard&lt;2.17,&gt;=2.16-&gt;tensorflow) (3.0.1)\nRequirement already satisfied: MarkupSafe&gt;=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard&lt;2.17,&gt;=2.16-&gt;tensorflow) (2.1.3)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich-&gt;keras&gt;=3.0.0-&gt;tensorflow) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich-&gt;keras&gt;=3.0.0-&gt;tensorflow) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras&gt;=3.0.0-&gt;tensorflow) (0.1.0)\n\n\n\n#load packages\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n#packages for manipulating dataframe\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport sklearn\n\n#packages for machine learning\n##train-test-split\nfrom sklearn.model_selection import train_test_split, GridSearchCV, validation_curve\n\n##method 1: Linear Regression (LR)\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import cross_val_score\n\n##method 2: Random Forest Regressor (RF)\nimport rfpimp\nfrom sklearn.ensemble import RandomForestRegressor\n\n##method 3: Gradient Boosting Regressor (XGB)\nimport xgboost\nfrom xgboost import XGBRegressor\n\n##method 4: Artificial Neural Network (ANN)\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n##cross validation\n\n##evaluation metrics (R2 and RMSE)\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n#data visualization\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_rows', 300) # specifies number of rows to show\npd.options.display.float_format = '{:40,.4f}'.format # specifies default number format to 4 decimal places\nplt.style.use('ggplot') # specifies that graphs should use ggplot styling\n%matplotlib inline"
  },
  {
    "objectID": "Modeling/soybean_supervised_learning.html#load-cleaning-data",
    "href": "Modeling/soybean_supervised_learning.html#load-cleaning-data",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "1. Load & Cleaning Data",
    "text": "1. Load & Cleaning Data\n\n#load data\nsoybean_2018 = pd.read_csv('https://www.dropbox.com/scl/fi/ixibqk9pyuehwvoz7mr1h/2018_County_Summary_Merged.csv?rlkey=wueodqdvzsht5or4eftwdd3ys&dl=1')\nsoybean_2019 = pd.read_csv('https://www.dropbox.com/scl/fi/x3oiqrikr0xagqrh5tiqu/2019_County_Summary_Merged.csv?rlkey=zr5wru8sg5iybp43lnjf7ls5a&dl=1')\nsoybean_2020 = pd.read_csv('https://www.dropbox.com/scl/fi/m4r74ydgw3yno93nakagl/2020_County_Summary_Merged.csv?rlkey=kmd8bozo9z9jxznoa775gg33z&dl=1')\nsoybean_2021 = pd.read_csv('https://www.dropbox.com/scl/fi/2vmvsyjloz9hvzejtdez4/2021_County_Summary_Merged.csv?rlkey=ossgr4x3fvzgebhqprchntt2f&dl=1')\nsoybean_2022 = pd.read_csv('https://www.dropbox.com/scl/fi/x61224yvwtq4idnqphwjy/2022_County_Summary_Merged.csv?rlkey=s3zm9ooap8pjqom3jr0aklc6t&dl=1')\nsoybean_2023 = pd.read_csv('https://www.dropbox.com/scl/fi/zrjnmeqysrokfsua24hb3/2023_County_Summary_Merged.csv?rlkey=jb4w1pt295zeagbvgm7c2t7pk&dl=1')\n\n\nsoybean_list = [soybean_2018, soybean_2019, soybean_2020, soybean_2021, soybean_2022, soybean_2023]\nsoybean_df = pd.concat(soybean_list)\nsoybean_df = soybean_df.drop(['NAME','GEOID','SMS_PM','SMS_AM','SAR'], axis=1)\nsoybean_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 316 entries, 0 to 52\nData columns (total 7 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   LST_DAY    316 non-null    float64\n 1   PA         316 non-null    float64\n 2   NDVI       316 non-null    float64\n 3   ET         316 non-null    float64\n 4   LST_NIGHT  316 non-null    float64\n 5   PAR        316 non-null    float64\n 6   YIELD      316 non-null    float64\ndtypes: float64(7)\nmemory usage: 19.8 KB\n\n\n\nsoybean_df.YIELD.hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n# Correlation coefficients\ncorrelation_matrix = soybean_df.corr()\n\nplt.rcParams[\"axes.grid\"] = False\nf = plt.figure(figsize=(19, 15))\nplt.matshow(soybean_df.corr(), fignum=f.number,cmap='bwr')\nplt.xticks(range(soybean_df.shape[1]), soybean_df.columns, fontsize=14, rotation=90)\nplt.yticks(range(soybean_df.shape[1]), soybean_df.columns, fontsize=14)\n\nfor i in range(soybean_df.shape[1]):\n    for j in range(soybean_df.shape[1]):\n        plt.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}', ha='center', va='center', fontsize=12, color='black')\n\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)\n\nplt.savefig('correlation_matrix_soybean.png', bbox_inches='tight')\n\nplt.title('Correlation Matrix', fontsize=16)\n\nText(0.5, 1.0, 'Correlation Matrix')"
  },
  {
    "objectID": "Modeling/soybean_supervised_learning.html#train-test-data-split",
    "href": "Modeling/soybean_supervised_learning.html#train-test-data-split",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "2.1 Train & Test Data Split",
    "text": "2.1 Train & Test Data Split\n\n#split the dataset\nX = soybean_df.drop('YIELD', axis=1)\ny = soybean_df['YIELD']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=100)"
  },
  {
    "objectID": "Modeling/soybean_supervised_learning.html#train-test-data-standarderlized",
    "href": "Modeling/soybean_supervised_learning.html#train-test-data-standarderlized",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "2.2 Train & Test Data Standarderlized",
    "text": "2.2 Train & Test Data Standarderlized\n\ndef standardize_columns(file_name, columns_to_standardize):\n    scaler = StandardScaler()\n\n    df = file_name\n    df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n    return df\n\ndef standardize_series(series):\n    scaler = StandardScaler()\n    series = scaler.fit_transform(series.values.reshape(-1, 1)).flatten()\n    return series\n    \nX_columns = ['LST_DAY','PA','NDVI','ET','LST_NIGHT','PAR']\ny_columns = ['YIELD']\n\nX_train = standardize_columns(X_train, X_columns)\nX_test = standardize_columns(X_test, X_columns)\n\n#y_train = standardize_series(y_train)\n#y_test = standardize_series(y_test)"
  },
  {
    "objectID": "Modeling/soybean_supervised_learning.html#model-training-and-parameter-tuning",
    "href": "Modeling/soybean_supervised_learning.html#model-training-and-parameter-tuning",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "3. Model Training and Parameter Tuning",
    "text": "3. Model Training and Parameter Tuning\n\n3.1. Linear Regression (LR)\n\nmodel_lr = LinearRegression()\n\n# Cross validation\nscores = cross_val_score(model_lr, X, y, cv=5, scoring='neg_mean_squared_error')\n\nmean_mse = np.mean(scores)\nstd_mse = np.std(scores)\n\nprint(f'Mean MSE: {mean_mse}')\nprint(f'Standard Deviation of MSE: {std_mse}')\n\nMean MSE: -13.599376472183371\nStandard Deviation of MSE: 1.42475853144851\n\n\n\nmodel_lr.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\n3.2. Random Forest Regressor (RF)\n\n# values of max_depth and min_samples_split\nhyperparameters = {'max_depth':[3,5,10,20,30], 'min_samples_split':[2,4,6,8,10]}\n\n\nrandomState_dt = 10000\nmodel_rf = RandomForestRegressor(random_state=randomState_dt)\n\n# cv=5 by default, which means 5-fold cross-validation\nclf = GridSearchCV(model_rf, hyperparameters)\n\nclf.fit(X_train, y_train)\n\n# we can query the best parameter value and its accuracy score\nprint (\"The best parameter value is: \")\nprint (clf.best_params_)\nprint (\"The best score is: \")\nprint (clf.best_score_)\n\nThe best parameter value is: \n{'max_depth': 20, 'min_samples_split': 4}\nThe best score is: \n0.8122505039988435\n\n\n\n# Train the final RF\nrf_final = RandomForestRegressor(max_depth=clf.best_params_['max_depth'], min_samples_split=clf.best_params_['min_samples_split'], random_state=randomState_dt)\nrf_final.fit(X_train, y_train)\n\nRandomForestRegressor(max_depth=20, min_samples_split=4, random_state=10000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_depth=20, min_samples_split=4, random_state=10000)\n\n\n\n\n3.3. Gradient Boosting Regressor (XGB)\n\nimport warnings\n\n# 设置忽略 FutureWarning 类型的警告\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\n# model_xgb =\n# soybean_df\n\n# values of max_depth and min_samples_split\nhyperparameters = {'max_depth':[2,4,6,8,10], 'n_estimators':[4,8,12,16,20]}\n\nrandomState_xgb = 125\nxgb = XGBRegressor(random_state=randomState_xgb)\n\n# cv=5 by default, which means 5-fold cross-validation\ngscv_xgb = GridSearchCV(xgb, hyperparameters)\n\ngscv_xgb.fit(X_train, y_train)\n\n# we can query the best parameter value and its accuracy score\nprint (\"The best parameter value is: \")\nprint (gscv_xgb.best_params_)\nprint (\"The best score is: \")\nprint (gscv_xgb.best_score_)\n\nThe best parameter value is: \n{'max_depth': 2, 'n_estimators': 20}\nThe best score is: \n0.8119468473201896\n\n\n\n\n3.4. Artificial Neural Network (ANN)\n\nmodel_ann = keras.Sequential([\n    layers.Input(shape=(6,)),  # Input layer\n    layers.Dense(128, activation='relu'),  # Hidden layer with ReLU activation\n    layers.Dropout(0.5),  # Dropout layer for regularization\n    layers.Dense(64, activation='relu'),  # Additional hidden layer\n    layers.Dropout(0.3),  # Another dropout layer\n    layers.Dense(1)  # Output layer\n])\n\n#measuring the training with certain metrics\nmodel_ann.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n\n\n#train the model\nmodel_ann.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n\nEpoch 1/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 20ms/step - accuracy: 0.0000e+00 - loss: 908.8058 - val_accuracy: 0.0000e+00 - val_loss: 874.9622\nEpoch 2/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 881.8877 - val_accuracy: 0.0000e+00 - val_loss: 824.8320\nEpoch 3/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 807.2585 - val_accuracy: 0.0000e+00 - val_loss: 761.8193\nEpoch 4/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 770.3871 - val_accuracy: 0.0000e+00 - val_loss: 682.5032\nEpoch 5/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 673.8093 - val_accuracy: 0.0000e+00 - val_loss: 588.1645\nEpoch 6/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 548.4451 - val_accuracy: 0.0000e+00 - val_loss: 482.6881\nEpoch 7/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 474.4578 - val_accuracy: 0.0000e+00 - val_loss: 372.2153\nEpoch 8/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 345.3992 - val_accuracy: 0.0000e+00 - val_loss: 267.8072\nEpoch 9/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 253.2850 - val_accuracy: 0.0000e+00 - val_loss: 177.0001\nEpoch 10/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 163.8889 - val_accuracy: 0.0000e+00 - val_loss: 115.4538\nEpoch 11/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 143.7180 - val_accuracy: 0.0000e+00 - val_loss: 83.0094\nEpoch 12/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 105.2606 - val_accuracy: 0.0000e+00 - val_loss: 66.7544\nEpoch 13/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 105.3339 - val_accuracy: 0.0000e+00 - val_loss: 61.2751\nEpoch 14/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 100.4879 - val_accuracy: 0.0000e+00 - val_loss: 57.5591\nEpoch 15/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 84.1489 - val_accuracy: 0.0000e+00 - val_loss: 54.7739\nEpoch 16/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 80.5980 - val_accuracy: 0.0000e+00 - val_loss: 53.6406\nEpoch 17/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 79.7008 - val_accuracy: 0.0000e+00 - val_loss: 51.5685\nEpoch 18/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 80.1594 - val_accuracy: 0.0000e+00 - val_loss: 48.3465\nEpoch 19/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 69.1882 - val_accuracy: 0.0000e+00 - val_loss: 44.7548\nEpoch 20/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 65.8164 - val_accuracy: 0.0000e+00 - val_loss: 42.1481\nEpoch 21/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 70.6251 - val_accuracy: 0.0000e+00 - val_loss: 41.2548\nEpoch 22/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 67.4814 - val_accuracy: 0.0000e+00 - val_loss: 39.9367\nEpoch 23/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 65.2247 - val_accuracy: 0.0000e+00 - val_loss: 37.4850\nEpoch 24/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 54.6812 - val_accuracy: 0.0000e+00 - val_loss: 35.5363\nEpoch 25/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 55.7305 - val_accuracy: 0.0000e+00 - val_loss: 34.7602\nEpoch 26/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 58.8423 - val_accuracy: 0.0000e+00 - val_loss: 34.5759\nEpoch 27/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 50.8253 - val_accuracy: 0.0000e+00 - val_loss: 33.1217\nEpoch 28/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 57.8260 - val_accuracy: 0.0000e+00 - val_loss: 32.6150\nEpoch 29/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.0000e+00 - loss: 47.4429 - val_accuracy: 0.0000e+00 - val_loss: 31.8825\nEpoch 30/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 45.1291 - val_accuracy: 0.0000e+00 - val_loss: 30.8348\nEpoch 31/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 56.9867 - val_accuracy: 0.0000e+00 - val_loss: 29.5875\nEpoch 32/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 55.9614 - val_accuracy: 0.0000e+00 - val_loss: 29.6100\nEpoch 33/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 57.8997 - val_accuracy: 0.0000e+00 - val_loss: 28.9729\nEpoch 34/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 49.0065 - val_accuracy: 0.0000e+00 - val_loss: 27.6935\nEpoch 35/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 43.3303 - val_accuracy: 0.0000e+00 - val_loss: 27.1780\nEpoch 36/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 40.8218 - val_accuracy: 0.0000e+00 - val_loss: 26.3893\nEpoch 37/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 48.5167 - val_accuracy: 0.0000e+00 - val_loss: 25.7885\nEpoch 38/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 43.8141 - val_accuracy: 0.0000e+00 - val_loss: 25.7485\nEpoch 39/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 34.2318 - val_accuracy: 0.0000e+00 - val_loss: 25.6844\nEpoch 40/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 48.8074 - val_accuracy: 0.0000e+00 - val_loss: 25.6061\nEpoch 41/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 45.4220 - val_accuracy: 0.0000e+00 - val_loss: 26.5852\nEpoch 42/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 42.3302 - val_accuracy: 0.0000e+00 - val_loss: 26.4074\nEpoch 43/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 42.0265 - val_accuracy: 0.0000e+00 - val_loss: 25.6921\nEpoch 44/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 49.0766 - val_accuracy: 0.0000e+00 - val_loss: 24.7880\nEpoch 45/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 43.9120 - val_accuracy: 0.0000e+00 - val_loss: 23.5887\nEpoch 46/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 35.7929 - val_accuracy: 0.0000e+00 - val_loss: 22.7178\nEpoch 47/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 41.7750 - val_accuracy: 0.0000e+00 - val_loss: 21.6916\nEpoch 48/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 43.5615 - val_accuracy: 0.0000e+00 - val_loss: 21.9642\nEpoch 49/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 41.1430 - val_accuracy: 0.0000e+00 - val_loss: 22.0206\nEpoch 50/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 42.4059 - val_accuracy: 0.0000e+00 - val_loss: 21.1436\nEpoch 51/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 37.0274 - val_accuracy: 0.0000e+00 - val_loss: 20.8737\nEpoch 52/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 35.3589 - val_accuracy: 0.0000e+00 - val_loss: 21.8772\nEpoch 53/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 35.1861 - val_accuracy: 0.0000e+00 - val_loss: 21.2295\nEpoch 54/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 37.7701 - val_accuracy: 0.0000e+00 - val_loss: 21.5283\nEpoch 55/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 38.2671 - val_accuracy: 0.0000e+00 - val_loss: 21.6889\nEpoch 56/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 38.9968 - val_accuracy: 0.0000e+00 - val_loss: 20.8944\nEpoch 57/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 34.1285 - val_accuracy: 0.0000e+00 - val_loss: 20.2047\nEpoch 58/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 34.1664 - val_accuracy: 0.0000e+00 - val_loss: 20.2900\nEpoch 59/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 35.9370 - val_accuracy: 0.0000e+00 - val_loss: 19.6327\nEpoch 60/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.0000e+00 - loss: 40.1818 - val_accuracy: 0.0000e+00 - val_loss: 18.8925\nEpoch 61/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 41.7917 - val_accuracy: 0.0000e+00 - val_loss: 19.2338\nEpoch 62/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 33.7920 - val_accuracy: 0.0000e+00 - val_loss: 19.6023\nEpoch 63/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 31.8736 - val_accuracy: 0.0000e+00 - val_loss: 18.7633\nEpoch 64/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 39.9844 - val_accuracy: 0.0000e+00 - val_loss: 18.3042\nEpoch 65/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 30.1128 - val_accuracy: 0.0000e+00 - val_loss: 19.0860\nEpoch 66/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 40.9909 - val_accuracy: 0.0000e+00 - val_loss: 19.5888\nEpoch 67/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 29.7351 - val_accuracy: 0.0000e+00 - val_loss: 18.9636\nEpoch 68/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 43.0850 - val_accuracy: 0.0000e+00 - val_loss: 18.1852\nEpoch 69/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 38.9383 - val_accuracy: 0.0000e+00 - val_loss: 17.4548\nEpoch 70/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 33.1937 - val_accuracy: 0.0000e+00 - val_loss: 17.7960\nEpoch 71/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 33.8170 - val_accuracy: 0.0000e+00 - val_loss: 19.5283\nEpoch 72/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 35.2376 - val_accuracy: 0.0000e+00 - val_loss: 19.1320\nEpoch 73/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 45.4136 - val_accuracy: 0.0000e+00 - val_loss: 16.9785\nEpoch 74/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 22.9196 - val_accuracy: 0.0000e+00 - val_loss: 17.3707\nEpoch 75/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 30.3228 - val_accuracy: 0.0000e+00 - val_loss: 17.2736\nEpoch 76/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 27.4552 - val_accuracy: 0.0000e+00 - val_loss: 17.2925\nEpoch 77/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 32.9510 - val_accuracy: 0.0000e+00 - val_loss: 16.6056\nEpoch 78/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 32.1852 - val_accuracy: 0.0000e+00 - val_loss: 17.3915\nEpoch 79/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 34.4185 - val_accuracy: 0.0000e+00 - val_loss: 17.5377\nEpoch 80/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 36.6280 - val_accuracy: 0.0000e+00 - val_loss: 17.1050\nEpoch 81/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 31.5856 - val_accuracy: 0.0000e+00 - val_loss: 15.6781\nEpoch 82/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 31.1735 - val_accuracy: 0.0000e+00 - val_loss: 15.2313\nEpoch 83/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 29.6116 - val_accuracy: 0.0000e+00 - val_loss: 15.1555\nEpoch 84/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 30.3405 - val_accuracy: 0.0000e+00 - val_loss: 15.2459\nEpoch 85/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 32.2704 - val_accuracy: 0.0000e+00 - val_loss: 15.9612\nEpoch 86/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 30.0105 - val_accuracy: 0.0000e+00 - val_loss: 16.4121\nEpoch 87/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 30.2470 - val_accuracy: 0.0000e+00 - val_loss: 15.8861\nEpoch 88/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 28.9338 - val_accuracy: 0.0000e+00 - val_loss: 15.2324\nEpoch 89/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 32.1413 - val_accuracy: 0.0000e+00 - val_loss: 15.2657\nEpoch 90/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 33.2470 - val_accuracy: 0.0000e+00 - val_loss: 15.1221\nEpoch 91/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 28.3143 - val_accuracy: 0.0000e+00 - val_loss: 15.8794\nEpoch 92/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 32.5550 - val_accuracy: 0.0000e+00 - val_loss: 15.7008\nEpoch 93/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 30.4661 - val_accuracy: 0.0000e+00 - val_loss: 15.6911\nEpoch 94/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 28.4072 - val_accuracy: 0.0000e+00 - val_loss: 15.2969\nEpoch 95/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 24.3947 - val_accuracy: 0.0000e+00 - val_loss: 15.1924\nEpoch 96/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 31.4213 - val_accuracy: 0.0000e+00 - val_loss: 14.7516\nEpoch 97/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 29.6972 - val_accuracy: 0.0000e+00 - val_loss: 15.2054\nEpoch 98/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 29.3912 - val_accuracy: 0.0000e+00 - val_loss: 16.0227\nEpoch 99/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 29.0754 - val_accuracy: 0.0000e+00 - val_loss: 16.6007\nEpoch 100/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 32.3250 - val_accuracy: 0.0000e+00 - val_loss: 14.9619\n\n\n&lt;keras.src.callbacks.history.History at 0x7f0760118910&gt;"
  },
  {
    "objectID": "Modeling/soybean_supervised_learning.html#model-evaluation-and-performance-comparison",
    "href": "Modeling/soybean_supervised_learning.html#model-evaluation-and-performance-comparison",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "4. Model Evaluation and Performance Comparison",
    "text": "4. Model Evaluation and Performance Comparison\nComparing the performance with using R2 and Root Mean Squared Error (RMSE).\n\n4.1. E. Linear Regression (LR)\n\ntrain_predictions = model_lr.predict(X_train)\ntest_predictions = model_lr.predict(X_test)\n\nr2_train_lr = r2_score(y_train, train_predictions)\nr2_test_lr = r2_score(y_test, test_predictions)\n\nrmse_train_lr = mean_squared_error(y_train, train_predictions, squared=False)\nrmse_test_lr = mean_squared_error(y_test, test_predictions, squared=False)\n\nprint(f\"Training R^2: {r2_train_lr:.4f}\")\nprint(f\"Test R^2: {r2_test_lr:.4f}\")\nprint(f\"Training RMSE: {rmse_train_lr:.4f}\")\nprint(f\"Test RMSE: {rmse_test_lr:.4f}\")\n\nTraining R^2: 0.7965\nTest R^2: 0.7977\nTraining RMSE: 3.4311\nTest RMSE: 3.5609\n\n\n\n\n4.2. E. Random Forest Regressor (RF)\n\nr2_train_rf = rf_final.score(X=X_train, y=y_train)\nr2_test_rf = rf_final.score(X=X_test, y=y_test)\n\nprint(\"R2 on the training data:\")\nprint(r2_train_rf)\nprint(\"R2 on the testing data:\")\nprint(r2_test_rf)\n\nR2 on the training data:\n0.9688517783796329\nR2 on the testing data:\n0.8370134081603526\n\n\n\nrmse_train_rf = mean_squared_error(y_train, rf_final.predict(X_train), squared=False)\nrmse_test_rf = mean_squared_error(y_test, rf_final.predict(X_test), squared=False)\n\nprint(\"RMSE on the training data:\")\nprint(rmse_train_rf)\nprint(\"RMSE on the testing data:\")\nprint(rmse_test_rf)\n\nRMSE on the training data:\n1.3425104723153702\nRMSE on the testing data:\n3.196618497059926\n\n\n\n# Calculate and plot the feature importance of the RF model\nimp = rfpimp.importances(rf_final, X_test, y_test)\nprint(imp)\nviz = rfpimp.plot_importances(imp)\nviz.view()\n\n                                        Importance\nFeature                                           \nNDVI                                        0.4985\nET                                          0.3073\nPA                                          0.0500\nLST_DAY                                     0.0366\nLST_NIGHT                                   0.0193\nPAR                                         0.0123\n\n\n\n\n\n\n\n4.3. E. Gradient Boosting Regressor (XGB)\n\nmodel_xgb = XGBRegressor(max_depth=gscv_xgb.best_params_['max_depth'], n_estimators=gscv_xgb.best_params_['n_estimators'], random_state=randomState_xgb)\nmodel_xgb.fit(X_train, y_train)\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=2, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=20, n_jobs=None,\n             num_parallel_tree=None, random_state=125, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=2, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=20, n_jobs=None,\n             num_parallel_tree=None, random_state=125, ...)\n\n\n\n# r2_train_xgb, r2_test_xgb, rmse_train_xgb, rmse_test_xgb\nr2_train_xgb = model_xgb.score(X=X_train, y=y_train)\nr2_test_xgb = model_xgb.score(X=X_test, y=y_test)\nrmse_train_xgb = mean_squared_error(y_train, model_xgb.predict(X_train), squared=False)\nrmse_test_xgb = mean_squared_error(y_test, model_xgb.predict(X_test), squared=False)\n\n\nprint(\"R2 on the training data:\")\nprint(r2_train_xgb)\nprint(\"R2 on the testing data:\")\nprint(r2_test_xgb)\n\nR2 on the training data:\n0.9100252787798567\nR2 on the testing data:\n0.8426691231672218\n\n\n\nprint(\"RMSE on the training data:\")\nprint(rmse_train_xgb)\nprint(\"RMSE on the testing data:\")\nprint(rmse_test_xgb)\n\nRMSE on the training data:\n2.281714534929037\nRMSE on the testing data:\n3.14066670480111\n\n\n\nimp_xgb = rfpimp.importances(model_xgb, X_test, y_test) # permutation\nprint(imp_xgb)\nviz_xgb = rfpimp.plot_importances(imp_xgb)\nviz_xgb.view()\n\n                                        Importance\nFeature                                           \nNDVI                                        0.4580\nET                                          0.3588\nLST_NIGHT                                   0.0464\nPA                                          0.0407\nLST_DAY                                     0.0358\nPAR                                         0.0213\n\n\n\n\n\n\n\n4.4. E. Artificial Neural Network (ANN)\n\n#predictions\ny_pred_train_ann = model_ann.predict(X_train).flatten()\ny_pred_test_ann = model_ann.predict(X_test).flatten()\n\n#Compute R2 and RMSE\nr2_train_ann = np.round(r2_score(y_train, y_pred_train_ann),2)\nr2_test_ann = np.round(r2_score(y_test, y_pred_test_ann),2)\nrmse_train_ann = np.round(np.sqrt(mean_squared_error(y_train, y_pred_train_ann)),2)\nrmse_test_ann = np.round(np.sqrt(mean_squared_error(y_test, y_pred_test_ann)),2)\n\n#print the result\nprint(\"Train R2:\", r2_train_ann)\nprint(\"Test R2:\", r2_test_ann)\nprint(\"Train RMSE:\", rmse_train_ann)\nprint(\"Test RMSE:\", rmse_test_ann)\n\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step \n3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step \nTrain R2: 0.82\nTest R2: 0.8\nTrain RMSE: 3.2\nTest RMSE: 3.55\n\n\n\n#crosscheck the y value between real and predicted\ncrosscheck_y_dict = {\n    'y_test' : y_test,\n    'y_pred' : np.round(y_pred_test_ann,0),\n    'delta' : np.abs(np.round((y_test - y_pred_test_ann),0))\n}\n\n#plotting histogram\ncrosscheck_y_df = pd.DataFrame(crosscheck_y_dict)\nplt.hist(crosscheck_y_df['delta'], bins=10)\nplt.xlabel(f'Delta\\nR2_test = {r2_test_ann}, RMSE_test = {rmse_test_ann}, delta_max = {crosscheck_y_df.delta.max()}')\nplt.ylabel('Frequency (Number of Data)')\nplt.title(f\"Accuracy of ANN Model Based on Delta of Y Test and Y Pred)\")\nplt.show()\n\n\n\n\n\n\n4.5. Model Performance Comparison\n\n#please input your metrics in here\nmetrics_dict = {\n    'metrics': [\"Train R2\",\"Test R2\",\"Train RMSE\",\"Test RMSE\"],\n    'LR': [r2_train_lr, r2_test_lr, rmse_train_lr, rmse_test_lr],\n    'RF': [r2_train_rf, r2_test_rf, rmse_train_rf, rmse_test_rf],\n    'XGB': [r2_train_xgb, r2_test_xgb, rmse_train_xgb, rmse_test_xgb],\n    'ANN': [r2_train_ann, r2_test_ann, rmse_train_ann, rmse_test_ann]\n}\n\n#create dataframe\nmetrics_df = pd.DataFrame(metrics_dict)\nmetrics_df.set_index('metrics')\n\n\n\n\n\n\n\n\nLR\nRF\nXGB\nANN\n\n\nmetrics\n\n\n\n\n\n\n\n\nTrain R2\n0.7965\n0.9689\n0.9100\n0.8200\n\n\nTest R2\n0.7977\n0.8370\n0.8427\n0.8000\n\n\nTrain RMSE\n3.4311\n1.3425\n2.2817\n3.2000\n\n\nTest RMSE\n3.5609\n3.1966\n3.1407\n3.5500\n\n\n\n\n\n\n\nBased on the comparison, it can be said that the best model that can be used for estimating crop yield is []. From this point, [] model would be used for estimation phase (step 5)."
  },
  {
    "objectID": "Modeling/soybean_supervised_learning.html#crop-yield-estimation-and-export-result",
    "href": "Modeling/soybean_supervised_learning.html#crop-yield-estimation-and-export-result",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "5. Crop Yield Estimation and Export Result",
    "text": "5. Crop Yield Estimation and Export Result"
  },
  {
    "objectID": "Modeling/corn_supervised_learning.html",
    "href": "Modeling/corn_supervised_learning.html",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "",
    "text": "This jupyter notebook is addressed for creating model that can estimate crop yield in North Dakota, United States. The detail workflow of this project can be seen in here."
  },
  {
    "objectID": "Modeling/corn_supervised_learning.html#install-load-packages",
    "href": "Modeling/corn_supervised_learning.html#install-load-packages",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "0. Install & Load Packages",
    "text": "0. Install & Load Packages\n\n#install packages\n!pip install tensorflow\n\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.16.1)\nRequirement already satisfied: absl-py&gt;=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.0)\nRequirement already satisfied: astunparse&gt;=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers&gt;=23.5.26 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,&gt;=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta&gt;=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py&gt;=3.10.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.10.0)\nRequirement already satisfied: libclang&gt;=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.3.2)\nRequirement already satisfied: opt-einsum&gt;=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (23.1)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,&lt;5.0.0dev,&gt;=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.23.3)\nRequirement already satisfied: requests&lt;3,&gt;=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.31.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (68.1.2)\nRequirement already satisfied: six&gt;=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor&gt;=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions&gt;=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.7.1)\nRequirement already satisfied: wrapt&gt;=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: grpcio&lt;2.0,&gt;=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.62.1)\nRequirement already satisfied: tensorboard&lt;2.17,&gt;=2.16 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.16.2)\nRequirement already satisfied: keras&gt;=3.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.1.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem&gt;=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.36.0)\nRequirement already satisfied: numpy&lt;2.0.0,&gt;=1.23.5 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.24.4)\nRequirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse&gt;=1.6.0-&gt;tensorflow) (0.41.2)\nRequirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from keras&gt;=3.0.0-&gt;tensorflow) (13.6.0)\nRequirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras&gt;=3.0.0-&gt;tensorflow) (0.0.7)\nRequirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras&gt;=3.0.0-&gt;tensorflow) (0.11.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/conda/lib/python3.11/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (3.2.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.11/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (2.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (2023.7.22)\nRequirement already satisfied: markdown&gt;=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard&lt;2.17,&gt;=2.16-&gt;tensorflow) (3.6)\nRequirement already satisfied: tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard&lt;2.17,&gt;=2.16-&gt;tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug&gt;=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard&lt;2.17,&gt;=2.16-&gt;tensorflow) (3.0.1)\nRequirement already satisfied: MarkupSafe&gt;=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard&lt;2.17,&gt;=2.16-&gt;tensorflow) (2.1.3)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich-&gt;keras&gt;=3.0.0-&gt;tensorflow) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich-&gt;keras&gt;=3.0.0-&gt;tensorflow) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras&gt;=3.0.0-&gt;tensorflow) (0.1.0)\n\n\n\n#load packages\n\nfrom sklearn.preprocessing import StandardScaler\n\n#packages for manipulating dataframe\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport sklearn\n\n#packages for machine learning\n##train-test-split\nfrom sklearn.model_selection import train_test_split, validation_curve\n\n##method 1: Linear Regression (LR)\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n##method 2: Random Forest Regressor (RF)\nimport rfpimp\nfrom sklearn.ensemble import RandomForestRegressor\n\n##method 3: Gradient Boosting Regressor (XGB)\nimport xgboost\nfrom xgboost import XGBRegressor\n\n##method 4: Artificial Neural Network (ANN)\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n##cross validation\nfrom sklearn.model_selection import GridSearchCV\n\n##evaluation metrics (R2 and RMSE)\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n#data visualization\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_rows', 300) # specifies number of rows to show\npd.options.display.float_format = '{:40,.4f}'.format # specifies default number format to 4 decimal places\nplt.style.use('ggplot') # specifies that graphs should use ggplot styling\n%matplotlib inline"
  },
  {
    "objectID": "Modeling/corn_supervised_learning.html#load-cleaning-data",
    "href": "Modeling/corn_supervised_learning.html#load-cleaning-data",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "1. Load & Cleaning Data",
    "text": "1. Load & Cleaning Data\n\n#load data\ncorn_2018 = pd.read_csv('https://www.dropbox.com/scl/fi/fi5tg278l6gtdk08pvwil/Combined_Summary_By_County_2018_9_Parameters.csv?rlkey=w23aanh5dqvyczfe67bw49qjm&dl=1')\ncorn_2019 = pd.read_csv('https://www.dropbox.com/scl/fi/hu6n2eo1472q5fr3amt6i/Combined_Summary_By_County_2019_Parameters.csv?rlkey=232te8uu73b5apv3mrsul42i7&dl=1')\ncorn_2020 = pd.read_csv('https://www.dropbox.com/scl/fi/gf182tns9c30ighnyh2gx/Combined_Summary_By_County_2020_9_Parameters.csv?rlkey=74z3m1505saor7n9zdgnmprnd&dl=1')\ncorn_2021 = pd.read_csv('https://www.dropbox.com/scl/fi/y8a4ewsgjn8h8ij8ndvdd/Combined_Summary_By_County_2021_9_Parameters.csv?rlkey=fx9pia4medwkfr4minm1up9u1&dl=1')\ncorn_2022 = pd.read_csv('https://www.dropbox.com/scl/fi/bbpfgz7v4h9zp8wbcajh6/Combined_Summary_By_County_2022_9_Parameters.csv?rlkey=dxehj1e9bhygn6nvkb748n9d2&dl=1')\ncorn_2023 = pd.read_csv('https://www.dropbox.com/scl/fi/q9lp9gsge46tla76sg6ge/Combined_Summary_By_County_2023_9_Parameters.csv?rlkey=f3nhoewi35iswj3xurfqppipm&dl=1')\n\n\ncorn_list = [corn_2018, corn_2019, corn_2020, corn_2021, corn_2022, corn_2023]\ncorn_df = pd.concat(corn_list)\ncorn_df.dropna(inplace=True)\ncorn_df.YIELD = corn_df.YIELD.astype('float64')\n\n\ncorn_df.reset_index(inplace=True)\ncorn_df.drop(\"index\", axis = 1, inplace = True)\n\n\n# Correlation coefficients\n# correlation_matrix = soybean_df.corr()\n\n# plt.rcParams[\"axes.grid\"] = False\n# f = plt.figure(figsize=(19, 15))\n# plt.matshow(soybean_df.corr(), fignum=f.number,cmap='bwr')\n# plt.xticks(range(soybean_df.shape[1]), soybean_df.columns, fontsize=14, rotation=90)\n# plt.yticks(range(soybean_df.shape[1]), soybean_df.columns, fontsize=14)\n\n# for i in range(soybean_df.shape[1]):\n#    for j in range(soybean_df.shape[1]):\n#        plt.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}', ha='center', va='center', fontsize=12, color='black')\n\n# cb = plt.colorbar()\n# cb.ax.tick_params(labelsize=14)\n\n# plt.savefig('correlation_matrix_soybean.png', bbox_inches='tight')\n\n# plt.title('Correlation Matrix', fontsize=16)\n\nText(0.5, 1.0, 'Correlation Matrix')"
  },
  {
    "objectID": "Modeling/corn_supervised_learning.html#train-test-data-split",
    "href": "Modeling/corn_supervised_learning.html#train-test-data-split",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "2. Train & Test Data Split",
    "text": "2. Train & Test Data Split\n\ncorn_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 318 entries, 0 to 317\nData columns (total 13 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   NAME         318 non-null    object \n 1   Ag District  318 non-null    object \n 2   GEOID        318 non-null    int64  \n 3   NDVI         318 non-null    float64\n 4   PA           318 non-null    float64\n 5   SAR          318 non-null    float64\n 6   SMS_AM       318 non-null    float64\n 7   LST_DAY      318 non-null    float64\n 8   SMS_PM       318 non-null    float64\n 9   LST_NIGHT    318 non-null    float64\n 10  PAR          318 non-null    float64\n 11  ET           318 non-null    float64\n 12  YIELD        318 non-null    float64\ndtypes: float64(10), int64(1), object(2)\nmemory usage: 32.4+ KB\n\n\n\n#split the dataset\nX = corn_df.drop(['YIELD','NAME','Ag District','GEOID','SMS_AM','SMS_PM'], axis=1)\ny = corn_df['YIELD']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=100)\n\n\nX.select_dtypes(include=['number']).columns.to_list()\n\n['NDVI', 'PA', 'SAR', 'LST_DAY', 'LST_NIGHT', 'PAR', 'ET']\n\n\n\n#standardized X and Y data accordingly\ndef standardize_columns(file_name, columns_to_standardize):\n    scaler = StandardScaler()\n\n    df = file_name\n    df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n    return df\n\ndef standardize_series(series):\n    scaler = StandardScaler()\n    series = scaler.fit_transform(series.values.reshape(-1, 1)).flatten()\n    return series\n    \nX_columns = X.select_dtypes(include=['number']).columns.to_list()\ny_columns = ['YIELD']\n\nX_train = standardize_columns(X_train, X_columns)\nX_test = standardize_columns(X_test, X_columns)\n\n#y_train = standardize_series(y_train)\n#y_test = standardize_series(y_test)"
  },
  {
    "objectID": "Modeling/corn_supervised_learning.html#model-training-and-parameter-tuning",
    "href": "Modeling/corn_supervised_learning.html#model-training-and-parameter-tuning",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "3. Model Training and Parameter Tuning",
    "text": "3. Model Training and Parameter Tuning\n\n3.1. Linear Regression (LR)\n\nmodel_lr = LinearRegression()\n\nmodel_lr.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\n3.2. Random Forest Regressor (RF)\n\n# values of max_depth and min_samples_split\nhyperparameters = {'max_depth':[3,5,10,20,30], 'min_samples_split':[2,4,6,8,10]}\n\n\nrandomState_dt = 100\nmodel_rf = RandomForestRegressor(random_state=randomState_dt)\n\n# cv=5 by default, which means 5-fold cross-validation\nclf = GridSearchCV(model_rf, hyperparameters)\n\nclf.fit(X_train, y_train)\n\n# we can query the best parameter value and its accuracy score\nprint (\"The best parameter value is: \")\nprint (clf.best_params_)\nprint (\"The best score is: \")\nprint (clf.best_score_)\n\nThe best parameter value is: \n{'max_depth': 10, 'min_samples_split': 2}\nThe best score is: \n0.7278766556902562\n\n\n\n# Train the final RF\nrf_final = RandomForestRegressor(max_depth=clf.best_params_['max_depth'], min_samples_split=clf.best_params_['min_samples_split'], random_state=randomState_dt)\nrf_final.fit(X_train, y_train)\n\nRandomForestRegressor(max_depth=10, random_state=100)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_depth=10, random_state=100)\n\n\n\n\n3.3. Gradient Boosting Regressor (XGB)\n\nimport warnings\n\n# 设置忽略 FutureWarning 类型的警告\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\n# model_xgb =\n# soybean_df\n\n# values of max_depth and min_samples_split\nhyperparameters = {'max_depth':[2,4,6,8,10], 'n_estimators':[4,8,12,16,20]}\n\nrandomState_xgb = 100\nxgb = XGBRegressor(random_state=randomState_xgb)\n\n# cv=5 by default, which means 5-fold cross-validation\ngscv_xgb = GridSearchCV(xgb, hyperparameters)\n\ngscv_xgb.fit(X_train, y_train)\n\n# we can query the best parameter value and its accuracy score\nprint (\"The best parameter value is: \")\nprint (gscv_xgb.best_params_)\nprint (\"The best score is: \")\nprint (gscv_xgb.best_score_)\n\nThe best parameter value is: \n{'max_depth': 2, 'n_estimators': 20}\nThe best score is: \n0.7121220343454169\n\n\n\n\n3.4. Artificial Neural Network (ANN)\n\nmodel_ann = keras.Sequential([\n    layers.Input(shape=(7,)),  # Input layer\n    layers.Dense(128, activation='relu'),  # Hidden layer with ReLU activation\n    layers.Dropout(0.5),  # Dropout layer for regularization\n    layers.Dense(64, activation='relu'),  # Additional hidden layer\n    layers.Dropout(0.3),  # Another dropout layer\n    layers.Dense(1)  # Output layer\n])\n\n#measuring the training with certain metrics\nmodel_ann.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n\n\n#train the model\nmodel_ann.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n\nEpoch 1/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 2s 42ms/step - accuracy: 0.0000e+00 - loss: 13492.8203 - val_accuracy: 0.0000e+00 - val_loss: 13332.3604\nEpoch 2/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 14162.4619 - val_accuracy: 0.0000e+00 - val_loss: 13206.9385\nEpoch 3/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.0000e+00 - loss: 13756.4443 - val_accuracy: 0.0000e+00 - val_loss: 13051.2881\nEpoch 4/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.0000e+00 - loss: 13332.2607 - val_accuracy: 0.0000e+00 - val_loss: 12844.4424\nEpoch 5/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.0000e+00 - loss: 12795.7266 - val_accuracy: 0.0000e+00 - val_loss: 12571.5576\nEpoch 6/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.0000e+00 - loss: 12933.5332 - val_accuracy: 0.0000e+00 - val_loss: 12213.5586\nEpoch 7/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.0000e+00 - loss: 12052.7168 - val_accuracy: 0.0000e+00 - val_loss: 11749.3467\nEpoch 8/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 11450.0723 - val_accuracy: 0.0000e+00 - val_loss: 11164.1836\nEpoch 9/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 10787.1143 - val_accuracy: 0.0000e+00 - val_loss: 10432.6064\nEpoch 10/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.0000e+00 - loss: 10519.8975 - val_accuracy: 0.0000e+00 - val_loss: 9580.5723\nEpoch 11/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - accuracy: 0.0000e+00 - loss: 9669.5732 - val_accuracy: 0.0000e+00 - val_loss: 8584.3271\nEpoch 12/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.0000e+00 - loss: 8581.1729 - val_accuracy: 0.0000e+00 - val_loss: 7485.4805\nEpoch 13/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 7281.5098 - val_accuracy: 0.0000e+00 - val_loss: 6326.3516\nEpoch 14/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 5978.1055 - val_accuracy: 0.0000e+00 - val_loss: 5162.8003\nEpoch 15/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 4995.2368 - val_accuracy: 0.0000e+00 - val_loss: 4066.4890\nEpoch 16/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 3705.8855 - val_accuracy: 0.0000e+00 - val_loss: 3140.8457\nEpoch 17/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 3242.4534 - val_accuracy: 0.0000e+00 - val_loss: 2435.2188\nEpoch 18/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 2482.9851 - val_accuracy: 0.0000e+00 - val_loss: 1959.2799\nEpoch 19/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 2227.7222 - val_accuracy: 0.0000e+00 - val_loss: 1683.4948\nEpoch 20/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 1987.5525 - val_accuracy: 0.0000e+00 - val_loss: 1526.1865\nEpoch 21/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.0000e+00 - loss: 1746.2271 - val_accuracy: 0.0000e+00 - val_loss: 1418.6797\nEpoch 22/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 1742.3623 - val_accuracy: 0.0000e+00 - val_loss: 1331.7485\nEpoch 23/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 1856.7023 - val_accuracy: 0.0000e+00 - val_loss: 1248.3656\nEpoch 24/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 1705.0605 - val_accuracy: 0.0000e+00 - val_loss: 1180.4703\nEpoch 25/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.0000e+00 - loss: 1582.2346 - val_accuracy: 0.0000e+00 - val_loss: 1114.3368\nEpoch 26/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 1612.5668 - val_accuracy: 0.0000e+00 - val_loss: 1059.7699\nEpoch 27/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.0000e+00 - loss: 1362.2079 - val_accuracy: 0.0000e+00 - val_loss: 1011.2990\nEpoch 28/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.0000e+00 - loss: 1566.2343 - val_accuracy: 0.0000e+00 - val_loss: 947.0475\nEpoch 29/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 1257.4071 - val_accuracy: 0.0000e+00 - val_loss: 900.7272\nEpoch 30/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 1231.9034 - val_accuracy: 0.0000e+00 - val_loss: 856.7522\nEpoch 31/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 1489.4359 - val_accuracy: 0.0000e+00 - val_loss: 821.1494\nEpoch 32/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 1112.2179 - val_accuracy: 0.0000e+00 - val_loss: 791.2066\nEpoch 33/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 1048.1464 - val_accuracy: 0.0000e+00 - val_loss: 757.6331\nEpoch 34/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 1158.5731 - val_accuracy: 0.0000e+00 - val_loss: 719.2524\nEpoch 35/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 1144.9857 - val_accuracy: 0.0000e+00 - val_loss: 678.7062\nEpoch 36/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 1302.0231 - val_accuracy: 0.0000e+00 - val_loss: 663.3835\nEpoch 37/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 985.7791 - val_accuracy: 0.0000e+00 - val_loss: 640.2110\nEpoch 38/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 1020.3046 - val_accuracy: 0.0000e+00 - val_loss: 616.9422\nEpoch 39/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 1061.5134 - val_accuracy: 0.0000e+00 - val_loss: 593.6237\nEpoch 40/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 1069.6046 - val_accuracy: 0.0000e+00 - val_loss: 572.3646\nEpoch 41/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 1012.7648 - val_accuracy: 0.0000e+00 - val_loss: 564.4551\nEpoch 42/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 1098.4084 - val_accuracy: 0.0000e+00 - val_loss: 552.8108\nEpoch 43/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 866.8872 - val_accuracy: 0.0000e+00 - val_loss: 526.9116\nEpoch 44/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 1054.1882 - val_accuracy: 0.0000e+00 - val_loss: 501.1353\nEpoch 45/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 963.0295 - val_accuracy: 0.0000e+00 - val_loss: 482.6159\nEpoch 46/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 1006.3253 - val_accuracy: 0.0000e+00 - val_loss: 470.3776\nEpoch 47/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.0000e+00 - loss: 923.7576 - val_accuracy: 0.0000e+00 - val_loss: 456.0844\nEpoch 48/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 879.2374 - val_accuracy: 0.0000e+00 - val_loss: 441.0634\nEpoch 49/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 881.5299 - val_accuracy: 0.0000e+00 - val_loss: 439.2146\nEpoch 50/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 790.1486 - val_accuracy: 0.0000e+00 - val_loss: 439.2279\nEpoch 51/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 917.8427 - val_accuracy: 0.0000e+00 - val_loss: 441.7203\nEpoch 52/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 829.0428 - val_accuracy: 0.0000e+00 - val_loss: 437.0248\nEpoch 53/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 803.2091 - val_accuracy: 0.0000e+00 - val_loss: 422.4519\nEpoch 54/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 968.7137 - val_accuracy: 0.0000e+00 - val_loss: 425.9000\nEpoch 55/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 825.2831 - val_accuracy: 0.0000e+00 - val_loss: 415.2858\nEpoch 56/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 772.5063 - val_accuracy: 0.0000e+00 - val_loss: 404.5883\nEpoch 57/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 794.0522 - val_accuracy: 0.0000e+00 - val_loss: 400.0487\nEpoch 58/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 880.9077 - val_accuracy: 0.0000e+00 - val_loss: 401.1310\nEpoch 59/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 872.8740 - val_accuracy: 0.0000e+00 - val_loss: 396.3031\nEpoch 60/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 832.2770 - val_accuracy: 0.0000e+00 - val_loss: 403.4333\nEpoch 61/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 775.3467 - val_accuracy: 0.0000e+00 - val_loss: 402.0116\nEpoch 62/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 744.9315 - val_accuracy: 0.0000e+00 - val_loss: 401.1469\nEpoch 63/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 819.1769 - val_accuracy: 0.0000e+00 - val_loss: 393.2069\nEpoch 64/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.0000e+00 - loss: 687.8067 - val_accuracy: 0.0000e+00 - val_loss: 370.2102\nEpoch 65/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 673.0170 - val_accuracy: 0.0000e+00 - val_loss: 351.1604\nEpoch 66/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 905.0669 - val_accuracy: 0.0000e+00 - val_loss: 348.4458\nEpoch 67/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 740.7355 - val_accuracy: 0.0000e+00 - val_loss: 352.6963\nEpoch 68/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 699.9937 - val_accuracy: 0.0000e+00 - val_loss: 355.4905\nEpoch 69/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 643.4801 - val_accuracy: 0.0000e+00 - val_loss: 351.3142\nEpoch 70/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.0000e+00 - loss: 714.3232 - val_accuracy: 0.0000e+00 - val_loss: 356.0028\nEpoch 71/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 655.9907 - val_accuracy: 0.0000e+00 - val_loss: 345.8247\nEpoch 72/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 798.1910 - val_accuracy: 0.0000e+00 - val_loss: 339.5587\nEpoch 73/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 630.7967 - val_accuracy: 0.0000e+00 - val_loss: 332.5110\nEpoch 74/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 983.3794 - val_accuracy: 0.0000e+00 - val_loss: 340.9774\nEpoch 75/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 727.7103 - val_accuracy: 0.0000e+00 - val_loss: 342.4319\nEpoch 76/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 684.0475 - val_accuracy: 0.0000e+00 - val_loss: 343.4070\nEpoch 77/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 744.1881 - val_accuracy: 0.0000e+00 - val_loss: 341.1006\nEpoch 78/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 639.5761 - val_accuracy: 0.0000e+00 - val_loss: 334.1671\nEpoch 79/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.0000e+00 - loss: 733.9393 - val_accuracy: 0.0000e+00 - val_loss: 332.2702\nEpoch 80/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 688.0764 - val_accuracy: 0.0000e+00 - val_loss: 334.4595\nEpoch 81/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 708.3487 - val_accuracy: 0.0000e+00 - val_loss: 328.5920\nEpoch 82/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 688.8785 - val_accuracy: 0.0000e+00 - val_loss: 324.4105\nEpoch 83/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 590.3506 - val_accuracy: 0.0000e+00 - val_loss: 325.1876\nEpoch 84/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 659.8479 - val_accuracy: 0.0000e+00 - val_loss: 328.5043\nEpoch 85/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 879.9040 - val_accuracy: 0.0000e+00 - val_loss: 331.3829\nEpoch 86/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 696.3372 - val_accuracy: 0.0000e+00 - val_loss: 314.9474\nEpoch 87/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 641.8287 - val_accuracy: 0.0000e+00 - val_loss: 310.6088\nEpoch 88/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 759.5827 - val_accuracy: 0.0000e+00 - val_loss: 316.9583\nEpoch 89/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 757.3443 - val_accuracy: 0.0000e+00 - val_loss: 325.9931\nEpoch 90/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 823.4269 - val_accuracy: 0.0000e+00 - val_loss: 327.8417\nEpoch 91/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 653.2341 - val_accuracy: 0.0000e+00 - val_loss: 318.9357\nEpoch 92/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 704.8210 - val_accuracy: 0.0000e+00 - val_loss: 308.7674\nEpoch 93/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 710.7300 - val_accuracy: 0.0000e+00 - val_loss: 304.5253\nEpoch 94/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 671.2079 - val_accuracy: 0.0000e+00 - val_loss: 299.4852\nEpoch 95/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 801.4570 - val_accuracy: 0.0000e+00 - val_loss: 298.8641\nEpoch 96/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - accuracy: 0.0000e+00 - loss: 657.6065 - val_accuracy: 0.0000e+00 - val_loss: 292.7097\nEpoch 97/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 758.5868 - val_accuracy: 0.0000e+00 - val_loss: 294.8198\nEpoch 98/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - accuracy: 0.0000e+00 - loss: 736.0466 - val_accuracy: 0.0000e+00 - val_loss: 301.5949\nEpoch 99/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 639.1855 - val_accuracy: 0.0000e+00 - val_loss: 304.0910\nEpoch 100/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - accuracy: 0.0000e+00 - loss: 583.7020 - val_accuracy: 0.0000e+00 - val_loss: 317.7045\n\n\n&lt;keras.src.callbacks.history.History at 0x7fa8e0639f90&gt;"
  },
  {
    "objectID": "Modeling/corn_supervised_learning.html#model-evaluation-and-performance-comparison",
    "href": "Modeling/corn_supervised_learning.html#model-evaluation-and-performance-comparison",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "4. Model Evaluation and Performance Comparison",
    "text": "4. Model Evaluation and Performance Comparison\nComparing the performance with using R2 and Root Mean Squared Error (RMSE).\n\n4.1. E. Linear Regression (LR)\n\ntrain_predictions = model_lr.predict(X_train)\ntest_predictions = model_lr.predict(X_test)\n\nr2_train_lr = r2_score(y_train, train_predictions)\nr2_test_lr = r2_score(y_test, test_predictions)\n\nrmse_train_lr = mean_squared_error(y_train, train_predictions, squared=False)\nrmse_test_lr = mean_squared_error(y_test, test_predictions, squared=False)\n\nprint(f\"Training R^2: {r2_train_lr:.4f}\")\nprint(f\"Test R^2: {r2_test_lr:.4f}\")\nprint(f\"Training RMSE: {rmse_train_lr:.4f}\")\nprint(f\"Test RMSE: {rmse_test_lr:.4f}\")\n\nTraining R^2: 0.7173\nTest R^2: 0.7732\nTraining RMSE: 18.5641\nTest RMSE: 16.6800\n\n\n\n\n4.2. E. Random Forest Regressor (RF)\n\nr2_train_rf = rf_final.score(X=X_train, y=y_train)\nr2_test_rf = rf_final.score(X=X_test, y=y_test)\n\nprint(\"R2 on the training data:\")\nprint(r2_train_rf)\nprint(\"R2 on the testing data:\")\nprint(r2_test_rf)\n\nR2 on the training data:\n0.9642508395150555\nR2 on the testing data:\n0.8124597634000534\n\n\n\nrmse_train_rf = mean_squared_error(y_train, rf_final.predict(X_train), squared=False)\nrmse_test_rf = mean_squared_error(y_test, rf_final.predict(X_test), squared=False)\n\nprint(\"RMSE on the training data:\")\nprint(rmse_train_rf)\nprint(\"RMSE on the testing data:\")\nprint(rmse_test_rf)\n\nRMSE on the training data:\n6.601176994035226\nRMSE on the testing data:\n15.166271795818888\n\n\n\n# Calculate and plot the feature importance of the RF model\nimp = rfpimp.importances(rf_final, X_test, y_test)\nprint(imp)\nviz = rfpimp.plot_importances(imp)\nviz.view()\n\n                                        Importance\nFeature                                           \nNDVI                                        0.6072\nLST_DAY                                     0.0911\nET                                          0.0833\nPAR                                         0.0537\nSAR                                         0.0277\nPA                                          0.0200\nLST_NIGHT                                   0.0041\n\n\n\n\n\n\n\n4.3. E. Gradient Boosting Regressor (XGB)\n\nmodel_xgb = XGBRegressor(max_depth=gscv_xgb.best_params_['max_depth'], n_estimators=gscv_xgb.best_params_['n_estimators'], random_state=randomState_xgb)\nmodel_xgb.fit(X_train, y_train)\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=2, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=20, n_jobs=None,\n             num_parallel_tree=None, random_state=100, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=2, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=20, n_jobs=None,\n             num_parallel_tree=None, random_state=100, ...)\n\n\n\n# r2_train_xgb, r2_test_xgb, rmse_train_xgb, rmse_test_xgb\nr2_train_xgb = model_xgb.score(X=X_train, y=y_train)\nr2_test_xgb = model_xgb.score(X=X_test, y=y_test)\nrmse_train_xgb = mean_squared_error(y_train, model_xgb.predict(X_train), squared=False)\nrmse_test_xgb = mean_squared_error(y_test, model_xgb.predict(X_test), squared=False)\n\n\nprint(\"R2 on the training data:\")\nprint(r2_train_xgb)\nprint(\"R2 on the testing data:\")\nprint(r2_test_xgb)\n\nR2 on the training data:\n0.8632947936612397\nR2 on the testing data:\n0.7945931877980402\n\n\n\nprint(\"RMSE on the training data:\")\nprint(rmse_train_xgb)\nprint(\"RMSE on the testing data:\")\nprint(rmse_test_xgb)\n\nRMSE on the training data:\n12.908654618048486\nRMSE on the testing data:\n15.872269404332858\n\n\n\nimp_xgb = rfpimp.importances(model_xgb, X_test, y_test) # permutation\nprint(imp_xgb)\nviz_xgb = rfpimp.plot_importances(imp_xgb)\nviz_xgb.view()\n\n                                        Importance\nFeature                                           \nNDVI                                        0.3331\nLST_DAY                                     0.1103\nET                                          0.0842\nLST_NIGHT                                   0.0387\nPAR                                         0.0189\nPA                                          0.0187\nSAR                                        -0.0010\n\n\n\n\n\n\n\n4.4. E. Artificial Neural Network (ANN)\n\n#predictions\ny_pred_train_ann = model_ann.predict(X_train).flatten()\ny_pred_test_ann = model_ann.predict(X_test).flatten()\n\n#Compute R2 and RMSE\nr2_train_ann = np.round(r2_score(y_train, y_pred_train_ann),2)\nr2_test_ann = np.round(r2_score(y_test, y_pred_test_ann),2)\nrmse_train_ann = np.round(np.sqrt(mean_squared_error(y_train, y_pred_train_ann)),2)\nrmse_test_ann = np.round(np.sqrt(mean_squared_error(y_test, y_pred_test_ann)),2)\n\n#print the result\nprint(\"Train R2:\", r2_train_ann)\nprint(\"Test R2:\", r2_test_ann)\nprint(\"Train RMSE:\", rmse_train_ann)\nprint(\"Test RMSE:\", rmse_test_ann)\n\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step\n3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step \nTrain R2: 0.72\nTest R2: 0.73\nTrain RMSE: 18.61\nTest RMSE: 18.14\n\n\n\n#crosscheck the y value between real and predicted\ncrosscheck_y_dict = {\n    'y_test' : y_test,\n    'y_pred' : np.round(y_pred_test_ann,0),\n    'delta' : np.abs(np.round((y_test - y_pred_test_ann),0))\n}\n\n#plotting histogram\ncrosscheck_y_df = pd.DataFrame(crosscheck_y_dict)\nplt.hist(crosscheck_y_df['delta'], bins=10)\nplt.xlabel(f'Delta\\nR2_test = {r2_test_ann}, RMSE_test = {rmse_test_ann}, delta_max = {crosscheck_y_df.delta.max()}')\nplt.ylabel('Frequency (Number of Data)')\nplt.title(f\"Accuracy of ANN Model Based on Delta of Y Test and Y Pred)\")\nplt.show()\n\n\n\n\n\n\n4.5. Model Performance Comparison\n\n#please input your metrics in here\nmetrics_dict = {\n    'metrics': [\"Train R2\",\"Test R2\",\"Train RMSE\",\"Test RMSE\"],\n    'LR': [r2_train_lr, r2_test_lr, rmse_train_lr, rmse_test_lr],\n    'RF': [r2_train_rf, r2_test_rf, rmse_train_rf, rmse_test_rf],\n    'XGB': [r2_train_xgb, r2_test_xgb, rmse_train_xgb, rmse_test_xgb],\n    'ANN': [r2_train_ann, r2_test_ann, rmse_train_ann, rmse_test_ann]\n}\n\n#create dataframe\nmetrics_df = pd.DataFrame(metrics_dict)\nmetrics_df.set_index('metrics')\n\n\n\n\n\n\n\n\nLR\nRF\nXGB\nANN\n\n\nmetrics\n\n\n\n\n\n\n\n\nTrain R2\n0.7173\n0.9643\n0.8633\n0.7200\n\n\nTest R2\n0.7732\n0.8125\n0.7946\n0.7300\n\n\nTrain RMSE\n18.5641\n6.6012\n12.9087\n18.6100\n\n\nTest RMSE\n16.6800\n15.1663\n15.8723\n18.1400\n\n\n\n\n\n\n\nBased on the comparison, it can be said that the best model that can be used for estimating crop yield is []. From this point, [] model would be used for estimation phase (step 5)."
  },
  {
    "objectID": "Modeling/corn_supervised_learning.html#crop-yield-estimation-and-export-result",
    "href": "Modeling/corn_supervised_learning.html#crop-yield-estimation-and-export-result",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "5. Crop Yield Estimation and Export Result",
    "text": "5. Crop Yield Estimation and Export Result"
  },
  {
    "objectID": "CASA0025_Project_Template-main/index.html#project-summary",
    "href": "CASA0025_Project_Template-main/index.html#project-summary",
    "title": "CROPINVEST - Crop Yield Estimator",
    "section": "Project Summary",
    "text": "Project Summary\nCROPINVEST - Crop Yield Estimator for the State of North Dakota, USA"
  },
  {
    "objectID": "CASA0025_Project_Template-main/index.html#problem-statement",
    "href": "CASA0025_Project_Template-main/index.html#problem-statement",
    "title": "CROPINVEST - Crop Yield Estimator",
    "section": "Problem Statement",
    "text": "Problem Statement\nNorth Dakota, a state in the Midwestern United States, is popular with agricultural investors for its abundant agricultural resources and vast farmland. However, the agencies like – banks, crop insurance companies, etc need a tool that can predict the crop yield value of a particular farm to ensure that they can safeguard their investment. In this regard, we create CROPINVEST, a GEE application that monitors the environment to predict crop yields, and based on these yields, we obtain the market value of the crop, which in turn can help end users (banks, crop insurance companies ,etc) to accurately and scientifically capture the farms potential to pay the crop loan amount or the risk associated with the insured crops and enable stakeholders to make informed decisions that optimize agricultural productivity, market strategies, and economic outcomes."
  },
  {
    "objectID": "CASA0025_Project_Template-main/index.html#end-user",
    "href": "CASA0025_Project_Template-main/index.html#end-user",
    "title": "CROPINVEST - Crop Yield Estimator",
    "section": "End User",
    "text": "End User\n\nInsurance Companies\n\nBased on potential crop yields, charge Insurance premiums to farmers thus mitigating financial losses from decreased production.\nManaging financial exposure if low yield or drought conditions prevail, which could lead to substantial claims from farmers.\n\nBanks\n\nUtilize it to evaluate loan feasibility for farmers and adjust interest rates based on predicted yields and associated risks.\n\nFarmer Associations\n\nUtilize it to forecast yields, plan for adverse conditions, and identify areas needing more irrigation.\n\nGovernments\n\nTo anticipate and prepare for potential crop deficits or surpluses at a county level and thus developing policies/infrastructure for the same."
  },
  {
    "objectID": "CASA0025_Project_Template-main/index.html#data",
    "href": "CASA0025_Project_Template-main/index.html#data",
    "title": "CROPINVEST - Crop Yield Estimator",
    "section": "Data",
    "text": "Data\n\nDatasets\n\nA1 - United States Census Bureau TIGER Dataset: Boundary information for U.S. counties.\nA2 - USDA NASS Cropland Data Layers: Crop-specific Land cover data.\nA3 - MOD16A2GF.061 Terra Net Evapotranspiration: Total evapotranspiration over land.\nA4 - MOD11A1.061 Terra Land Surface Temperature: Land surface temperature data.\nA5 - MOD13Q1.061 Terra Vegetation Indices 16-Day Global: Vegetation indices.\nA6 - GRIDMET: Gridded Surface Meteorological Dataset: Meteorological data.\nA7 - MCD18C2.061 Photosynthetically Active Radiation Daily 3 hour: Solar Radiation Levels.\nA8 - SPL3SMP_E.005 SMAP L3 Radiometer Global Daily 9km Soil Moisture: Soil moisture.\nA9 - Crop Yield Data: Contains county-wise crop yield data.USDA\nA10 - Current Crop Price Data: Uses current crop price data from Financial Times\n\n\n\nData Variables Overview\n\n\n\n\n\n\n\n\n\nData Type\nDataset\nDescription\nData Used\n\n\n\n\nGEE DATA\nA1\nCounty wise Boundary Information\nSTATEFP\n\n\nGEE DATA\nA2\nCrop-specific Land Cover Data Type\ncropland\n\n\nGEE DATA\nA3\nEvapo-Transpiration\nET\n\n\nGEE DATA\nA4\nLand Surface Temperature Day\nLST_Day_1km\n\n\nGEE DATA\nA4\nLand Surface Temperature Night\nLST_Night_1km\n\n\nGEE DATA\nA5\nNormalized Difference Vegetation Index\nNDVI\n\n\nGEE DATA\nA6\nPrecipitation\npr\n\n\nGEE DATA\nA7\nPhotosynthetically Active Radiation\nGMT_1200_PAR\n\n\nGEE DATA\nA8\nDay Soil Moisture\nsoil_moisture_am\n\n\nGEE DATA\nA8\nNight Soil Moisture\nsoil_moisture_pm\n\n\nOTHER DATA\nA9\nUSDA Crop Yield (Year Wise)\nYield\n\n\nOTHER DATA\nA10\nCrop Day Price $ - Financial Times\nPrice"
  },
  {
    "objectID": "CASA0025_Project_Template-main/index.html#methodology",
    "href": "CASA0025_Project_Template-main/index.html#methodology",
    "title": "CROPINVEST - Crop Yield Estimator",
    "section": "Methodology",
    "text": "Methodology\nWe explored machine learning techniques to forecast crop yields, evaluating models like Linear Regression, Random Forest, Gradient Boosting, and Neural Networks based on their R2 and RMSE scores ,and found RF best fit for each crop. Previous studies showed that RF is an effective and universal machine-learning method for crop yield prediction on a regional and global scale with high accuracy, precision, and ease of use (Jeong et al., 2016.; Prasad et al., 2021).\n\nBuilding Random Forest Model:\n\nPrepare original CSV file\n\nincluding the three crops (corn, wheat, soybean) among several X variables and Y variable (the crop yield).\n\n\nPrepare Training Data(80%) / Validation Data(20%)\nUse the Training data to train three different Random Forest Regression Models in GEE\n\n\n\n\nValidation\nTo get the performance of our models, we can use the test data from the previous split. We used R square and Root Mean Squared Error (RMSE) to validate our models. Some graphs showing these metrics are below:"
  },
  {
    "objectID": "CASA0025_Project_Template-main/index.html#interface",
    "href": "CASA0025_Project_Template-main/index.html#interface",
    "title": "CROPINVEST - Crop Yield Estimator",
    "section": "Interface",
    "text": "Interface\nCROPINVEST supports insurance companies, banks, and farmer associations with advanced features for forecasting crop yields in North Dakota for wheat, corn, and soybeans. Users can select different Crops and Years and utilize two key functionalities: a County feature to display crop yields, planting area, total production, and county-wide crop price totals via clickable maps; and a custom Area feature allowing users to draw specific regions to analyze crop yields, areas planted, and aggregate pricing. This interface ensures banks and insurance companies can assess financial risks effectively, while farmers gain crucial insights into expected yields and market conditions. Here is brief overview of how our application works:"
  },
  {
    "objectID": "CASA0025_Project_Template-main/index.html#the-application",
    "href": "CASA0025_Project_Template-main/index.html#the-application",
    "title": "CROPINVEST - Crop Yield Estimator",
    "section": "The Application",
    "text": "The Application"
  },
  {
    "objectID": "CASA0025_Project_Template-main/index.html#how-it-works",
    "href": "CASA0025_Project_Template-main/index.html#how-it-works",
    "title": "CROPINVEST - Crop Yield Estimator",
    "section": "How it Works",
    "text": "How it Works\n\nData Extraction Code:\n\nHere we use the Python environment to extract the data from different datasets using Google Earth Engine API\nAfterwards, we first use the crop-specific land cover data to distinguish different crops- wheat, soybean or corn for each county in North Dakota.\nFurther, NDVI, Precipitation, SAR, Soil Moisture & other values are used to get the county-wise values from the year 2000-2024.\nAfterwards, the Yield data is obtained from the United States Department of Agriculture for each of the years and a final dataset is obtained which has all the X Variables (NDVI,PA,SMS_AM,LST_DAY,SMS_PM,LST_NIGHT,PAR,ET) & Y variable (YIELD).\n\n\npip install earthengine-api\nimport ee\nee.AuthcessYear(year):\n    # Load the CDL dataset for the given year\n    dataset = ee.ImageCollection('USDA/NASS/CDL')\\\n                .filter(ee.Filter.date(f'{year}-01-01', f'{year}-12-31'))\\\n                .first()\n    crop_landcover = dataset.select('cropland')\n\n    # Filter for North Dakota counties\n    #`STATEFP` parameter of the dataset which provides the State FIPS code & the North Dakota value is used.\n    counties = ee.FeatureCollection('TIGER/2016/Counties')\n    nd = counties.filter(ee.Filter.eq('STATEFP', '38'))\n    \n    # Identify corn areas in North Dakota\n    #`cropland` values for different crops of our study are used Wheat, Corn & Soybean Values provided from the Cropland Table.\n    corn = crop_landcover.eq(1).Or(crop_landcover.eq(12)).Or(crop_landcover.eq(13))\n    masked_corn = crop_landcover.updateMask(corn).clipToCollection(nd)\n\n    # Calculate NDVI for corn areas using MODIS data\n    #`NDVI` parameter of the dataset and we obtain the mean over the growth period of the crop\n    NDVI_dataset = ee.ImageCollection('MODIS/061/MOD13Q1')\\\n                    .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\n    ndvi = NDVI_dataset.select('NDVI')\n    mean_ndvi = ndvi.mean().rename('NDVI')\n    cornNDVI = mean_ndvi.updateMask(masked_corn)\n    \n    # Calculate precipitation using GRIDMET data\n    #`pr` parameter of the dataset which provides the 'Precipitation amount' in mm (daily total)\n    precipitation_dataset = ee.ImageCollection(\"IDAHO_EPSCOR/GRIDMET\")\\\n                             .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\\\n                             .select('pr')\n    mean_precipitation = precipitation_dataset.mean().rename('PA')\n\n    # Load Radiometer Global Daily 9 km Soil Moisture AM\n    #`soil_moisture_am` & `soil_moisture_pm` parameter of the dataset which provides 'Retrieved soil moisture estimate from the\n    # disaggregated/downscaled vertical polarization brightness temperature at 9-km grid cell one at AM overpass & other at  PM overpass. in dB.\n    smap_dataset = ee.ImageCollection(\"NASA/SMAP/SPL3SMP_E/005\")\\\n                    .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\\\n                    .select('soil_moisture_am')\n    mean_soil_moisture = smap_dataset.mean().rename('SMS_AM')\n    \n    # Load Radiometer Global Daily 9 km Soil Moisture PM\n    smapDataset_pm = ee.ImageCollection(\"NASA/SMAP/SPL3SMP_E/005\")\\\n                       .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\\\n                       .select('soil_moisture_pm') \n    meanSoilMoisture_pm = smapDataset_pm.mean().rename('SMS_PM')\n    \n    # Load MODIS Land Surface Temperature DAY\n    #`LST_Day_1km` & `LST_Night_1km` parameter of the dataset which provides 'Daytime Land Surface Temperature' &\n    # Nighttime Land Surface Temperature' both in Kelvin (K).\n    lstDataset = ee.ImageCollection(\"MODIS/061/MOD11A1\")\\\n                   .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\n \n    lstmean_celsius = lstDataset.select('LST_Day_1km')\\\n                                .mean()\\\n                                .multiply(0.02)\\\n                                .subtract(273.15)\\\n                                .rename('LST_DAY')\n    # Load MODIS Land Surface Temperature NIGHT\n    lstDataset_night = ee.ImageCollection(\"MODIS/061/MOD11A1\")\\\n                         .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\n \n    lstmean_celsius_night = lstDataset_night.select('LST_Night_1km')\\\n                                              .mean()\\\n                                              .multiply(0.02)\\\n                                              .subtract(273.15)\\\n                                              .rename('LST_NIGHT')\n                         \n    # Photosynthetically Active Radiation Daily 3-Hour \n    #`GMT_1200_PAR` parameter of the dataset which provides 'Total PAR at GMT 12:00'. PAR is incident solar radiation in\n    # the visible spectrum (400-700 nanometers) and is an important variable in land-surface models having use in agriculture &\n    # other scientific applications.\n    par_12 = ee.ImageCollection(\"MODIS/061/MCD18C2\")\\\n               .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\\\n               .select('GMT_1200_PAR')\n                        \n    mean_par_12 = par_12.mean().rename('PAR'); # Calculate the Photosynthetically Active Radiation at 12\n\n                         \n    # Net Evapotranspiration\n    # `ET` parameter of the dataset which provides 'Total evapotranspiration' in kg/m^2/8day.s.\n    netevapo = ee.ImageCollection(\"MODIS/061/MOD16A2GF\")\\\n                 .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\\\n                 .select('ET')\n                    \n    mean_netevapo = netevapo.mean().rename('ET')  # Calculate the mean Soil Moisture\n\n\n    # Combine all layers\n    combinedDataset = cornNDVI.addBands(mean_precipitation).addBands(mean_s1_vv).addBands(mean_soil_moisture).addBands(lstmean_celsius).addBands(meanSoilMoisture_pm).addBands(lstmean_celsius_night).addBands(mean_par_12).addBands(mean_netevapo)\n\n    # Reduce regions and calculate mean values over the specified areas\n    combined_mean = combinedDataset.reduceRegions(\n        collection=nd,\n        reducer=ee.Reducer.mean(),\n        scale=30,\n        tileScale=4,\n    )\n\n    # Define export parameters\n    export_params = {\n        'collection': combined_mean,\n        'description': f'combined_{year}',\n        'folder': 'GEE_Folder',\n        'fileNamePrefix': f'Combined_{year}',\n        'fileFormat': 'CSV',\n        'selectors': ['NAME', 'GEOID', 'NDVI', 'PA', 'SAR', 'SMS_AM', 'LST_DAY', 'SMS_PM', 'LST_NIGHT', 'PAR', 'ET']\n    }\n\n    # Commented the line below as I have got the data in my drive already\n    #ee.batch.Export.table.toDrive(**export_params).start()\n\n# Example of processing each year\nfor year in range(2000, 2024):\n    processYear(year)\n\n\nMethodology Code:\n\nFirst, we experimented with various machine learning methods in Python, including Linear Regression (LR), Random Forest Regressor (RF), Gradient Boosting Regressor (XGB), and Artificial Neural Network (ANN). By comparing their R2 and RMSE scores, we identified the most suitable method for each of the three crops.\nConsidering the impact of environmental factors on crop growth within a specified time frame, we initially explored all nine parameters. Subsequently, based on the importance tests from the Random Forest model, five to six key variables were selected as independent variables in the predictive models for each crop.\nIn GEE, we trained Random Forest (RF) predictive models separately for three types of crops using data from six years, spanning 2018 to 2023, and split them into training/validation datasets. After obtaining the models, we assessed their performance using R2 and RMSE.\nUsing the historical average method to make a simple prediction for the crop yield of the coming year.\n\n\nPart 1\n#install packages\n!pip install tensorflow\n\n#load packages\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n#packages for manipulating dataframe\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport sklearn\n\n#packages for machine learning\n##train-test-split\nfrom sklearn.model_selection import train_test_split, GridSearchCV, validation_curve\n\n##method 1: Linear Regression (LR)\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import cross_val_score\n\n##method 2: Random Forest Regressor (RF)\nimport rfpimp\nfrom sklearn.ensemble import RandomForestRegressor\n\n##method 3: Gradient Boosting Regressor (XGB)\nimport xgboost\nfrom xgboost import XGBRegressor\n\n##method 4: Artificial Neural Network (ANN)\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n##cross validation\n\n##evaluation metrics (R2 and RMSE)\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n#data visualization\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_rows', 300) # specifies number of rows to show\npd.options.display.float_format = '{:40,.4f}'.format # specifies default number format to 4 decimal places\nplt.style.use('ggplot') # specifies that graphs should use ggplot styling\n%matplotlib inline\n\n# 1. Load & Cleaning Data\n#load data\nsoybean_2018 = pd.read_csv('https://www.dropbox.com/scl/fi/ixibqk9pyuehwvoz7mr1h/2018_County_Summary_Merged.csv?rlkey=wueodqdvzsht5or4eftwdd3ys&dl=1')\nsoybean_2019 = pd.read_csv('https://www.dropbox.com/scl/fi/x3oiqrikr0xagqrh5tiqu/2019_County_Summary_Merged.csv?rlkey=zr5wru8sg5iybp43lnjf7ls5a&dl=1')\nsoybean_2020 = pd.read_csv('https://www.dropbox.com/scl/fi/m4r74ydgw3yno93nakagl/2020_County_Summary_Merged.csv?rlkey=kmd8bozo9z9jxznoa775gg33z&dl=1')\nsoybean_2021 = pd.read_csv('https://www.dropbox.com/scl/fi/2vmvsyjloz9hvzejtdez4/2021_County_Summary_Merged.csv?rlkey=ossgr4x3fvzgebhqprchntt2f&dl=1')\nsoybean_2022 = pd.read_csv('https://www.dropbox.com/scl/fi/x61224yvwtq4idnqphwjy/2022_County_Summary_Merged.csv?rlkey=s3zm9ooap8pjqom3jr0aklc6t&dl=1')\nsoybean_2023 = pd.read_csv('https://www.dropbox.com/scl/fi/zrjnmeqysrokfsua24hb3/2023_County_Summary_Merged.csv?rlkey=jb4w1pt295zeagbvgm7c2t7pk&dl=1')\n\nsoybean_list = [soybean_2018, soybean_2019, soybean_2020, soybean_2021, soybean_2022, soybean_2023]\nsoybean_df = pd.concat(soybean_list)\nsoybean_df = soybean_df.drop(['NAME','GEOID','SMS_PM','SMS_AM','SAR'], axis=1)\nsoybean_df.info()\n\n# 2.1 Train & Test Data Split\n#split the dataset\nX = soybean_df.drop('YIELD', axis=1)\ny = soybean_df['YIELD']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=100)\n\n# 2.2 Train & Test Data Standarderlized\ndef standardize_columns(file_name, columns_to_standardize):\n    scaler = StandardScaler()\n\n    df = file_name\n    df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n    return df\n\ndef standardize_series(series):\n    scaler = StandardScaler()\n    series = scaler.fit_transform(series.values.reshape(-1, 1)).flatten()\n    return series\n    \nX_columns = ['LST_DAY','PA','NDVI','ET','LST_NIGHT','PAR']\ny_columns = ['YIELD']\n\nX_train = standardize_columns(X_train, X_columns)\nX_test = standardize_columns(X_test, X_columns)\n\n# 3. Model Training and Parameter Tuning\n# 3.1. Linear Regression (LR)\nmodel_lr = LinearRegression()\n\n# Cross validation\nscores = cross_val_score(model_lr, X, y, cv=5, scoring='neg_mean_squared_error')\n\nmean_mse = np.mean(scores)\nstd_mse = np.std(scores)\n\nprint(f'Mean MSE: {mean_mse}')\nprint(f'Standard Deviation of MSE: {std_mse}')\nmodel_lr.fit(X_train, y_train)\n\n# 3.2. Random Forest Regressor (RF)\n# values of max_depth and min_samples_split\nhyperparameters = {'max_depth':[3,5,10,20,30], 'min_samples_split':[2,4,6,8,10]}\n\n\nrandomState_dt = 10000\nmodel_rf = RandomForestRegressor(random_state=randomState_dt)\n\n# cv=5 by default, which means 5-fold cross-validation\nclf = GridSearchCV(model_rf, hyperparameters)\n\nclf.fit(X_train, y_train)\n\n# we can query the best parameter value and its accuracy score\nprint (\"The best parameter value is: \")\nprint (clf.best_params_)\nprint (\"The best score is: \")\nprint (clf.best_score_)\n\n# Train the final RF\nrf_final = RandomForestRegressor(max_depth=clf.best_params_['max_depth'], min_samples_split=clf.best_params_['min_samples_split'], random_state=randomState_dt)\nrf_final.fit(X_train, y_train)\n\n# 3.3. Gradient Boosting Regressor (XGB)\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n# values of max_depth and min_samples_split\nhyperparameters = {'max_depth':[2,4,6,8,10], 'n_estimators':[4,8,12,16,20]}\n\nrandomState_xgb = 125\nxgb = XGBRegressor(random_state=randomState_xgb)\n\n# cv=5 by default, which means 5-fold cross-validation\ngscv_xgb = GridSearchCV(xgb, hyperparameters)\n\ngscv_xgb.fit(X_train, y_train)\n\n# we can query the best parameter value and its accuracy score\nprint (\"The best parameter value is: \")\nprint (gscv_xgb.best_params_)\nprint (\"The best score is: \")\nprint (gscv_xgb.best_score_)\n\n# 3.4. Artificial Neural Network (ANN)\nmodel_ann = keras.Sequential([\n    layers.Input(shape=(6,)),  # Input layer\n    layers.Dense(128, activation='relu'),  # Hidden layer with ReLU activation\n    layers.Dropout(0.5),  # Dropout layer for regularization\n    layers.Dense(64, activation='relu'),  # Additional hidden layer\n    layers.Dropout(0.3),  # Another dropout layer\n    layers.Dense(1)  # Output layer\n])\n\n#measuring the training with certain metrics\nmodel_ann.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n\n#train the model\nmodel_ann.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n\n# 4. Model Evaluation and Performance Comparison\n# 4.1. E. Linear Regression (LR)\ntrain_predictions = model_lr.predict(X_train)\ntest_predictions = model_lr.predict(X_test)\n\nr2_train_lr = r2_score(y_train, train_predictions)\nr2_test_lr = r2_score(y_test, test_predictions)\n\nrmse_train_lr = mean_squared_error(y_train, train_predictions, squared=False)\nrmse_test_lr = mean_squared_error(y_test, test_predictions, squared=False)\n\nprint(f\"Training R^2: {r2_train_lr:.4f}\")\nprint(f\"Test R^2: {r2_test_lr:.4f}\")\nprint(f\"Training RMSE: {rmse_train_lr:.4f}\")\nprint(f\"Test RMSE: {rmse_test_lr:.4f}\")\n\n# 4.2. E. Random Forest Regressor (RF)\nr2_train_rf = rf_final.score(X=X_train, y=y_train)\nr2_test_rf = rf_final.score(X=X_test, y=y_test)\n\nprint(\"R2 on the training data:\")\nprint(r2_train_rf)\nprint(\"R2 on the testing data:\")\nprint(r2_test_rf)\n\nrmse_train_rf = mean_squared_error(y_train, rf_final.predict(X_train), squared=False)\nrmse_test_rf = mean_squared_error(y_test, rf_final.predict(X_test), squared=False)\n\nprint(\"RMSE on the training data:\")\nprint(rmse_train_rf)\nprint(\"RMSE on the testing data:\")\nprint(rmse_test_rf)\n\n# Calculate and plot the feature importance of the RF model\nimp = rfpimp.importances(rf_final, X_test, y_test)\nprint(imp)\nviz = rfpimp.plot_importances(imp)\nviz.view()\n\n# 4.3. E. Gradient Boosting Regressor (XGB)\nmodel_xgb = XGBRegressor(max_depth=gscv_xgb.best_params_['max_depth'], n_estimators=gscv_xgb.best_params_['n_estimators'], random_state=randomState_xgb)\nmodel_xgb.fit(X_train, y_train)\n\n# r2_train_xgb, r2_test_xgb, rmse_train_xgb, rmse_test_xgb\nr2_train_xgb = model_xgb.score(X=X_train, y=y_train)\nr2_test_xgb = model_xgb.score(X=X_test, y=y_test)\nrmse_train_xgb = mean_squared_error(y_train, model_xgb.predict(X_train), squared=False)\nrmse_test_xgb = mean_squared_error(y_test, model_xgb.predict(X_test), squared=False)\n\nprint(\"R2 on the training data:\")\nprint(r2_train_xgb)\nprint(\"R2 on the testing data:\")\nprint(r2_test_xgb)\n\nprint(\"RMSE on the training data:\")\nprint(rmse_train_xgb)\nprint(\"RMSE on the testing data:\")\nprint(rmse_test_xgb)\n\nimp_xgb = rfpimp.importances(model_xgb, X_test, y_test) # permutation\nprint(imp_xgb)\nviz_xgb = rfpimp.plot_importances(imp_xgb)\nviz_xgb.view()\n\n# 4.4. E. Artificial Neural Network (ANN)\n#predictions\ny_pred_train_ann = model_ann.predict(X_train).flatten()\ny_pred_test_ann = model_ann.predict(X_test).flatten()\n\n#Compute R2 and RMSE\nr2_train_ann = np.round(r2_score(y_train, y_pred_train_ann),2)\nr2_test_ann = np.round(r2_score(y_test, y_pred_test_ann),2)\nrmse_train_ann = np.round(np.sqrt(mean_squared_error(y_train, y_pred_train_ann)),2)\nrmse_test_ann = np.round(np.sqrt(mean_squared_error(y_test, y_pred_test_ann)),2)\n\n#print the result\nprint(\"Train R2:\", r2_train_ann)\nprint(\"Test R2:\", r2_test_ann)\nprint(\"Train RMSE:\", rmse_train_ann)\nprint(\"Test RMSE:\", rmse_test_ann)\n\n#crosscheck the y value between real and predicted\ncrosscheck_y_dict = {\n    'y_test' : y_test,\n    'y_pred' : np.round(y_pred_test_ann,0),\n    'delta' : np.abs(np.round((y_test - y_pred_test_ann),0))\n}\n\n#plotting histogram\ncrosscheck_y_df = pd.DataFrame(crosscheck_y_dict)\nplt.hist(crosscheck_y_df['delta'], bins=10)\nplt.xlabel(f'Delta\\nR2_test = {r2_test_ann}, RMSE_test = {rmse_test_ann}, delta_max = {crosscheck_y_df.delta.max()}')\nplt.ylabel('Frequency (Number of Data)')\nplt.title(f\"Accuracy of ANN Model Based on Delta of Y Test and Y Pred)\")\nplt.show()\n\n# 4.5. Model Performance Comparison\n#please input your metrics in here\nmetrics_dict = {\n    'metrics': [\"Train R2\",\"Test R2\",\"Train RMSE\",\"Test RMSE\"],\n    'LR': [r2_train_lr, r2_test_lr, rmse_train_lr, rmse_test_lr],\n    'RF': [r2_train_rf, r2_test_rf, rmse_train_rf, rmse_test_rf],\n    'XGB': [r2_train_xgb, r2_test_xgb, rmse_train_xgb, rmse_test_xgb],\n    'ANN': [r2_train_ann, r2_test_ann, rmse_train_ann, rmse_test_ann]\n}\n\n#create dataframe\nmetrics_df = pd.DataFrame(metrics_dict)\nmetrics_df.set_index('metrics')\n\n\nPart 2\n\n//  ========================== GEE  ========================================\n// Import CSV data  \n// Load data from CSV file\nvar csvFile = ee.FeatureCollection('projects/ee-ucfnfli/assets/corn_df');\n\n// ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B9', 'B10', 'B11', 'BQA']\nvar bands = ['NDVI', 'LST_DAY', 'ET', 'PAR', 'PA'];\n\n\n//  ========================== train the RF classification model  ========================================\n// Split the training and testing data\nvar withRandom = csvFile.randomColumn({\n  columnName:'random',\n  seed:2,\n  distribution: 'uniform',\n});\n\n\nvar split = 0.8; \nvar trainingData = withRandom.filter(ee.Filter.lt('random', split));\nvar validationData = withRandom.filter(ee.Filter.gte('random', split));\n\n\n// var classifier = ee.Classifier.smileRandomForest(100, null, 8, 0.5, null, 125)\nvar classifier = ee.Classifier.smileRandomForest(100, null, 1, 0.7, null, 0).setOutputMode('REGRESSION')\n    .train({\n      features: trainingData, \n      classProperty: 'YIELD', \n      inputProperties: bands\n    });\n//print(classifier);\n\n// =================================== img  ======================\n\nvar featureTest = trainingData.select(['NDVI', 'LST_DAY', 'ET', 'PAR', 'PA','YIELD']);\n\nvar classifiedFeatures = featureTest.classify(classifier,'YIELDPredicted');\n\n\n// ============================ Validation  =================================\n// ============================ variable importance  =================================\nvar dict = classifier.explain();\nprint(\"Classifier information:\", dict);\nvar variableImportance = ee.Feature(null, ee.Dictionary(dict)\n  .get('importance'));\n// Make chart, print it\nvar chart =\n  ui.Chart.feature.byProperty(variableImportance)\n  .setChartType('ColumnChart')\n  .setOptions({\n    title: 'Random Forest Variable Importance',\n    legend: {\n      position: 'none'\n    },\n    hAxis: {\n      title: 'Bands'\n    },\n    vAxis: {\n      title: 'Importance'\n    }\n  });\nprint(chart);\n\n// ===================================  Compute RSME R2 =====================\n// Separate the observed (REDOX_CM) and predicted (regression) properties\n// var sampleTraining = predictedTraining.select(['Yield', 'predicted']);\n// Create chart, print it\nvar chartTraining = ui.Chart.feature.byFeature({features:classifiedFeatures, xProperty:'YIELD', yProperties:['YIELDPredicted']})\n  .setChartType('ScatterChart')\n  .setOptions({\n    title: 'Predicted vs Observed - Training data ',\n    hAxis: {\n      'title': 'observed'\n    },\n    vAxis: {\n      'title': 'predicted'\n    },\n    pointSize: 3,\n    trendlines: {\n      0: {\n        showR2: true,\n        visibleInLegend: true\n      },\n      1: {\n        showR2: true,\n        visibleInLegend: true\n      }\n    }\n  });\nprint(chartTraining);\n// Training data\n// R2 = 0.964;  RMSE = 6.601 in python\n// R2 = 0.948;  RMSE = 8.396 in GEE\n\n// Compute RSME\n// Get array of observation and prediction values \nvar observationTraining = ee.Array(classifiedFeatures.aggregate_array('YIELD'));\nvar predictionTraining = ee.Array(classifiedFeatures.aggregate_array('YIELDPredicted'));\n//print('observationTraining', observationTraining)\n//print('predictionTraining', predictionTraining)\n\n// Compute residuals\nvar residualsTraining = observationTraining.subtract(predictionTraining);\n//print('residualsTraining', residualsTraining)\n// Compute RMSE with equation, print it\nvar rmseTraining = residualsTraining.pow(2)\n  .reduce('mean', [0])\n  .sqrt();\nprint('Training RMSE', rmseTraining); \n\n\n// =============================== Validation =================================\n\nvar featureValidation = validationData.select(['NDVI', 'LST_DAY', 'ET', 'PAR', 'PA','YIELD']);\n//  Validation data \nvar classifiedValidation = featureValidation.classify(classifier,'YIELDPredicted');\n// print result\n//print('ValidRegressionResult:', classifiedValidation);\n\nvar sampleValidation = classifiedValidation.select(['YIELD', 'YIELDPredicted']);\n// Create chart, print it\nvar chartValidation = ui.Chart.feature.byFeature(sampleValidation, 'YIELD', 'YIELDPredicted')\n  .setChartType('ScatterChart')\n  .setOptions({\n    title: 'Predicted vs Observed - Validation data',\n    hAxis: {\n      'title': 'predicted'\n    },\n    vAxis: {\n      'title': 'observed'\n    },\n    pointSize: 3,\n    trendlines: {\n      0: {\n        showR2: true,\n        visibleInLegend: true\n      },\n      1: {\n        showR2: true,\n        visibleInLegend: true\n      }\n    }\n  });\nprint(chartValidation);\n\n// Get array of observation and prediction values \nvar observationValidation = ee.Array(sampleValidation.aggregate_array('YIELD'));\nvar predictionValidation = ee.Array(sampleValidation.aggregate_array('YIELDPredicted'));\n// Compute residuals\nvar residualsValidation = observationValidation.subtract(predictionValidation);\n// Compute RMSE with equation, print it\nvar rmseValidation = residualsValidation.pow(2)\n  .reduce('mean', [0])\n  .sqrt();\nprint('Validation RMSE', rmseValidation);\n// Testing data\n// R2 = 0.812;  RMSE = 15.166 in python\n// R2 = 0.841;  RMSE = 12.990 in GEE\n// ============================== merge raster data ============================\n// Merge \nvar yield2021 = processYear(2021);\nvar yield2022 = processYear(2022);\nvar yield2023 = processYear(2023);\n// Concatenate two images into one multi-band image.\nvar mergeYield = ee.Image.cat([yield2021, yield2022, yield2023]);\nprint('mergeImg:', mergeYield);\n\n// Obtain the union of all images in the image collection and sum them\n// sum all three years yields\nvar summedImage = mergeYield.reduce(ee.Reducer.sum());\nprint(summedImage)\n\n// Print the results\nprint('sumImg:', summemage);\n// Map.addLayer(summedImage,{}, 'TotalYield2024');\n\nvar yield2024 = summedImage.divide(3);\nprint('yield2024:', yield2024);\nMap.addLayer(yield2024,{}, 'yield2024');\n\n\n\n\n\nInterface Code:\n\nIn this section, we perform calculations on the crop layers obtained from the random forest prediction model to generate various outputs. By defining global variables, we dynamically update the crop layers based on the user’s selected year and crop type. The core functionality lies in the computation logic for the County level and Area level modes.\nIn the County level mode, a click event listener is created to provide real-time feedback on the user-selected county. The number of pixels within the county is calculated, enabling the estimation of the crop area. Utilizing historical and predicted average yield data and the current year’s average market price, the total production and total value are computed.\nIn the Area level mode extends the calculation scope to a user-defined polygon region, with a similar computation method to the County level mode. The “Clear Selected Area Button” allows users to remove unnecessary polygons in the Area level mode, enhancing usability.\nThe Interface Code is as follows :\n\n\n// ——————————————————————————define crop layers————————————————————————————————\n\nvar cropLayers = {\n  Corn: {\n    // Add layers of corn\n    '2018': ee.Image(\"projects/ee-songzimeng/assets/corn2018\"),\n    '2019': ee.Image(\"projects/ee-songzimeng/assets/corn2019\"),\n    '2020': ee.Image(\"projects/ee-songzimeng/assets/corn2020\"),\n    '2021': ee.Image(\"projects/ee-songzimeng/assets/corn2021\"),\n    '2022': ee.Image(\"projects/ee-songzimeng/assets/corn2022\"),\n    '2023': ee.Image(\"projects/ee-songzimeng/assets/corn2023\"),\n    '2024': ee.Image(\"projects/ee-songzimeng/assets/corn2024\")\n    \n  },\n  \n  Soybean: {\n    // Add layers of soybean\n\n    '2018': ee.Image(\"projects/ee-songzimeng/assets/soybean2018\"),\n    '2019': ee.Image(\"projects/ee-songzimeng/assets/soybean2019\"),\n    '2020': ee.Image(\"projects/ee-songzimeng/assets/soybean2020\"),\n    '2021': ee.Image(\"projects/ee-songzimeng/assets/soybean2021\"),\n    '2022': ee.Image(\"projects/ee-songzimeng/assets/soybean2022\"),\n    '2023': ee.Image(\"projects/ee-songzimeng/assets/soybean2023\"),\n    '2024': ee.Image(\"projects/ee-songzimeng/assets/soybean2024\")\n  },\n  \n  Wheat: {\n    // Add layers of wheat\n\n    '2018': ee.Image(\"projects/ee-songzimeng/assets/wheat2018\"),\n    '2019': ee.Image(\"projects/ee-songzimeng/assets/wheat2019\"),\n    '2020': ee.Image(\"projects/ee-songzimeng/assets/wheat2020\"),\n    '2021': ee.Image(\"projects/ee-songzimeng/assets/wheat2021\"),\n    '2022': ee.Image(\"projects/ee-songzimeng/assets/wheat2022\"),\n    '2023': ee.Image(\"projects/ee-songzimeng/assets/wheat2023\"),\n    '2024': ee.Image(\"projects/ee-songzimeng/assets/wheat2024\")\n  }\n};\n\n// -------------------------- Data  ------------------------------\nMap.setCenter(-100.55, 47.5, 7);\nMap.setOptions('SATELLITE');\n\n// clip the north dakota\nvar counties = ee.FeatureCollection('TIGER/2016/Counties');\nvar nd = counties.filter(ee.Filter.eq('STATEFP', '38'));\n\n// Formatted county name function\nvar nd = nd.map(function(feature) {\n  var name = ee.String(feature.get('NAME')).toUpperCase().replace(' ', '', 'g');\n  return feature.set('NAME', name);\n});\n\n// Show the county boundary\nvar ndCounties = ee.Image().byte().paint({\n  featureCollection: nd,\n  color: null, \n  width: 1\n});\n\n// Add the counties layer\nMap.addLayer(ndCounties, {}, 'ND Counties');\n\n/// ——————————————Function and global variables——————————————————————————\n// Function to read csv\nfunction readCsvFile(selectedYear, selectedCrop) {\n  var fileName = selectedYear +'_'+ selectedCrop;\n  var csvFile = ee.FeatureCollection('projects/ee-songzimeng/assets/' + fileName); \n\n  return csvFile;\n}\n\n// Function to fomat county name\nfunction processCountyColumn(table) {\n  var countyColumnName = 'County';\n  function processCountyName(countyName) {\n    return ee.String(countyName).toUpperCase().replace('\\\\s+', '');\n  }\n  \n  var processedCountyColumn = table.map(function(feature) {\n    var countyName = feature.get(countyColumnName);\n    var processedCountyName = processCountyName(countyName);\n    return feature.set(countyColumnName, processedCountyName);\n  });\n  \n  // return FeatureCollection\n  return processedCountyColumn;\n}\n\nvar selectedCrop='Select...';\nvar selectedYear='Select...';\nvar soybeanPrice = 11.90; // 2024 average\nvar CornPrice = 41.68; // 2024 average\nvar wheatPrice = 6.07; // 2024 average\nvar cropPrice = 0; //\n\nvar crops = {\n  'Corn': 1,\n  'Wheat': 23,\n  'Soybean': 5\n};\n\n\n// ————————————————interface——————————————————————————\n// set default year\nvar defaultYear = '2018';\n\nvar cropYieldLayer = null;\n\nvar statsLabel_1 = ui.Label('Click on County to see info:');\nvar statsLabel_2 = ui.Label('Select an area to see info:');\n\n// set original info status\nstatsLabel_1.style().set('shown', true);\nstatsLabel_2.style().set('shown', false);\n\n// Clear button to remove all selected layers\nvar drawingTools = Map.drawingTools();\nvar clearButton = ui.Button({\n  label: 'Clear Selected Area',\n  onClick: function() {\n\n    var layers = drawingTools.layers();\n\n    layers.forEach(function(layer) {\n      drawingTools.layers().remove(layer);\n    });\n\n    resultsPanel.clear();\n  },\n  style: {margin: '10px'}\n});\n\n\n// the main panel to select mode, year, croptype\nvar panel = ui.Panel({\n  widgets: [\n    \n    ui.Label('North Dakota Crop Yield', {\n      fontWeight: 'bold',\n      fontSize: '22px',\n      textAlign: 'center',\n      stretch: 'horizontal'\n      \n    }),\n    \n    ui.Label('Select Mode:'),\n    ui.Select({\n      items: ['Select...','County Level', 'Area Level'],\n      value: 'Select...',\n      onChange: function(mode) {\n        \n        // operate different \n        if (mode === 'County Level') {\n          // County Level\n          statsLabel_1.style().set('shown', true);\n          statsLabel_2.style().set('shown', false);\n          \n          // reset button\n          panel.remove(clearButton);\n          panel.add(clearButton);\n          \n          // ban polygon drawing selection\n          var drawingTools = Map.drawingTools();\n          drawingTools.setShown(false);\n          \n          //Function for getting value from image\n          var getCalculation = function(countyName, cropYieldLayer) {\n            var county = nd.filter(ee.Filter.eq('NAME', countyName)).first();\n            var countyGeometry = county.geometry();\n            \n             //print(selectedYear, selectedCrop);\n            var countyData=readCsvFile(selectedYear, selectedCrop);\n            // print(countyData);\n            countyData = processCountyColumn(countyData);\n            \n            resultsPanel.clear();\n          \n            var countStats = cropYieldLayer.reduceRegion({\n              reducer: ee.Reducer.count(),\n              geometry: countyGeometry,\n              scale: 30,\n              maxPixels: 1e9\n            });\n           //print(countStats);\n          \n            var selectedCounty = countyData.filter(ee.Filter.eq('County', countyName));\n            var averYield = selectedCounty.reduceColumns({\n            reducer: ee.Reducer.mean(),\n            selectors: ['Value']\n          });\n            //print(averYield);\n          \n            // create labels\n            var countyLabel = ui.Label({\n              value: 'County: ' + countyName,\n              style: {fontSize: '13px', padding: '0px 50px'}\n            });\n          \n            var count_sumLabel = ui.Label({\n              value: 'Calculating...',\n              style: {fontSize: '13px', padding: '0px 50px'}\n            });\n          \n          // update labels by calculating\n          // get the mean yield data\n            averYield.evaluate(function(result) {\n              var meanYield = result.mean;\n              var count_averYieldLabel = ui.Label({\n                value: 'Average Yield: ' + meanYield.toFixed(2) + ' BU/Acre', \n                style: {fontSize: '13px', padding: '0px 50px'}\n              });\n                resultsPanel.add(count_averYieldLabel);\n          });\n          \n            // calculate the area and total yield\n            countStats.get('YIELDpredicted').evaluate(function(value){\n\n              var areaInSqKm = (value / 1e6) * 900;\n              var areaInAcres = areaInSqKm * 247.105;\n              count_sumLabel.setValue('Crop Area: ' + areaInSqKm.toFixed(2) + \n                                      ' km² (' + areaInAcres.toFixed(2) + ' Acres)');\n                                      \n              averYield.evaluate(function(result) {\n                var meanYield = result.mean;\n                var totalYield = areaInAcres * meanYield;\n                var count_totalYieldLabel = ui.Label({\n                  value: 'Total Yield: ' + totalYield.toFixed(2) + ' BU', \n                  style: {fontSize: '13px', padding: '0px 50px'}\n                });\n                var yieldPrice = totalYield * cropPrice;\n                var yieldPriceLabel = ui.Label({\n                  value: 'Total Yield Value: ' + yieldPrice.toFixed(2) + ' $', \n                  style: {fontSize: '13px', padding: '0px 50px'}\n                });\n                resultsPanel.add(count_totalYieldLabel);\n                resultsPanel.add(yieldPriceLabel);\n          });\n            });\n          \n            // add the new label to sub-panel\n            resultsPanel.add(countyLabel);\n            resultsPanel.add(count_sumLabel);\n          };\n          \n          Map.unlisten()\n          \n            // create onclick function\n          Map.onClick(function(coords) {\n            \n          var point = ee.Geometry.Point(coords.lon, coords.lat);\n          var county = ee.Feature(nd.filterBounds(point).first());\n          var countyName = county.get('NAME');\n          countyName.evaluate(function(name) {\n            getCalculation(name, cropYieldLayer);\n          });\n          })\n          \n\n          // Area level\n        } else if (mode === 'Area Level') {\n\n          statsLabel_1.style().set('shown', false);\n          statsLabel_2.style().set('shown', true);\n          \n          // delet onclick monitor\n          Map.unlisten()\n          \n          //reset button\n          panel.remove(clearButton);\n          panel.add(clearButton);\n          \n          // draw polygon\n          var drawingTools = Map.drawingTools();\n          drawingTools.setShown(true);\n    \n    \n          // function under area level\n          function initializeAreaLevelMode() {\n            // create a new drawing tools\n            var drawingTools = Map.drawingTools();\n            drawingTools.setShown(true);\n            \n            drawingTools.onDraw(function(geometry) {\n              // get the polygon user drawing\n              var userPolygon = geometry;\n              \n              // calculate pixels number inside the polygon user draw\n              var pixelCount = cropYieldLayer.reduceRegion({\n                reducer: ee.Reducer.count(),\n                geometry: userPolygon,\n                scale: 30,\n                maxPixels: 1e9\n              });\n              \n              //calculate average yield user draw\n             var meanStats = cropYieldLayer.reduceRegion({\n              reducer: ee.Reducer.mean(),\n              geometry: userPolygon,\n              scale: 30,\n              maxPixels: 1e9\n            });\n              // print(meanStats)\n\n                // combined 2 results\n              var results = ee.Dictionary({\n                  meanYield: meanStats.get('YIELDpredicted'),\n                  pixelCount: pixelCount.get('YIELDpredicted')\n              });\n\n              // calculate average yield, crop area, total yield, and update labels\n              results.evaluate(function(values)  {\n                resultsPanel.clear();\n                \n              var area_sumLabel = ui.Label({\n                value: 'Calculating...',\n                style: {fontSize: '14px', padding: '0px 50px'}\n              });\n              \n              var meanYield_sumLabel = ui.Label({\n                value: 'Calculating...',\n                style:{fontSize: '14px', padding: '0px 50px'}\n              });\n              \n              var count_totalYieldLabel = ui.Label({\n                value: 'Calculating...',\n                style:{fontSize: '14px', padding: '0px 50px'}\n              });\n          \n              resultsPanel.add(area_sumLabel);\n              resultsPanel.add(meanYield_sumLabel);\n              resultsPanel.add(count_totalYieldLabel);\n          \n              meanYield_sumLabel.setValue('Average Yield: ' + values.meanYield.toFixed(2) + ' BU/Acre');\n          \n              var areaInSqKm = (values.pixelCount / 1e6) * 900;\n              var areaInAcres = areaInSqKm * 247.105;\n              area_sumLabel.setValue('Crop Area: ' + areaInSqKm.toFixed(2) + \n                                      ' km² (' + areaInAcres.toFixed(2) + ' Acres)');\n                                      \n              var totalYield = areaInAcres * values.meanYield;\n              count_totalYieldLabel.setValue('Total Yield: ' + totalYield.toFixed(2) + ' BU'); \n               \n              var yieldPrice = totalYield * cropPrice;\n              var yieldPriceLabel = ui.Label({\n                  value: 'Total Yield Value: ' + yieldPrice.toFixed(2) + ' $', \n                  style: {fontSize: '13px', padding: '0px 50px'}\n                });\n              resultsPanel.add(yieldPriceLabel);\n                \n                });\n                \n            });\n\n          }\n          initializeAreaLevelMode();\n          \n        }\n        \n      }\n    }),\n    \n    ui.Label('Select Year:'),\n    ui.Select({\n      items: ['Select...', '2018', '2019', '2020', \n                 '2021', '2022', '2023', '2024'],\n      value: 'Select...',\n      onChange: function(year) {\n        \n        // update global variable selectedYear, the year user chose\n        selectedYear = year;\n        updateMap();\n\n      }\n    }),\n    \n    ui.Label('Select Crop:'),\n    ui.Select({\n      items: ['Select...', 'Soybean', 'Corn', 'Wheat'],\n      value: 'Select...',\n      onChange: function(crop) {\n        \n        selectedCrop = crop;\n        \n        // set cropPrice according to selected \n        if (selectedCrop === 'Soybean') {\n          cropPrice = 11.90; \n        } else if (selectedCrop === 'Wheat') {\n          cropPrice = 6.07; \n        } else if (selectedCrop === 'Corn') {\n          cropPrice = 5.80; \n        } else {\n          cropPrice = 0;\n        }\n        \n        updateMap();\n        \n      }\n    }),\n    \n    statsLabel_1,\n    statsLabel_2\n  ],\n  style: {position: 'top-right'}\n});\n\nMap.add(panel);\n\n// Add a sub-panel to show calculation info\nvar resultsPanel = ui.Panel({\n  layout: ui.Panel.Layout.Flow('vertical'),\n  style: {width: '310px'} \n});\npanel.add(resultsPanel);\n\n// update new layers accoording to user's selection\nfunction updateMap() {\n\n  // // Remove particular layers\n  // Map.layers().forEach(function(layer) {\n  //   var layerName = layer.getName();\n  //   if (layerName.indexOf('YIELD_') === 0) {\n  //     Map.remove(layer);\n  //   }\n  // });\n  \n  Map.layers().reset();\n\n  // Show layers if user choose both selections\n  if (selectedYear !== 'Select...' && selectedCrop !== 'Select...') {\n    \n      cropYieldLayer = cropLayers[selectedCrop][selectedYear];\n\n    if (cropYieldLayer) {\n      var layerName = selectedCrop + '_' + selectedYear;\n      Map.addLayer(cropYieldLayer, {}, 'YIELD_' + layerName);\n    }\n\n  }\n  \n  // add the counties layer\n  Map.addLayer(ndCounties, {}, 'ND Counties');\n  \n}"
  },
  {
    "objectID": "index.html#project-summary",
    "href": "index.html#project-summary",
    "title": "CROPINVEST - Crop Yield Estimator",
    "section": "Project Summary",
    "text": "Project Summary\nCROPINVEST - Crop Yield Estimator for the State of North Dakota, USA"
  },
  {
    "objectID": "index.html#problem-statement",
    "href": "index.html#problem-statement",
    "title": "CROPINVEST - Crop Yield Estimator",
    "section": "Problem Statement",
    "text": "Problem Statement\nNorth Dakota, a state in the Midwestern United States, is popular with agricultural investors for its abundant agricultural resources and vast farmland. However, the agencies like – banks, crop insurance companies, etc need a tool that can predict the crop yield value of a particular farm to ensure that they can safeguard their investment. In this regard, we create CROPINVEST, a GEE application that monitors the environment to predict crop yields, and based on these yields, we obtain the market value of the crop, which in turn can help end users (banks, crop insurance companies ,etc) to accurately and scientifically capture the farms potential to pay the crop loan amount or the risk associated with the insured crops and enable stakeholders to make informed decisions that optimize agricultural productivity, market strategies, and economic outcomes."
  },
  {
    "objectID": "index.html#end-user",
    "href": "index.html#end-user",
    "title": "CROPINVEST - Crop Yield Estimator",
    "section": "End User",
    "text": "End User\n\nInsurance Companies\n\nBased on potential crop yields, charge Insurance premiums to farmers thus mitigating financial losses from decreased production.\nManaging financial exposure if low yield or drought conditions prevail, which could lead to substantial claims from farmers.\n\nBanks\n\nUtilize it to evaluate loan feasibility for farmers and adjust interest rates based on predicted yields and associated risks.\n\nFarmer Associations\n\nUtilize it to forecast yields, plan for adverse conditions, and identify areas needing more irrigation.\n\nGovernments\n\nTo anticipate and prepare for potential crop deficits or surpluses at a county level and thus developing policies/infrastructure for the same."
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "CROPINVEST - Crop Yield Estimator",
    "section": "Data",
    "text": "Data\n\nDatasets\n\nA1 - United States Census Bureau TIGER Dataset: Boundary information for U.S. counties.\nA2 - USDA NASS Cropland Data Layers: Crop-specific Land cover data.\nA3 - MOD16A2GF.061 Terra Net Evapotranspiration: Total evapotranspiration over land.\nA4 - MOD11A1.061 Terra Land Surface Temperature: Land surface temperature data.\nA5 - MOD13Q1.061 Terra Vegetation Indices 16-Day Global: Vegetation indices.\nA6 - GRIDMET: Gridded Surface Meteorological Dataset: Meteorological data.\nA7 - MCD18C2.061 Photosynthetically Active Radiation Daily 3 hour: Solar Radiation Levels.\nA8 - SPL3SMP_E.005 SMAP L3 Radiometer Global Daily 9km Soil Moisture: Soil moisture.\nA9 - Crop Yield Data: Contains county-wise crop yield data.USDA\nA10 - Current Crop Price Data: Uses current crop price data from Financial Times\n\n\n\nData Variables Overview\n\n\n\n\n\n\n\n\n\nData Type\nDataset\nDescription\nData Used\n\n\n\n\nGEE DATA\nA1\nCounty wise Boundary Information\nSTATEFP\n\n\nGEE DATA\nA2\nCrop-specific Land Cover Data Type\ncropland\n\n\nGEE DATA\nA3\nEvapo-Transpiration\nET\n\n\nGEE DATA\nA4\nLand Surface Temperature Day\nLST_Day_1km\n\n\nGEE DATA\nA4\nLand Surface Temperature Night\nLST_Night_1km\n\n\nGEE DATA\nA5\nNormalized Difference Vegetation Index\nNDVI\n\n\nGEE DATA\nA6\nPrecipitation\npr\n\n\nGEE DATA\nA7\nPhotosynthetically Active Radiation\nGMT_1200_PAR\n\n\nGEE DATA\nA8\nDay Soil Moisture\nsoil_moisture_am\n\n\nGEE DATA\nA8\nNight Soil Moisture\nsoil_moisture_pm\n\n\nOTHER DATA\nA9\nUSDA Crop Yield (Year Wise)\nYield\n\n\nOTHER DATA\nA10\nCrop Day Price $ - Financial Times\nPrice"
  },
  {
    "objectID": "index.html#methodology",
    "href": "index.html#methodology",
    "title": "CROPINVEST - Crop Yield Estimator",
    "section": "Methodology",
    "text": "Methodology\nWe explored machine learning techniques to forecast crop yields, evaluating models like Linear Regression, Random Forest, Gradient Boosting, and Neural Networks based on their R2 and RMSE scores ,and found RF best fit for each crop. Previous studies showed that RF is an effective and universal machine-learning method for crop yield prediction on a regional and global scale with high accuracy, precision, and ease of use (Jeong et al., 2016.; Prasad et al., 2021).\n\nBuilding Random Forest Model:\n\nPrepare original CSV file\n\nincluding the three crops (corn, wheat, soybean) among several X variables and Y variable (the crop yield).\n\n\nPrepare Training Data(80%) / Validation Data(20%)\nUse the Training data to train three different Random Forest Regression Models in GEE\n\n\n\n\nValidation\nTo get the performance of our models, we can use the test data from the previous split. We used R square and Root Mean Squared Error (RMSE) to validate our models. Some graphs showing these metrics are below:"
  },
  {
    "objectID": "index.html#interface",
    "href": "index.html#interface",
    "title": "CROPINVEST - Crop Yield Estimator",
    "section": "Interface",
    "text": "Interface\nCROPINVEST supports insurance companies, banks, and farmer associations with advanced features for forecasting crop yields in North Dakota for wheat, corn, and soybeans. Users can select different Crops and Years and utilize two key functionalities: a County feature to display crop yields, planting area, total production, and county-wide crop price totals via clickable maps; and a custom Area feature allowing users to draw specific regions to analyze crop yields, areas planted, and aggregate pricing. This interface ensures banks and insurance companies can assess financial risks effectively, while farmers gain crucial insights into expected yields and market conditions. Here is brief overview of how our application works:"
  },
  {
    "objectID": "index.html#the-application",
    "href": "index.html#the-application",
    "title": "CROPINVEST - Crop Yield Estimator",
    "section": "The Application",
    "text": "The Application"
  },
  {
    "objectID": "index.html#how-it-works",
    "href": "index.html#how-it-works",
    "title": "CROPINVEST - Crop Yield Estimator",
    "section": "How it Works",
    "text": "How it Works\n\nData Extraction Code:\n\nHere we use the Python environment to extract the data from different datasets using Google Earth Engine API\nAfterwards, we first use the crop-specific land cover data to distinguish different crops- wheat, soybean or corn for each county in North Dakota.\nFurther, NDVI, Precipitation, SAR, Soil Moisture & other values are used to get the county-wise values from the year 2000-2024.\nAfterwards, the Yield data is obtained from the United States Department of Agriculture for each of the years and a final dataset is obtained which has all the X Variables (NDVI,PA,SMS_AM,LST_DAY,SMS_PM,LST_NIGHT,PAR,ET) & Y variable (YIELD).\n\n\npip install earthengine-api\nimport ee\nee.AuthcessYear(year):\n    # Load the CDL dataset for the given year\n    dataset = ee.ImageCollection('USDA/NASS/CDL')\\\n                .filter(ee.Filter.date(f'{year}-01-01', f'{year}-12-31'))\\\n                .first()\n    crop_landcover = dataset.select('cropland')\n\n    # Filter for North Dakota counties\n    #`STATEFP` parameter of the dataset which provides the State FIPS code & the North Dakota value is used.\n    counties = ee.FeatureCollection('TIGER/2016/Counties')\n    nd = counties.filter(ee.Filter.eq('STATEFP', '38'))\n    \n    # Identify corn areas in North Dakota\n    #`cropland` values for different crops of our study are used Wheat, Corn & Soybean Values provided from the Cropland Table.\n    corn = crop_landcover.eq(1).Or(crop_landcover.eq(12)).Or(crop_landcover.eq(13))\n    masked_corn = crop_landcover.updateMask(corn).clipToCollection(nd)\n\n    # Calculate NDVI for corn areas using MODIS data\n    #`NDVI` parameter of the dataset and we obtain the mean over the growth period of the crop\n    NDVI_dataset = ee.ImageCollection('MODIS/061/MOD13Q1')\\\n                    .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\n    ndvi = NDVI_dataset.select('NDVI')\n    mean_ndvi = ndvi.mean().rename('NDVI')\n    cornNDVI = mean_ndvi.updateMask(masked_corn)\n    \n    # Calculate precipitation using GRIDMET data\n    #`pr` parameter of the dataset which provides the 'Precipitation amount' in mm (daily total)\n    precipitation_dataset = ee.ImageCollection(\"IDAHO_EPSCOR/GRIDMET\")\\\n                             .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\\\n                             .select('pr')\n    mean_precipitation = precipitation_dataset.mean().rename('PA')\n\n    # Load Radiometer Global Daily 9 km Soil Moisture AM\n    #`soil_moisture_am` & `soil_moisture_pm` parameter of the dataset which provides 'Retrieved soil moisture estimate from the\n    # disaggregated/downscaled vertical polarization brightness temperature at 9-km grid cell one at AM overpass & other at  PM overpass. in dB.\n    smap_dataset = ee.ImageCollection(\"NASA/SMAP/SPL3SMP_E/005\")\\\n                    .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\\\n                    .select('soil_moisture_am')\n    mean_soil_moisture = smap_dataset.mean().rename('SMS_AM')\n    \n    # Load Radiometer Global Daily 9 km Soil Moisture PM\n    smapDataset_pm = ee.ImageCollection(\"NASA/SMAP/SPL3SMP_E/005\")\\\n                       .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\\\n                       .select('soil_moisture_pm') \n    meanSoilMoisture_pm = smapDataset_pm.mean().rename('SMS_PM')\n    \n    # Load MODIS Land Surface Temperature DAY\n    #`LST_Day_1km` & `LST_Night_1km` parameter of the dataset which provides 'Daytime Land Surface Temperature' &\n    # Nighttime Land Surface Temperature' both in Kelvin (K).\n    lstDataset = ee.ImageCollection(\"MODIS/061/MOD11A1\")\\\n                   .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\n \n    lstmean_celsius = lstDataset.select('LST_Day_1km')\\\n                                .mean()\\\n                                .multiply(0.02)\\\n                                .subtract(273.15)\\\n                                .rename('LST_DAY')\n    # Load MODIS Land Surface Temperature NIGHT\n    lstDataset_night = ee.ImageCollection(\"MODIS/061/MOD11A1\")\\\n                         .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\n \n    lstmean_celsius_night = lstDataset_night.select('LST_Night_1km')\\\n                                              .mean()\\\n                                              .multiply(0.02)\\\n                                              .subtract(273.15)\\\n                                              .rename('LST_NIGHT')\n                         \n    # Photosynthetically Active Radiation Daily 3-Hour \n    #`GMT_1200_PAR` parameter of the dataset which provides 'Total PAR at GMT 12:00'. PAR is incident solar radiation in\n    # the visible spectrum (400-700 nanometers) and is an important variable in land-surface models having use in agriculture &\n    # other scientific applications.\n    par_12 = ee.ImageCollection(\"MODIS/061/MCD18C2\")\\\n               .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\\\n               .select('GMT_1200_PAR')\n                        \n    mean_par_12 = par_12.mean().rename('PAR'); # Calculate the Photosynthetically Active Radiation at 12\n\n                         \n    # Net Evapotranspiration\n    # `ET` parameter of the dataset which provides 'Total evapotranspiration' in kg/m^2/8day.s.\n    netevapo = ee.ImageCollection(\"MODIS/061/MOD16A2GF\")\\\n                 .filter(ee.Filter.date(f'{year}-05-01', f'{year}-10-01'))\\\n                 .select('ET')\n                    \n    mean_netevapo = netevapo.mean().rename('ET')  # Calculate the mean Soil Moisture\n\n\n    # Combine all layers\n    combinedDataset = cornNDVI.addBands(mean_precipitation).addBands(mean_s1_vv).addBands(mean_soil_moisture).addBands(lstmean_celsius).addBands(meanSoilMoisture_pm).addBands(lstmean_celsius_night).addBands(mean_par_12).addBands(mean_netevapo)\n\n    # Reduce regions and calculate mean values over the specified areas\n    combined_mean = combinedDataset.reduceRegions(\n        collection=nd,\n        reducer=ee.Reducer.mean(),\n        scale=30,\n        tileScale=4,\n    )\n\n    # Define export parameters\n    export_params = {\n        'collection': combined_mean,\n        'description': f'combined_{year}',\n        'folder': 'GEE_Folder',\n        'fileNamePrefix': f'Combined_{year}',\n        'fileFormat': 'CSV',\n        'selectors': ['NAME', 'GEOID', 'NDVI', 'PA', 'SAR', 'SMS_AM', 'LST_DAY', 'SMS_PM', 'LST_NIGHT', 'PAR', 'ET']\n    }\n\n    # Commented the line below as I have got the data in my drive already\n    #ee.batch.Export.table.toDrive(**export_params).start()\n\n# Example of processing each year\nfor year in range(2000, 2024):\n    processYear(year)\n\n\nMethodology Code:\n\nFirst, we experimented with various machine learning methods in Python, including Linear Regression (LR), Random Forest Regressor (RF), Gradient Boosting Regressor (XGB), and Artificial Neural Network (ANN). By comparing their R2 and RMSE scores, we identified the most suitable method for each of the three crops.\nConsidering the impact of environmental factors on crop growth within a specified time frame, we initially explored all nine parameters. Subsequently, based on the importance tests from the Random Forest model, five to six key variables were selected as independent variables in the predictive models for each crop.\nIn GEE, we trained Random Forest (RF) predictive models separately for three types of crops using data from six years, spanning 2018 to 2023, and split them into training/validation datasets. After obtaining the models, we assessed their performance using R2 and RMSE.\nUsing the historical average method to make a simple prediction for the crop yield of the coming year.\n\n\nPart 1:\n#install packages\n!pip install tensorflow\n\n#load packages\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n#packages for manipulating dataframe\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport sklearn\n\n#packages for machine learning\n##train-test-split\nfrom sklearn.model_selection import train_test_split, GridSearchCV, validation_curve\n\n##method 1: Linear Regression (LR)\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import cross_val_score\n\n##method 2: Random Forest Regressor (RF)\nimport rfpimp\nfrom sklearn.ensemble import RandomForestRegressor\n\n##method 3: Gradient Boosting Regressor (XGB)\nimport xgboost\nfrom xgboost import XGBRegressor\n\n##method 4: Artificial Neural Network (ANN)\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n##cross validation\n\n##evaluation metrics (R2 and RMSE)\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n#data visualization\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_rows', 300) # specifies number of rows to show\npd.options.display.float_format = '{:40,.4f}'.format # specifies default number format to 4 decimal places\nplt.style.use('ggplot') # specifies that graphs should use ggplot styling\n%matplotlib inline\n\n# 1. Load & Cleaning Data\n#load data\nsoybean_2018 = pd.read_csv('https://www.dropbox.com/scl/fi/ixibqk9pyuehwvoz7mr1h/2018_County_Summary_Merged.csv?rlkey=wueodqdvzsht5or4eftwdd3ys&dl=1')\nsoybean_2019 = pd.read_csv('https://www.dropbox.com/scl/fi/x3oiqrikr0xagqrh5tiqu/2019_County_Summary_Merged.csv?rlkey=zr5wru8sg5iybp43lnjf7ls5a&dl=1')\nsoybean_2020 = pd.read_csv('https://www.dropbox.com/scl/fi/m4r74ydgw3yno93nakagl/2020_County_Summary_Merged.csv?rlkey=kmd8bozo9z9jxznoa775gg33z&dl=1')\nsoybean_2021 = pd.read_csv('https://www.dropbox.com/scl/fi/2vmvsyjloz9hvzejtdez4/2021_County_Summary_Merged.csv?rlkey=ossgr4x3fvzgebhqprchntt2f&dl=1')\nsoybean_2022 = pd.read_csv('https://www.dropbox.com/scl/fi/x61224yvwtq4idnqphwjy/2022_County_Summary_Merged.csv?rlkey=s3zm9ooap8pjqom3jr0aklc6t&dl=1')\nsoybean_2023 = pd.read_csv('https://www.dropbox.com/scl/fi/zrjnmeqysrokfsua24hb3/2023_County_Summary_Merged.csv?rlkey=jb4w1pt295zeagbvgm7c2t7pk&dl=1')\n\nsoybean_list = [soybean_2018, soybean_2019, soybean_2020, soybean_2021, soybean_2022, soybean_2023]\nsoybean_df = pd.concat(soybean_list)\nsoybean_df = soybean_df.drop(['NAME','GEOID','SMS_PM','SMS_AM','SAR'], axis=1)\nsoybean_df.info()\n\n# 2.1 Train & Test Data Split\n#split the dataset\nX = soybean_df.drop('YIELD', axis=1)\ny = soybean_df['YIELD']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=100)\n\n# 2.2 Train & Test Data Standarderlized\ndef standardize_columns(file_name, columns_to_standardize):\n    scaler = StandardScaler()\n\n    df = file_name\n    df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n    return df\n\ndef standardize_series(series):\n    scaler = StandardScaler()\n    series = scaler.fit_transform(series.values.reshape(-1, 1)).flatten()\n    return series\n    \nX_columns = ['LST_DAY','PA','NDVI','ET','LST_NIGHT','PAR']\ny_columns = ['YIELD']\n\nX_train = standardize_columns(X_train, X_columns)\nX_test = standardize_columns(X_test, X_columns)\n\n# 3. Model Training and Parameter Tuning\n# 3.1. Linear Regression (LR)\nmodel_lr = LinearRegression()\n\n# Cross validation\nscores = cross_val_score(model_lr, X, y, cv=5, scoring='neg_mean_squared_error')\n\nmean_mse = np.mean(scores)\nstd_mse = np.std(scores)\n\nprint(f'Mean MSE: {mean_mse}')\nprint(f'Standard Deviation of MSE: {std_mse}')\nmodel_lr.fit(X_train, y_train)\n\n# 3.2. Random Forest Regressor (RF)\n# values of max_depth and min_samples_split\nhyperparameters = {'max_depth':[3,5,10,20,30], 'min_samples_split':[2,4,6,8,10]}\n\n\nrandomState_dt = 10000\nmodel_rf = RandomForestRegressor(random_state=randomState_dt)\n\n# cv=5 by default, which means 5-fold cross-validation\nclf = GridSearchCV(model_rf, hyperparameters)\n\nclf.fit(X_train, y_train)\n\n# we can query the best parameter value and its accuracy score\nprint (\"The best parameter value is: \")\nprint (clf.best_params_)\nprint (\"The best score is: \")\nprint (clf.best_score_)\n\n# Train the final RF\nrf_final = RandomForestRegressor(max_depth=clf.best_params_['max_depth'], min_samples_split=clf.best_params_['min_samples_split'], random_state=randomState_dt)\nrf_final.fit(X_train, y_train)\n\n# 3.3. Gradient Boosting Regressor (XGB)\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n# values of max_depth and min_samples_split\nhyperparameters = {'max_depth':[2,4,6,8,10], 'n_estimators':[4,8,12,16,20]}\n\nrandomState_xgb = 125\nxgb = XGBRegressor(random_state=randomState_xgb)\n\n# cv=5 by default, which means 5-fold cross-validation\ngscv_xgb = GridSearchCV(xgb, hyperparameters)\n\ngscv_xgb.fit(X_train, y_train)\n\n# we can query the best parameter value and its accuracy score\nprint (\"The best parameter value is: \")\nprint (gscv_xgb.best_params_)\nprint (\"The best score is: \")\nprint (gscv_xgb.best_score_)\n\n# 3.4. Artificial Neural Network (ANN)\nmodel_ann = keras.Sequential([\n    layers.Input(shape=(6,)),  # Input layer\n    layers.Dense(128, activation='relu'),  # Hidden layer with ReLU activation\n    layers.Dropout(0.5),  # Dropout layer for regularization\n    layers.Dense(64, activation='relu'),  # Additional hidden layer\n    layers.Dropout(0.3),  # Another dropout layer\n    layers.Dense(1)  # Output layer\n])\n\n#measuring the training with certain metrics\nmodel_ann.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n\n#train the model\nmodel_ann.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n\n# 4. Model Evaluation and Performance Comparison\n# 4.1. E. Linear Regression (LR)\ntrain_predictions = model_lr.predict(X_train)\ntest_predictions = model_lr.predict(X_test)\n\nr2_train_lr = r2_score(y_train, train_predictions)\nr2_test_lr = r2_score(y_test, test_predictions)\n\nrmse_train_lr = mean_squared_error(y_train, train_predictions, squared=False)\nrmse_test_lr = mean_squared_error(y_test, test_predictions, squared=False)\n\nprint(f\"Training R^2: {r2_train_lr:.4f}\")\nprint(f\"Test R^2: {r2_test_lr:.4f}\")\nprint(f\"Training RMSE: {rmse_train_lr:.4f}\")\nprint(f\"Test RMSE: {rmse_test_lr:.4f}\")\n\n# 4.2. E. Random Forest Regressor (RF)\nr2_train_rf = rf_final.score(X=X_train, y=y_train)\nr2_test_rf = rf_final.score(X=X_test, y=y_test)\n\nprint(\"R2 on the training data:\")\nprint(r2_train_rf)\nprint(\"R2 on the testing data:\")\nprint(r2_test_rf)\n\nrmse_train_rf = mean_squared_error(y_train, rf_final.predict(X_train), squared=False)\nrmse_test_rf = mean_squared_error(y_test, rf_final.predict(X_test), squared=False)\n\nprint(\"RMSE on the training data:\")\nprint(rmse_train_rf)\nprint(\"RMSE on the testing data:\")\nprint(rmse_test_rf)\n\n# Calculate and plot the feature importance of the RF model\nimp = rfpimp.importances(rf_final, X_test, y_test)\nprint(imp)\nviz = rfpimp.plot_importances(imp)\nviz.view()\n\n# 4.3. E. Gradient Boosting Regressor (XGB)\nmodel_xgb = XGBRegressor(max_depth=gscv_xgb.best_params_['max_depth'], n_estimators=gscv_xgb.best_params_['n_estimators'], random_state=randomState_xgb)\nmodel_xgb.fit(X_train, y_train)\n\n# r2_train_xgb, r2_test_xgb, rmse_train_xgb, rmse_test_xgb\nr2_train_xgb = model_xgb.score(X=X_train, y=y_train)\nr2_test_xgb = model_xgb.score(X=X_test, y=y_test)\nrmse_train_xgb = mean_squared_error(y_train, model_xgb.predict(X_train), squared=False)\nrmse_test_xgb = mean_squared_error(y_test, model_xgb.predict(X_test), squared=False)\n\nprint(\"R2 on the training data:\")\nprint(r2_train_xgb)\nprint(\"R2 on the testing data:\")\nprint(r2_test_xgb)\n\nprint(\"RMSE on the training data:\")\nprint(rmse_train_xgb)\nprint(\"RMSE on the testing data:\")\nprint(rmse_test_xgb)\n\nimp_xgb = rfpimp.importances(model_xgb, X_test, y_test) # permutation\nprint(imp_xgb)\nviz_xgb = rfpimp.plot_importances(imp_xgb)\nviz_xgb.view()\n\n# 4.4. E. Artificial Neural Network (ANN)\n#predictions\ny_pred_train_ann = model_ann.predict(X_train).flatten()\ny_pred_test_ann = model_ann.predict(X_test).flatten()\n\n#Compute R2 and RMSE\nr2_train_ann = np.round(r2_score(y_train, y_pred_train_ann),2)\nr2_test_ann = np.round(r2_score(y_test, y_pred_test_ann),2)\nrmse_train_ann = np.round(np.sqrt(mean_squared_error(y_train, y_pred_train_ann)),2)\nrmse_test_ann = np.round(np.sqrt(mean_squared_error(y_test, y_pred_test_ann)),2)\n\n#print the result\nprint(\"Train R2:\", r2_train_ann)\nprint(\"Test R2:\", r2_test_ann)\nprint(\"Train RMSE:\", rmse_train_ann)\nprint(\"Test RMSE:\", rmse_test_ann)\n\n#crosscheck the y value between real and predicted\ncrosscheck_y_dict = {\n    'y_test' : y_test,\n    'y_pred' : np.round(y_pred_test_ann,0),\n    'delta' : np.abs(np.round((y_test - y_pred_test_ann),0))\n}\n\n#plotting histogram\ncrosscheck_y_df = pd.DataFrame(crosscheck_y_dict)\nplt.hist(crosscheck_y_df['delta'], bins=10)\nplt.xlabel(f'Delta\\nR2_test = {r2_test_ann}, RMSE_test = {rmse_test_ann}, delta_max = {crosscheck_y_df.delta.max()}')\nplt.ylabel('Frequency (Number of Data)')\nplt.title(f\"Accuracy of ANN Model Based on Delta of Y Test and Y Pred)\")\nplt.show()\n\n# 4.5. Model Performance Comparison\n#please input your metrics in here\nmetrics_dict = {\n    'metrics': [\"Train R2\",\"Test R2\",\"Train RMSE\",\"Test RMSE\"],\n    'LR': [r2_train_lr, r2_test_lr, rmse_train_lr, rmse_test_lr],\n    'RF': [r2_train_rf, r2_test_rf, rmse_train_rf, rmse_test_rf],\n    'XGB': [r2_train_xgb, r2_test_xgb, rmse_train_xgb, rmse_test_xgb],\n    'ANN': [r2_train_ann, r2_test_ann, rmse_train_ann, rmse_test_ann]\n}\n\n#create dataframe\nmetrics_df = pd.DataFrame(metrics_dict)\nmetrics_df.set_index('metrics')\n\n\nPart 2:\n\n//  ========================== GEE  ========================================\n// Import CSV data  \n// Load data from CSV file\nvar csvFile = ee.FeatureCollection('projects/ee-ucfnfli/assets/corn_df');\n\n// ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B9', 'B10', 'B11', 'BQA']\nvar bands = ['NDVI', 'LST_DAY', 'ET', 'PAR', 'PA'];\n\n\n//  ========================== train the RF classification model  ========================================\n// Split the training and testing data\nvar withRandom = csvFile.randomColumn({\n  columnName:'random',\n  seed:2,\n  distribution: 'uniform',\n});\n\n\nvar split = 0.8; \nvar trainingData = withRandom.filter(ee.Filter.lt('random', split));\nvar validationData = withRandom.filter(ee.Filter.gte('random', split));\n\n\n// var classifier = ee.Classifier.smileRandomForest(100, null, 8, 0.5, null, 125)\nvar classifier = ee.Classifier.smileRandomForest(100, null, 1, 0.7, null, 0).setOutputMode('REGRESSION')\n    .train({\n      features: trainingData, \n      classProperty: 'YIELD', \n      inputProperties: bands\n    });\n//print(classifier);\n\n// =================================== img  ======================\n\nvar featureTest = trainingData.select(['NDVI', 'LST_DAY', 'ET', 'PAR', 'PA','YIELD']);\n\nvar classifiedFeatures = featureTest.classify(classifier,'YIELDPredicted');\n\n\n// ============================ Validation  =================================\n// ============================ variable importance  =================================\nvar dict = classifier.explain();\nprint(\"Classifier information:\", dict);\nvar variableImportance = ee.Feature(null, ee.Dictionary(dict)\n  .get('importance'));\n// Make chart, print it\nvar chart =\n  ui.Chart.feature.byProperty(variableImportance)\n  .setChartType('ColumnChart')\n  .setOptions({\n    title: 'Random Forest Variable Importance',\n    legend: {\n      position: 'none'\n    },\n    hAxis: {\n      title: 'Bands'\n    },\n    vAxis: {\n      title: 'Importance'\n    }\n  });\nprint(chart);\n\n// ===================================  Compute RSME R2 =====================\n// Separate the observed (REDOX_CM) and predicted (regression) properties\n// var sampleTraining = predictedTraining.select(['Yield', 'predicted']);\n// Create chart, print it\nvar chartTraining = ui.Chart.feature.byFeature({features:classifiedFeatures, xProperty:'YIELD', yProperties:['YIELDPredicted']})\n  .setChartType('ScatterChart')\n  .setOptions({\n    title: 'Predicted vs Observed - Training data ',\n    hAxis: {\n      'title': 'observed'\n    },\n    vAxis: {\n      'title': 'predicted'\n    },\n    pointSize: 3,\n    trendlines: {\n      0: {\n        showR2: true,\n        visibleInLegend: true\n      },\n      1: {\n        showR2: true,\n        visibleInLegend: true\n      }\n    }\n  });\nprint(chartTraining);\n// Training data\n// R2 = 0.964;  RMSE = 6.601 in python\n// R2 = 0.948;  RMSE = 8.396 in GEE\n\n// Compute RSME\n// Get array of observation and prediction values \nvar observationTraining = ee.Array(classifiedFeatures.aggregate_array('YIELD'));\nvar predictionTraining = ee.Array(classifiedFeatures.aggregate_array('YIELDPredicted'));\n//print('observationTraining', observationTraining)\n//print('predictionTraining', predictionTraining)\n\n// Compute residuals\nvar residualsTraining = observationTraining.subtract(predictionTraining);\n//print('residualsTraining', residualsTraining)\n// Compute RMSE with equation, print it\nvar rmseTraining = residualsTraining.pow(2)\n  .reduce('mean', [0])\n  .sqrt();\nprint('Training RMSE', rmseTraining); \n\n\n// =============================== Validation =================================\n\nvar featureValidation = validationData.select(['NDVI', 'LST_DAY', 'ET', 'PAR', 'PA','YIELD']);\n//  Validation data \nvar classifiedValidation = featureValidation.classify(classifier,'YIELDPredicted');\n// print result\n//print('ValidRegressionResult:', classifiedValidation);\n\nvar sampleValidation = classifiedValidation.select(['YIELD', 'YIELDPredicted']);\n// Create chart, print it\nvar chartValidation = ui.Chart.feature.byFeature(sampleValidation, 'YIELD', 'YIELDPredicted')\n  .setChartType('ScatterChart')\n  .setOptions({\n    title: 'Predicted vs Observed - Validation data',\n    hAxis: {\n      'title': 'predicted'\n    },\n    vAxis: {\n      'title': 'observed'\n    },\n    pointSize: 3,\n    trendlines: {\n      0: {\n        showR2: true,\n        visibleInLegend: true\n      },\n      1: {\n        showR2: true,\n        visibleInLegend: true\n      }\n    }\n  });\nprint(chartValidation);\n\n// Get array of observation and prediction values \nvar observationValidation = ee.Array(sampleValidation.aggregate_array('YIELD'));\nvar predictionValidation = ee.Array(sampleValidation.aggregate_array('YIELDPredicted'));\n// Compute residuals\nvar residualsValidation = observationValidation.subtract(predictionValidation);\n// Compute RMSE with equation, print it\nvar rmseValidation = residualsValidation.pow(2)\n  .reduce('mean', [0])\n  .sqrt();\nprint('Validation RMSE', rmseValidation);\n// Testing data\n// R2 = 0.812;  RMSE = 15.166 in python\n// R2 = 0.841;  RMSE = 12.990 in GEE\n// ============================== merge raster data ============================\n// Merge \nvar yield2021 = processYear(2021);\nvar yield2022 = processYear(2022);\nvar yield2023 = processYear(2023);\n// Concatenate two images into one multi-band image.\nvar mergeYield = ee.Image.cat([yield2021, yield2022, yield2023]);\nprint('mergeImg:', mergeYield);\n\n// Obtain the union of all images in the image collection and sum them\n// sum all three years yields\nvar summedImage = mergeYield.reduce(ee.Reducer.sum());\nprint(summedImage)\n\n// Print the results\nprint('sumImg:', summeImag // Show layers if user choose both selections\n  if (selectedYear !== 'Select...' && selectedCrop !== 'Select...') {\n    \n      cropYieldLayer = cropLayers[selectedCrop][selectedYear];\n\n    if (cropYieldLayer) {\n      var layerName = selectedCrop + '_' + selectedYear;\n      Map.addLayer(cropYieldLayer, {}, 'YIELD_' + layerName);\n    }\n\n  }\n  \n  // add the counties layer\n  Map.addLayer(ndCounties, {}, 'ND Counties');\n  \n}\n'_' + selectedYear;\n      \n      Map.addLayer(cropYieldLayer, {}, 'YIELD_' + layerName);\n    }\n\n  }\n  \n  // add the counties layer\n  Map.addLayer(ndCounties, {}, 'ND Counties');\n}\n\n\n\nInterface Code:\n\nIn this section, we perform calculations on the crop layers obtained from the random forest prediction model to generate various outputs. By defining global variables, we dynamically update the crop layers based on the user’s selected year and crop type. The core functionality lies in the computation logic for the County level and Area level modes.\nIn the County level mode, a click event listener is created to provide real-time feedback on the user-selected county. The number of pixels within the county is calculated, enabling the estimation of the crop area. Utilizing historical and predicted average yield data and the current year’s average market price, the total production and total value are computed.\nIn the Area level mode extends the calculation scope to a user-defined polygon region, with a similar computation method to the County level mode. The “Clear Selected Area Button” allows users to remove unnecessary polygons in the Area level mode, enhancing usability.\nThe Interface Code is as follows :\n\n\n// ——————————————————————————define crop layers————————————————————————————————\n\nvar cropLayers = {\n  Corn: {\n    // Add layers of corn\n    '2018': ee.Image(\"projects/ee-songzimeng/assets/corn2018\"),\n    '2019': ee.Image(\"projects/ee-songzimeng/assets/corn2019\"),\n    '2020': ee.Image(\"projects/ee-songzimeng/assets/corn2020\"),\n    '2021': ee.Image(\"projects/ee-songzimeng/assets/corn2021\"),\n    '2022': ee.Image(\"projects/ee-songzimeng/assets/corn2022\"),\n    '2023': ee.Image(\"projects/ee-songzimeng/assets/corn2023\"),\n    '2024': ee.Image(\"projects/ee-songzimeng/assets/corn2024\")\n    \n  },\n  \n  Soybean: {\n    // Add layers of soybean\n\n    '2018': ee.Image(\"projects/ee-songzimeng/assets/soybean2018\"),\n    '2019': ee.Image(\"projects/ee-songzimeng/assets/soybean2019\"),\n    '2020': ee.Image(\"projects/ee-songzimeng/assets/soybean2020\"),\n    '2021': ee.Image(\"projects/ee-songzimeng/assets/soybean2021\"),\n    '2022': ee.Image(\"projects/ee-songzimeng/assets/soybean2022\"),\n    '2023': ee.Image(\"projects/ee-songzimeng/assets/soybean2023\"),\n    '2024': ee.Image(\"projects/ee-songzimeng/assets/soybean2024\")\n  },\n  \n  Wheat: {\n    // Add layers of wheat\n\n    '2018': ee.Image(\"projects/ee-songzimeng/assets/wheat2018\"),\n    '2019': ee.Image(\"projects/ee-songzimeng/assets/wheat2019\"),\n    '2020': ee.Image(\"projects/ee-songzimeng/assets/wheat2020\"),\n    '2021': ee.Image(\"projects/ee-songzimeng/assets/wheat2021\"),\n    '2022': ee.Image(\"projects/ee-songzimeng/assets/wheat2022\"),\n    '2023': ee.Image(\"projects/ee-songzimeng/assets/wheat2023\"),\n    '2024': ee.Image(\"projects/ee-songzimeng/assets/wheat2024\")\n  }\n};\n\n// -------------------------- Data  ------------------------------\nMap.setCenter(-100.55, 47.5, 7);\nMap.setOptions('SATELLITE');\n\n// clip the north dakota\nvar counties = ee.FeatureCollection('TIGER/2016/Counties');\nvar nd = counties.filter(ee.Filter.eq('STATEFP', '38'));\n\n// Formatted county name function\nvar nd = nd.map(function(feature) {\n  var name = ee.String(feature.get('NAME')).toUpperCase().replace(' ', '', 'g');\n  return feature.set('NAME', name);\n});\n\n// Show the county boundary\nvar ndCounties = ee.Image().byte().paint({\n  featureCollection: nd,\n  color: null, \n  width: 1\n});\n\n// Add the counties layer\nMap.addLayer(ndCounties, {}, 'ND Counties');\n\n/// ——————————————Function and global variables——————————————————————————\n// Function to read csv\nfunction readCsvFile(selectedYear, selectedCrop) {\n  var fileName = selectedYear +'_'+ selectedCrop;\n  var csvFile = ee.FeatureCollection('projects/ee-songzimeng/assets/' + fileName); \n\n  return csvFile;\n}\n\n// Function to fomat county name\nfunction processCountyColumn(table) {\n  var countyColumnName = 'County';\n  function processCountyName(countyName) {\n    return ee.String(countyName).toUpperCase().replace('\\\\s+', '');\n  }\n  \n  var processedCountyColumn = table.map(function(feature) {\n    var countyName = feature.get(countyColumnName);\n    var processedCountyName = processCountyName(countyName);\n    return feature.set(countyColumnName, processedCountyName);\n  });\n  \n  // return FeatureCollection\n  return processedCountyColumn;\n}\n\nvar selectedCrop='Select...';\nvar selectedYear='Select...';\nvar soybeanPrice = 11.90; // 2024 average\nvar CornPrice = 41.68; // 2024 average\nvar wheatPrice = 6.07; // 2024 average\nvar cropPrice = 0; //\n\nvar crops = {\n  'Corn': 1,\n  'Wheat': 23,\n  'Soybean': 5\n};\n\n\n// ————————————————interface——————————————————————————\n// set default year\nvar defaultYear = '2018';\n\nvar cropYieldLayer = null;\n\nvar statsLabel_1 = ui.Label('Click on County to see info:');\nvar statsLabel_2 = ui.Label('Select an area to see info:');\n\n// set original info status\nstatsLabel_1.style().set('shown', true);\nstatsLabel_2.style().set('shown', false);\n\n// Clear button to remove all selected layers\nvar drawingTools = Map.drawingTools();\nvar clearButton = ui.Button({\n  label: 'Clear Selected Area',\n  onClick: function() {\n\n    var layers = drawingTools.layers();\n\n    layers.forEach(function(layer) {\n      drawingTools.layers().remove(layer);\n    });\n\n    resultsPanel.clear();\n  },\n  style: {margin: '10px'}\n});\n\n\n// the main panel to select mode, year, croptype\nvar panel = ui.Panel({\n  widgets: [\n    \n    ui.Label('North Dakota Crop Yield', {\n      fontWeight: 'bold',\n      fontSize: '22px',\n      textAlign: 'center',\n      stretch: 'horizontal'\n      \n    }),\n    \n    ui.Label('Select Mode:'),\n    ui.Select({\n      items: ['Select...','County Level', 'Area Level'],\n      value: 'Select...',\n      onChange: function(mode) {\n        \n        // operate different \n        if (mode === 'County Level') {\n          // County Level\n          statsLabel_1.style().set('shown', true);\n          statsLabel_2.style().set('shown', false);\n          \n          // reset button\n          panel.remove(clearButton);\n          panel.add(clearButton);\n          \n          // ban polygon drawing selection\n          var drawingTools = Map.drawingTools();\n          drawingTools.setShown(false);\n          \n          //Function for getting value from image\n          var getCalculation = function(countyName, cropYieldLayer) {\n            var county = nd.filter(ee.Filter.eq('NAME', countyName)).first();\n            var countyGeometry = county.geometry();\n            \n             //print(selectedYear, selectedCrop);\n            var countyData=readCsvFile(selectedYear, selectedCrop);\n            // print(countyData);\n            countyData = processCountyColumn(countyData);\n            \n            resultsPanel.clear();\n          \n            var countStats = cropYieldLayer.reduceRegion({\n              reducer: ee.Reducer.count(),\n              geometry: countyGeometry,\n              scale: 30,\n              maxPixels: 1e9\n            });\n           //print(countStats);\n          \n            var selectedCounty = countyData.filter(ee.Filter.eq('County', countyName));\n            var averYield = selectedCounty.reduceColumns({\n            reducer: ee.Reducer.mean(),\n            selectors: ['Value']\n          });\n            //print(averYield);\n          \n            // create labels\n            var countyLabel = ui.Label({\n              value: 'County: ' + countyName,\n              style: {fontSize: '13px', padding: '0px 50px'}\n            });\n          \n            var count_sumLabel = ui.Label({\n              value: 'Calculating...',\n              style: {fontSize: '13px', padding: '0px 50px'}\n            });\n          \n          // update labels by calculating\n          // get the mean yield data\n            averYield.evaluate(function(result) {\n              var meanYield = result.mean;\n              var count_averYieldLabel = ui.Label({\n                value: 'Average Yield: ' + meanYield.toFixed(2) + ' BU/Acre', \n                style: {fontSize: '13px', padding: '0px 50px'}\n              });\n                resultsPanel.add(count_averYieldLabel);\n          });\n          \n            // calculate the area and total yield\n            countStats.get('YIELDpredicted').evaluate(function(value){\n\n              var areaInSqKm = (value / 1e6) * 900;\n              var areaInAcres = areaInSqKm * 247.105;\n              count_sumLabel.setValue('Crop Area: ' + areaInSqKm.toFixed(2) + \n                                      ' km² (' + areaInAcres.toFixed(2) + ' Acres)');\n                                      \n              averYield.evaluate(function(result) {\n                var meanYield = result.mean;\n                var totalYield = areaInAcres * meanYield;\n                var count_totalYieldLabel = ui.Label({\n                  value: 'Total Yield: ' + totalYield.toFixed(2) + ' BU', \n                  style: {fontSize: '13px', padding: '0px 50px'}\n                });\n                var yieldPrice = totalYield * cropPrice;\n                var yieldPriceLabel = ui.Label({\n                  value: 'Total Yield Value: ' + yieldPrice.toFixed(2) + ' $', \n                  style: {fontSize: '13px', padding: '0px 50px'}\n                });\n                resultsPanel.add(count_totalYieldLabel);\n                resultsPanel.add(yieldPriceLabel);\n          });\n            });\n          \n            // add the new label to sub-panel\n            resultsPanel.add(countyLabel);\n            resultsPanel.add(count_sumLabel);\n          };\n          \n          Map.unlisten()\n          \n            // create onclick function\n          Map.onClick(function(coords) {\n            \n          var point = ee.Geometry.Point(coords.lon, coords.lat);\n          var county = ee.Feature(nd.filterBounds(point).first());\n          var countyName = county.get('NAME');\n          countyName.evaluate(function(name) {\n            getCalculation(name, cropYieldLayer);\n          });\n          })\n          \n\n          // Area level\n        } else if (mode === 'Area Level') {\n\n          statsLabel_1.style().set('shown', false);\n          statsLabel_2.style().set('shown', true);\n          \n          // delet onclick monitor\n          Map.unlisten()\n          \n          //reset button\n          panel.remove(clearButton);\n          panel.add(clearButton);\n          \n          // draw polygon\n          var drawingTools = Map.drawingTools();\n          drawingTools.setShown(true);\n    \n    \n          // function under area level\n          function initializeAreaLevelMode() {\n            // create a new drawing tools\n            var drawingTools = Map.drawingTools();\n            drawingTools.setShown(true);\n            \n            drawingTools.onDraw(function(geometry) {\n              // get the polygon user drawing\n              var userPolygon = geometry;\n              \n              // calculate pixels number inside the polygon user draw\n              var pixelCount = cropYieldLayer.reduceRegion({\n                reducer: ee.Reducer.count(),\n                geometry: userPolygon,\n                scale: 30,\n                maxPixels: 1e9\n              });\n              \n              //calculate average yield user draw\n             var meanStats = cropYieldLayer.reduceRegion({\n              reducer: ee.Reducer.mean(),\n              geometry: userPolygon,\n              scale: 30,\n              maxPixels: 1e9\n            });\n              // print(meanStats)\n\n                // combined 2 results\n              var results = ee.Dictionary({\n                  meanYield: meanStats.get('YIELDpredicted'),\n                  pixelCount: pixelCount.get('YIELDpredicted')\n              });\n\n              // calculate average yield, crop area, total yield, and update labels\n              results.evaluate(function(values)  {\n                resultsPanel.clear();\n                \n              var area_sumLabel = ui.Label({\n                value: 'Calculating...',\n                style: {fontSize: '14px', padding: '0px 50px'}\n              });\n              \n              var meanYield_sumLabel = ui.Label({\n                value: 'Calculating...',\n                style:{fontSize: '14px', padding: '0px 50px'}\n              });\n              \n              var count_totalYieldLabel = ui.Label({\n                value: 'Calculating...',\n                style:{fontSize: '14px', padding: '0px 50px'}\n              });\n          \n              resultsPanel.add(area_sumLabel);\n              resultsPanel.add(meanYield_sumLabel);\n              resultsPanel.add(count_totalYieldLabel);\n          \n              meanYield_sumLabel.setValue('Average Yield: ' + values.meanYield.toFixed(2) + ' BU/Acre');\n          \n              var areaInSqKm = (values.pixelCount / 1e6) * 900;\n              var areaInAcres = areaInSqKm * 247.105;\n              area_sumLabel.setValue('Crop Area: ' + areaInSqKm.toFixed(2) + \n                                      ' km² (' + areaInAcres.toFixed(2) + ' Acres)');\n                                      \n              var totalYield = areaInAcres * values.meanYield;\n              count_totalYieldLabel.setValue('Total Yield: ' + totalYield.toFixed(2) + ' BU'); \n               \n              var yieldPrice = totalYield * cropPrice;\n              var yieldPriceLabel = ui.Label({\n                  value: 'Total Yield Value: ' + yieldPrice.toFixed(2) + ' $', \n                  style: {fontSize: '13px', padding: '0px 50px'}\n                });\n              resultsPanel.add(yieldPriceLabel);\n                \n                });\n                \n            });\n\n          }\n          initializeAreaLevelMode();\n          \n        }\n        \n      }\n    }),\n    \n    ui.Label('Select Year:'),\n    ui.Select({\n      items: ['Select...', '2018', '2019', '2020', \n                 '2021', '2022', '2023', '2024'],\n      value: 'Select...',\n      onChange: function(year) {\n        \n        // update global variable selectedYear, the year user chose\n        selectedYear = year;\n        updateMap();\n\n      }\n    }),\n    \n    ui.Label('Select Crop:'),\n    ui.Select({\n      items: ['Select...', 'Soybean', 'Corn', 'Wheat'],\n      value: 'Select...',\n      onChange: function(crop) {\n        \n        selectedCrop = crop;\n        \n        // set cropPrice according to selected \n        if (selectedCrop === 'Soybean') {\n          cropPrice = 11.90; \n        } else if (selectedCrop === 'Wheat') {\n          cropPrice = 6.07; \n        } else if (selectedCrop === 'Corn') {\n          cropPrice = 5.80; \n        } else {\n          cropPrice = 0;\n        }\n        \n        updateMap();\n        \n      }\n    }),\n    \n    statsLabel_1,\n    statsLabel_2\n  ],\n  style: {position: 'top-right'}\n});\n\nMap.add(panel);\n\n// Add a sub-panel to show calculation info\nvar resultsPanel = ui.Panel({\n  layout: ui.Panel.Layout.Flow('vertical'),\n  style: {width: '310px'} \n});\npanel.add(resultsPanel);\n\n// update new layers accoording to user's selection\nfunction updateMap() {\n\n  // // Remove particular layers\n  // Map.layers().forEach(function(layer) {\n  //   var layerName = layer.getName();\n  //   if (layerName.indexOf('YIELD_') === 0) {\n  //     Map.remove(layer);\n  //   }\n  // });\n  \n  Map.layers().reset();\n\n  // Show layers if user choose both selections\n  if (selectedYear !== 'Select...' && selectedCrop !== 'Select...') {\n    \n      cropYieldLayer = cropLayers[selectedCrop][selectedYear];\n\n    if (cropYieldLayer) {\n      var layerName = selectedCrop + '_' + selectedYear;\n      Map.addLayer(cropYieldLayer, {}, 'YIELD_' + layerName);\n    }\n\n  }\n  \n  // add the counties layer\n  Map.addLayer(ndCounties, {}, 'ND Counties');\n  \n}"
  },
  {
    "objectID": "Modeling/old_supervised_learning.html",
    "href": "Modeling/old_supervised_learning.html",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "",
    "text": "This jupyter notebook is addressed for creating model that can estimate crop yield in North Dakota, United States. The detail workflow of this project can be seen in here."
  },
  {
    "objectID": "Modeling/old_supervised_learning.html#install-load-packages",
    "href": "Modeling/old_supervised_learning.html#install-load-packages",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "0. Install & Load Packages",
    "text": "0. Install & Load Packages\n\n#install packages\n!pip install tensorflow\n\nCollecting tensorflow\n  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/58/70/e8ac764ec80810eefcbab0cb1d21dbba6cf26719c44cd6d9a5e9f0407935/tensorflow-2.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading tensorflow-2.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\nCollecting absl-py&gt;=1.0.0 (from tensorflow)\n  Obtaining dependency information for absl-py&gt;=1.0.0 from https://files.pythonhosted.org/packages/a2/ad/e0d3c824784ff121c03cc031f944bc7e139a8f1870ffd2845cc2dd76f6c4/absl_py-2.1.0-py3-none-any.whl.metadata\n  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\nCollecting astunparse&gt;=1.6.0 (from tensorflow)\n  Obtaining dependency information for astunparse&gt;=1.6.0 from https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl.metadata\n  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\nCollecting flatbuffers&gt;=23.5.26 (from tensorflow)\n  Obtaining dependency information for flatbuffers&gt;=23.5.26 from https://files.pythonhosted.org/packages/41/f0/7e988a019bc54b2dbd0ad4182ef2d53488bb02e58694cd79d61369e85900/flatbuffers-24.3.25-py2.py3-none-any.whl.metadata\n  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\nCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,&gt;=0.2.1 (from tensorflow)\n  Obtaining dependency information for gast!=0.5.0,!=0.5.1,!=0.5.2,&gt;=0.2.1 from https://files.pythonhosted.org/packages/fa/39/5aae571e5a5f4de9c3445dae08a530498e5c53b0e74410eeeb0991c79047/gast-0.5.4-py3-none-any.whl.metadata\n  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\nCollecting google-pasta&gt;=0.1.1 (from tensorflow)\n  Obtaining dependency information for google-pasta&gt;=0.1.1 from https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl.metadata\n  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\nCollecting h5py&gt;=3.10.0 (from tensorflow)\n  Obtaining dependency information for h5py&gt;=3.10.0 from https://files.pythonhosted.org/packages/1e/e9/61d7338e503d63d2ce733373fa86256614f579b173cf3d0571d4f46cb561/h5py-3.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading h5py-3.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\nCollecting libclang&gt;=13.0.0 (from tensorflow)\n  Obtaining dependency information for libclang&gt;=13.0.0 from https://files.pythonhosted.org/packages/1d/fc/716c1e62e512ef1c160e7984a73a5fc7df45166f2ff3f254e71c58076f7c/libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata\n  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\nCollecting ml-dtypes~=0.3.1 (from tensorflow)\n  Obtaining dependency information for ml-dtypes~=0.3.1 from https://files.pythonhosted.org/packages/77/a0/d4ee9e3aca5b9101c590b58555820618e8201c2ccb7004eabb417ec046ac/ml_dtypes-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading ml_dtypes-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nCollecting opt-einsum&gt;=2.3.2 (from tensorflow)\n  Obtaining dependency information for opt-einsum&gt;=2.3.2 from https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl.metadata\n  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (23.1)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,&lt;5.0.0dev,&gt;=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.23.3)\nRequirement already satisfied: requests&lt;3,&gt;=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.31.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (68.1.2)\nRequirement already satisfied: six&gt;=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\nCollecting termcolor&gt;=1.1.0 (from tensorflow)\n  Obtaining dependency information for termcolor&gt;=1.1.0 from https://files.pythonhosted.org/packages/d9/5f/8c716e47b3a50cbd7c146f45881e11d9414def768b7cd9c5e6650ec2a80a/termcolor-2.4.0-py3-none-any.whl.metadata\n  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: typing-extensions&gt;=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.7.1)\nRequirement already satisfied: wrapt&gt;=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.15.0)\nCollecting grpcio&lt;2.0,&gt;=1.24.3 (from tensorflow)\n  Obtaining dependency information for grpcio&lt;2.0,&gt;=1.24.3 from https://files.pythonhosted.org/packages/71/fd/28fd4a325797e423f453c3718b08bb34e3aeb11801972eb3cbf6911b8630/grpcio-1.62.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading grpcio-1.62.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting tensorboard&lt;2.17,&gt;=2.16 (from tensorflow)\n  Obtaining dependency information for tensorboard&lt;2.17,&gt;=2.16 from https://files.pythonhosted.org/packages/3a/d0/b97889ffa769e2d1fdebb632084d5e8b53fc299d43a537acee7ec0c021a3/tensorboard-2.16.2-py3-none-any.whl.metadata\n  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\nCollecting keras&gt;=3.0.0 (from tensorflow)\n  Obtaining dependency information for keras&gt;=3.0.0 from https://files.pythonhosted.org/packages/59/a8/d94e8acb59d678d908fe1db0c7ad89dfa2c2e2e529eeb3c2b3cc218a758d/keras-3.1.1-py3-none-any.whl.metadata\n  Downloading keras-3.1.1-py3-none-any.whl.metadata (5.6 kB)\nCollecting tensorflow-io-gcs-filesystem&gt;=0.23.1 (from tensorflow)\n  Obtaining dependency information for tensorflow-io-gcs-filesystem&gt;=0.23.1 from https://files.pythonhosted.org/packages/44/66/10773d9ea847ba0ae5c36478333d92c6dae3396205bf18091910f63f3ee9/tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\nRequirement already satisfied: numpy&lt;2.0.0,&gt;=1.23.5 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.24.4)\nRequirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse&gt;=1.6.0-&gt;tensorflow) (0.41.2)\nRequirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from keras&gt;=3.0.0-&gt;tensorflow) (13.5.3)\nCollecting namex (from keras&gt;=3.0.0-&gt;tensorflow)\n  Obtaining dependency information for namex from https://files.pythonhosted.org/packages/cd/43/b971880e2eb45c0bee2093710ae8044764a89afe9620df34a231c6f0ecd2/namex-0.0.7-py3-none-any.whl.metadata\n  Downloading namex-0.0.7-py3-none-any.whl.metadata (246 bytes)\nCollecting optree (from keras&gt;=3.0.0-&gt;tensorflow)\n  Obtaining dependency information for optree from https://files.pythonhosted.org/packages/3c/ea/efd59fc3cec0d0d7f953b956ef881ca4b972696c8f4261cbca23c636b908/optree-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading optree-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (45 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.4/45.4 kB 10.4 MB/s eta 0:00:00\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/conda/lib/python3.11/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (3.2.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.11/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (2.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (2023.7.22)\nCollecting markdown&gt;=2.6.8 (from tensorboard&lt;2.17,&gt;=2.16-&gt;tensorflow)\n  Obtaining dependency information for markdown&gt;=2.6.8 from https://files.pythonhosted.org/packages/fc/b3/0c0c994fe49cd661084f8d5dc06562af53818cc0abefaca35bdc894577c3/Markdown-3.6-py3-none-any.whl.metadata\n  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\nCollecting tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 (from tensorboard&lt;2.17,&gt;=2.16-&gt;tensorflow)\n  Obtaining dependency information for tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 from https://files.pythonhosted.org/packages/73/c6/825dab04195756cf8ff2e12698f22513b3db2f64925bdd41671bfb33aaa5/tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata\n  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\nCollecting werkzeug&gt;=1.0.1 (from tensorboard&lt;2.17,&gt;=2.16-&gt;tensorflow)\n  Obtaining dependency information for werkzeug&gt;=1.0.1 from https://files.pythonhosted.org/packages/c3/fc/254c3e9b5feb89ff5b9076a23218dafbc99c96ac5941e900b71206e6313b/werkzeug-3.0.1-py3-none-any.whl.metadata\n  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: MarkupSafe&gt;=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard&lt;2.17,&gt;=2.16-&gt;tensorflow) (2.1.3)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich-&gt;keras&gt;=3.0.0-&gt;tensorflow) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich-&gt;keras&gt;=3.0.0-&gt;tensorflow) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras&gt;=3.0.0-&gt;tensorflow) (0.1.0)\nDownloading tensorflow-2.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 589.8/589.8 MB 870.1 kB/s eta 0:00:0000:0100:01\nDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.7/133.7 kB 1.0 MB/s eta 0:00:0000:01\nDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\nDownloading gast-0.5.4-py3-none-any.whl (19 kB)\nDownloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 15.6 MB/s eta 0:00:00\nDownloading grpcio-1.62.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 10.6 MB/s eta 0:00:0000:0100:01\nDownloading h5py-3.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 8.0 MB/s eta 0:00:0000:0100:01m\nDownloading keras-3.1.1-py3-none-any.whl (1.1 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 13.4 MB/s eta 0:00:0000:0100:01\nDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.5/24.5 MB 11.0 MB/s eta 0:00:0000:0100:01\nDownloading ml_dtypes-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 17.9 MB/s eta 0:00:0000:0100:01\nDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 4.7 MB/s eta 0:00:00\nDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 12.4 MB/s eta 0:00:0000:0100:01\nDownloading tensorflow_io_gcs_filesystem-0.36.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.1/5.1 MB 11.8 MB/s eta 0:00:0000:0100:01\nDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\nDownloading Markdown-3.6-py3-none-any.whl (105 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.4/105.4 kB 13.1 MB/s eta 0:00:00\nDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 12.5 MB/s eta 0:00:0000:0100:01\nDownloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 226.7/226.7 kB 9.1 MB/s eta 0:00:00\nDownloading namex-0.0.7-py3-none-any.whl (5.8 kB)\nDownloading optree-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (312 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 312.0/312.0 kB 13.3 MB/s eta 0:00:00\nInstalling collected packages: namex, libclang, flatbuffers, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, optree, opt-einsum, ml-dtypes, markdown, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow\n  Attempting uninstall: h5py\n    Found existing installation: h5py 3.9.0\n    Uninstalling h5py-3.9.0:\n      Successfully uninstalled h5py-3.9.0\nSuccessfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.5.4 google-pasta-0.2.0 grpcio-1.62.1 h5py-3.10.0 keras-3.1.1 libclang-18.1.1 markdown-3.6 ml-dtypes-0.3.2 namex-0.0.7 opt-einsum-3.3.0 optree-0.11.0 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.36.0 termcolor-2.4.0 werkzeug-3.0.1\n\n\n\n#load packages\n\n#packages for manipulating dataframe\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport sklearn\n\n#packages for machine learning\n##train-test-split\nfrom sklearn.model_selection import train_test_split, GridSearchCV, validation_curve\n\n##method 1: Linear Regression (LR)\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n##method 2: Random Forest Regressor (RF)\nimport rfpimp\nfrom sklearn.ensemble import RandomForestRegressor\n\n##method 3: Gradient Boosting Regressor (XGB)\nimport xgboost\nfrom xgboost import XGBRegressor\n\n##method 4: Artificial Neural Network (ANN)\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n##cross validation\n\n##evaluation metrics (R2 and RMSE)\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n#data visualization\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_rows', 300) # specifies number of rows to show\npd.options.display.float_format = '{:40,.4f}'.format # specifies default number format to 4 decimal places\nplt.style.use('ggplot') # specifies that graphs should use ggplot styling\n%matplotlib inline"
  },
  {
    "objectID": "Modeling/old_supervised_learning.html#load-cleaning-data",
    "href": "Modeling/old_supervised_learning.html#load-cleaning-data",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "1. Load & Cleaning Data",
    "text": "1. Load & Cleaning Data\n\n#load data\nsoybean_2018 = pd.read_csv('https://www.dropbox.com/scl/fi/7oje7pt1drpmurfqfaa2t/standardized_2018_County_Summary_Merged.csv?rlkey=i1ka8sg8g4vdykazjpkfjnp2a&dl=1')\nsoybean_2019 = pd.read_csv('https://www.dropbox.com/scl/fi/72z6twn70nd22tnrmzn1y/standardized_2019_County_Summary_Merged.csv?rlkey=48zahb5ad25gfomzkyf3ghnjp&dl=1')\nsoybean_2020 = pd.read_csv('https://www.dropbox.com/scl/fi/37hczf92ev9n6vkd5tzuc/standardized_2020_County_Summary_Merged.csv?rlkey=pviu2572il5prpt69j13o3rux&dl=1')\nsoybean_2021 = pd.read_csv('https://www.dropbox.com/scl/fi/87vyoohpo80wyd9nyk0y9/standardized_2021_County_Summary_Merged.csv?rlkey=sh0gydlfw56tfjbp9zh3kqjyd&dl=1')\nsoybean_2022 = pd.read_csv('https://www.dropbox.com/scl/fi/jvv0iugqk9x045awb7ivk/standardized_2022_County_Summary_Merged.csv?rlkey=yx8yzpy8idff7761jpsmvgkjn&dl=1')\nsoybean_2023 = pd.read_csv('https://www.dropbox.com/scl/fi/09ger7p2qetczj4p6u0nk/standardized_2023_County_Summary_Merged.csv?rlkey=va8suq9c9wt0psoy0f1iden9m&dl=1')\n\n\nsoybean_list = [soybean_2018, soybean_2019, soybean_2020, soybean_2021, soybean_2022, soybean_2023]\nsoybean_df = pd.concat(soybean_list)\nsoybean_df = soybean_df.drop(['NAME','GEOID'], axis=1)\nsoybean_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 316 entries, 0 to 52\nData columns (total 6 columns):\n #   Column  Non-Null Count  Dtype  \n---  ------  --------------  -----  \n 0   LST     316 non-null    float64\n 1   SMS     316 non-null    float64\n 2   SAR     316 non-null    float64\n 3   PA      316 non-null    float64\n 4   NDVI    316 non-null    float64\n 5   YIELD   316 non-null    float64\ndtypes: float64(6)\nmemory usage: 17.3 KB\n\n\n\nsoybean_df.YIELD.hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n# Correlation coefficients\ncorrelation_matrix = soybean_df.corr()\n\nplt.rcParams[\"axes.grid\"] = False\nf = plt.figure(figsize=(19, 15))\nplt.matshow(soybean_df.corr(), fignum=f.number,cmap='bwr')\nplt.xticks(range(soybean_df.shape[1]), soybean_df.columns, fontsize=14, rotation=90)\nplt.yticks(range(soybean_df.shape[1]), soybean_df.columns, fontsize=14)\n\nfor i in range(soybean_df.shape[1]):\n    for j in range(soybean_df.shape[1]):\n        plt.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}', ha='center', va='center', fontsize=12, color='black')\n\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)\n\nplt.savefig('correlation_matrix_soybean.png', bbox_inches='tight')\n\nplt.title('Correlation Matrix', fontsize=16)\n\nText(0.5, 1.0, 'Correlation Matrix')"
  },
  {
    "objectID": "Modeling/old_supervised_learning.html#train-test-data-split",
    "href": "Modeling/old_supervised_learning.html#train-test-data-split",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "2. Train & Test Data Split",
    "text": "2. Train & Test Data Split\n\n#split the dataset\nX = soybean_df.drop('YIELD', axis=1)\ny = soybean_df['YIELD']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=100)"
  },
  {
    "objectID": "Modeling/old_supervised_learning.html#model-training-and-parameter-tuning",
    "href": "Modeling/old_supervised_learning.html#model-training-and-parameter-tuning",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "3. Model Training and Parameter Tuning",
    "text": "3. Model Training and Parameter Tuning\n\n3.1. Linear Regression (LR)\n\nmodel_lr = LinearRegression()\n\nmodel_lr.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\n3.2. Random Forest Regressor (RF)\n\n# values of max_depth and min_samples_split\nhyperparameters = {'max_depth':[3,5,10,20,30], 'min_samples_split':[2,4,6,8,10]}\n\n\nrandomState_dt = 10000\nmodel_rf = RandomForestRegressor(random_state=randomState_dt)\n\n# cv=5 by default, which means 5-fold cross-validation\nclf = GridSearchCV(model_rf, hyperparameters)\n\nclf.fit(X_train, y_train)\n\n# we can query the best parameter value and its accuracy score\nprint (\"The best parameter value is: \")\nprint (clf.best_params_)\nprint (\"The best score is: \")\nprint (clf.best_score_)\n\nThe best parameter value is: \n{'max_depth': 3, 'min_samples_split': 2}\nThe best score is: \n0.5096073244215513\n\n\n\n# Train the final RF\nrf_final = RandomForestRegressor(max_depth=clf.best_params_['max_depth'], min_samples_split=clf.best_params_['min_samples_split'], random_state=randomState_dt)\nrf_final.fit(X_train, y_train)\n\nRandomForestRegressor(max_depth=3, random_state=10000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_depth=3, random_state=10000)\n\n\n\n\n3.3. Gradient Boosting Regressor (XGB)\n\nimport warnings\n\n# 设置忽略 FutureWarning 类型的警告\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\n# model_xgb =\n# soybean_df\n\n# values of max_depth and min_samples_split\nhyperparameters = {'max_depth':[2,4,6,8,10], 'n_estimators':[4,8,12,16,20]}\n\nrandomState_xgb = 125\nxgb = XGBRegressor(random_state=randomState_xgb)\n\n# cv=5 by default, which means 5-fold cross-validation\ngscv_xgb = GridSearchCV(xgb, hyperparameters)\n\ngscv_xgb.fit(X_train, y_train)\n\n# we can query the best parameter value and its accuracy score\nprint (\"The best parameter value is: \")\nprint (gscv_xgb.best_params_)\nprint (\"The best score is: \")\nprint (gscv_xgb.best_score_)\n\nThe best parameter value is: \n{'max_depth': 4, 'n_estimators': 8}\nThe best score is: \n0.5164364211456384\n\n\n\n\n3.4. Artificial Neural Network (ANN)\n\nmodel_ann = keras.Sequential([\n    layers.Input(shape=(5,)),  # Input layer\n    layers.Dense(128, activation='relu'),  # Hidden layer with ReLU activation\n    layers.Dropout(0.5),  # Dropout layer for regularization\n    layers.Dense(64, activation='relu'),  # Additional hidden layer\n    layers.Dropout(0.3),  # Another dropout layer\n    layers.Dense(1)  # Output layer\n])\n\n#measuring the training with certain metrics\nmodel_ann.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n\n\n#train the model\nmodel_ann.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n\nEpoch 1/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 15ms/step - accuracy: 0.0000e+00 - loss: 959.7900 - val_accuracy: 0.0000e+00 - val_loss: 902.0046\nEpoch 2/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 879.0038 - val_accuracy: 0.0000e+00 - val_loss: 858.4245\nEpoch 3/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 843.1398 - val_accuracy: 0.0000e+00 - val_loss: 810.4407\nEpoch 4/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 788.5454 - val_accuracy: 0.0000e+00 - val_loss: 753.4695\nEpoch 5/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 715.2192 - val_accuracy: 0.0000e+00 - val_loss: 685.0058\nEpoch 6/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 651.2931 - val_accuracy: 0.0000e+00 - val_loss: 604.6999\nEpoch 7/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 577.6396 - val_accuracy: 0.0000e+00 - val_loss: 514.6031\nEpoch 8/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 469.3680 - val_accuracy: 0.0000e+00 - val_loss: 421.3022\nEpoch 9/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 366.2227 - val_accuracy: 0.0000e+00 - val_loss: 332.2936\nEpoch 10/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 265.8002 - val_accuracy: 0.0000e+00 - val_loss: 254.2769\nEpoch 11/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 234.7352 - val_accuracy: 0.0000e+00 - val_loss: 194.5780\nEpoch 12/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 180.6132 - val_accuracy: 0.0000e+00 - val_loss: 153.9670\nEpoch 13/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 149.5953 - val_accuracy: 0.0000e+00 - val_loss: 129.8330\nEpoch 14/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 124.6816 - val_accuracy: 0.0000e+00 - val_loss: 117.2346\nEpoch 15/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 121.1943 - val_accuracy: 0.0000e+00 - val_loss: 110.1871\nEpoch 16/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 117.8561 - val_accuracy: 0.0000e+00 - val_loss: 105.5259\nEpoch 17/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 121.5890 - val_accuracy: 0.0000e+00 - val_loss: 101.2197\nEpoch 18/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 109.2341 - val_accuracy: 0.0000e+00 - val_loss: 96.7412\nEpoch 19/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 103.4385 - val_accuracy: 0.0000e+00 - val_loss: 91.5101\nEpoch 20/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 97.1665 - val_accuracy: 0.0000e+00 - val_loss: 86.5653\nEpoch 21/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 93.7850 - val_accuracy: 0.0000e+00 - val_loss: 82.7462\nEpoch 22/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 89.1321 - val_accuracy: 0.0000e+00 - val_loss: 78.3264\nEpoch 23/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 76.7174 - val_accuracy: 0.0000e+00 - val_loss: 73.8021\nEpoch 24/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 70.9613 - val_accuracy: 0.0000e+00 - val_loss: 68.9846\nEpoch 25/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 77.6756 - val_accuracy: 0.0000e+00 - val_loss: 66.4345\nEpoch 26/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 77.7050 - val_accuracy: 0.0000e+00 - val_loss: 64.9911\nEpoch 27/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 76.1848 - val_accuracy: 0.0000e+00 - val_loss: 62.7581\nEpoch 28/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 70.3004 - val_accuracy: 0.0000e+00 - val_loss: 60.0328\nEpoch 29/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 73.9990 - val_accuracy: 0.0000e+00 - val_loss: 56.8050\nEpoch 30/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 78.8800 - val_accuracy: 0.0000e+00 - val_loss: 53.7323\nEpoch 31/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 66.8850 - val_accuracy: 0.0000e+00 - val_loss: 51.7016\nEpoch 32/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 75.2361 - val_accuracy: 0.0000e+00 - val_loss: 50.9663\nEpoch 33/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 81.9755 - val_accuracy: 0.0000e+00 - val_loss: 50.7918\nEpoch 34/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 56.9485 - val_accuracy: 0.0000e+00 - val_loss: 49.6811\nEpoch 35/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 72.3256 - val_accuracy: 0.0000e+00 - val_loss: 50.2865\nEpoch 36/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 56.2855 - val_accuracy: 0.0000e+00 - val_loss: 49.5687\nEpoch 37/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 60.3247 - val_accuracy: 0.0000e+00 - val_loss: 48.0057\nEpoch 38/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 57.6872 - val_accuracy: 0.0000e+00 - val_loss: 45.8631\nEpoch 39/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 64.1933 - val_accuracy: 0.0000e+00 - val_loss: 45.2905\nEpoch 40/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 48.4025 - val_accuracy: 0.0000e+00 - val_loss: 46.2005\nEpoch 41/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 54.7778 - val_accuracy: 0.0000e+00 - val_loss: 45.2328\nEpoch 42/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 56.5875 - val_accuracy: 0.0000e+00 - val_loss: 44.2572\nEpoch 43/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 59.8346 - val_accuracy: 0.0000e+00 - val_loss: 44.4137\nEpoch 44/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 55.8345 - val_accuracy: 0.0000e+00 - val_loss: 44.8654\nEpoch 45/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 53.2572 - val_accuracy: 0.0000e+00 - val_loss: 44.4291\nEpoch 46/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 45.8534 - val_accuracy: 0.0000e+00 - val_loss: 43.8641\nEpoch 47/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 57.4393 - val_accuracy: 0.0000e+00 - val_loss: 42.7443\nEpoch 48/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 60.9788 - val_accuracy: 0.0000e+00 - val_loss: 41.8159\nEpoch 49/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 42.6229 - val_accuracy: 0.0000e+00 - val_loss: 41.4254\nEpoch 50/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 57.7426 - val_accuracy: 0.0000e+00 - val_loss: 41.8901\nEpoch 51/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 56.0551 - val_accuracy: 0.0000e+00 - val_loss: 42.7297\nEpoch 52/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 51.7607 - val_accuracy: 0.0000e+00 - val_loss: 42.5607\nEpoch 53/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 57.8260 - val_accuracy: 0.0000e+00 - val_loss: 41.6408\nEpoch 54/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 64.6804 - val_accuracy: 0.0000e+00 - val_loss: 40.7811\nEpoch 55/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 56.3315 - val_accuracy: 0.0000e+00 - val_loss: 40.7287\nEpoch 56/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 53.8310 - val_accuracy: 0.0000e+00 - val_loss: 40.9344\nEpoch 57/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 66.7426 - val_accuracy: 0.0000e+00 - val_loss: 40.5510\nEpoch 58/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 51.7180 - val_accuracy: 0.0000e+00 - val_loss: 40.3516\nEpoch 59/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 45.7973 - val_accuracy: 0.0000e+00 - val_loss: 40.1543\nEpoch 60/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 51.5113 - val_accuracy: 0.0000e+00 - val_loss: 39.9080\nEpoch 61/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 49.4880 - val_accuracy: 0.0000e+00 - val_loss: 40.3282\nEpoch 62/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 53.7686 - val_accuracy: 0.0000e+00 - val_loss: 39.6643\nEpoch 63/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 46.3732 - val_accuracy: 0.0000e+00 - val_loss: 39.7313\nEpoch 64/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 47.8603 - val_accuracy: 0.0000e+00 - val_loss: 39.7500\nEpoch 65/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 63.6558 - val_accuracy: 0.0000e+00 - val_loss: 40.6301\nEpoch 66/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 44.6502 - val_accuracy: 0.0000e+00 - val_loss: 40.1328\nEpoch 67/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 53.2585 - val_accuracy: 0.0000e+00 - val_loss: 39.1841\nEpoch 68/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 45.3951 - val_accuracy: 0.0000e+00 - val_loss: 38.3957\nEpoch 69/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 58.2892 - val_accuracy: 0.0000e+00 - val_loss: 38.0657\nEpoch 70/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 48.7261 - val_accuracy: 0.0000e+00 - val_loss: 38.5843\nEpoch 71/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 50.6169 - val_accuracy: 0.0000e+00 - val_loss: 40.1153\nEpoch 72/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 51.1171 - val_accuracy: 0.0000e+00 - val_loss: 41.1834\nEpoch 73/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 50.2808 - val_accuracy: 0.0000e+00 - val_loss: 39.6826\nEpoch 74/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 42.6859 - val_accuracy: 0.0000e+00 - val_loss: 38.0941\nEpoch 75/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 45.4503 - val_accuracy: 0.0000e+00 - val_loss: 37.5293\nEpoch 76/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 48.5696 - val_accuracy: 0.0000e+00 - val_loss: 37.4194\nEpoch 77/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 56.7574 - val_accuracy: 0.0000e+00 - val_loss: 38.8160\nEpoch 78/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 53.7819 - val_accuracy: 0.0000e+00 - val_loss: 38.5011\nEpoch 79/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 47.8279 - val_accuracy: 0.0000e+00 - val_loss: 37.4514\nEpoch 80/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 51.9195 - val_accuracy: 0.0000e+00 - val_loss: 37.1875\nEpoch 81/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 46.9745 - val_accuracy: 0.0000e+00 - val_loss: 37.1730\nEpoch 82/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 48.9286 - val_accuracy: 0.0000e+00 - val_loss: 37.1596\nEpoch 83/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 55.4300 - val_accuracy: 0.0000e+00 - val_loss: 37.9296\nEpoch 84/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 43.6146 - val_accuracy: 0.0000e+00 - val_loss: 39.4319\nEpoch 85/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 62.0971 - val_accuracy: 0.0000e+00 - val_loss: 38.7389\nEpoch 86/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 47.6694 - val_accuracy: 0.0000e+00 - val_loss: 37.5515\nEpoch 87/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 50.2839 - val_accuracy: 0.0000e+00 - val_loss: 37.1005\nEpoch 88/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 50.0970 - val_accuracy: 0.0000e+00 - val_loss: 36.6832\nEpoch 89/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 54.4404 - val_accuracy: 0.0000e+00 - val_loss: 36.5272\nEpoch 90/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 43.9714 - val_accuracy: 0.0000e+00 - val_loss: 37.6199\nEpoch 91/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 47.4199 - val_accuracy: 0.0000e+00 - val_loss: 39.6995\nEpoch 92/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 52.6187 - val_accuracy: 0.0000e+00 - val_loss: 38.5255\nEpoch 93/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 47.8058 - val_accuracy: 0.0000e+00 - val_loss: 36.4269\nEpoch 94/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 51.9957 - val_accuracy: 0.0000e+00 - val_loss: 36.5738\nEpoch 95/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 46.2351 - val_accuracy: 0.0000e+00 - val_loss: 36.7518\nEpoch 96/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 46.2549 - val_accuracy: 0.0000e+00 - val_loss: 38.4598\nEpoch 97/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 41.6489 - val_accuracy: 0.0000e+00 - val_loss: 38.2157\nEpoch 98/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 53.2832 - val_accuracy: 0.0000e+00 - val_loss: 37.1601\nEpoch 99/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 44.0120 - val_accuracy: 0.0000e+00 - val_loss: 36.8643\nEpoch 100/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 52.3485 - val_accuracy: 0.0000e+00 - val_loss: 36.6902\n\n\n&lt;keras.src.callbacks.history.History at 0x7fb8b3d85050&gt;"
  },
  {
    "objectID": "Modeling/old_supervised_learning.html#model-evaluation-and-performance-comparison",
    "href": "Modeling/old_supervised_learning.html#model-evaluation-and-performance-comparison",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "4. Model Evaluation and Performance Comparison",
    "text": "4. Model Evaluation and Performance Comparison\nComparing the performance with using R2 and Root Mean Squared Error (RMSE).\n\n4.1. E. Linear Regression (LR)\n\ntrain_predictions = model_lr.predict(X_train)\ntest_predictions = model_lr.predict(X_test)\n\nr2_train_lr = r2_score(y_train, train_predictions)\nr2_test_lr = r2_score(y_test, test_predictions)\n\nrmse_train_lr = mean_squared_error(y_train, train_predictions, squared=False)\nrmse_test_lr = mean_squared_error(y_test, test_predictions, squared=False)\n\nprint(f\"Training R^2: {r2_train_lr:.4f}\")\nprint(f\"Test R^2: {r2_test_lr:.4f}\")\nprint(f\"Training RMSE: {rmse_train_lr:.4f}\")\nprint(f\"Test RMSE: {rmse_test_lr:.4f}\")\n\nTraining R^2: 0.5434\nTest R^2: 0.4428\nTraining RMSE: 5.1400\nTest RMSE: 5.9105\n\n\n\n\n4.2. E. Random Forest Regressor (RF)\n\nr2_train_rf = rf_final.score(X=X_train, y=y_train)\nr2_test_rf = rf_final.score(X=X_test, y=y_test)\n\nprint(\"R2 on the training data:\")\nprint(r2_train_rf)\nprint(\"R2 on the testing data:\")\nprint(r2_test_rf)\n\nR2 on the training data:\n0.6832842781707053\nR2 on the testing data:\n0.49405126065030003\n\n\n\nrmse_train_rf = mean_squared_error(y_train, rf_final.predict(X_train), squared=False)\nrmse_test_rf = mean_squared_error(y_test, rf_final.predict(X_test), squared=False)\n\nprint(\"RMSE on the training data:\")\nprint(rmse_train_rf)\nprint(\"RMSE on the testing data:\")\nprint(rmse_test_rf)\n\nRMSE on the training data:\n4.280907710841681\nRMSE on the testing data:\n5.632071090003778\n\n\n\n# Calculate and plot the feature importance of the RF model\nimp = rfpimp.importances(rf_final, X_test, y_test)\nprint(imp)\nviz = rfpimp.plot_importances(imp)\nviz.view()\n\n                                      Importance\nFeature                                         \nNDVI                                      0.5739\nSAR                                       0.0049\nLST                                       0.0025\nSMS                                       0.0012\nPA                                       -0.0023\n\n\n\n\n\n\n\n4.3. E. Gradient Boosting Regressor (XGB)\n\nmodel_xgb = XGBRegressor(max_depth=gscv_xgb.best_params_['max_depth'], n_estimators=gscv_xgb.best_params_['n_estimators'], random_state=randomState_xgb)\nmodel_xgb.fit(X_train, y_train)\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=4, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=8, n_jobs=None,\n             num_parallel_tree=None, random_state=125, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=4, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=8, n_jobs=None,\n             num_parallel_tree=None, random_state=125, ...)\n\n\n\n# r2_train_xgb, r2_test_xgb, rmse_train_xgb, rmse_test_xgb\nr2_train_xgb = model_xgb.score(X=X_train, y=y_train)\nr2_test_xgb = model_xgb.score(X=X_test, y=y_test)\nrmse_train_xgb = mean_squared_error(y_train, model_xgb.predict(X_train), squared=False)\nrmse_test_xgb = mean_squared_error(y_test, model_xgb.predict(X_test), squared=False)\n\n\nprint(\"R2 on the training data:\")\nprint(r2_train_xgb)\nprint(\"R2 on the testing data:\")\nprint(r2_test_xgb)\n\nR2 on the training data:\n0.838531183577921\nR2 on the testing data:\n0.4787396524931533\n\n\n\nprint(\"RMSE on the training data:\")\nprint(rmse_train_xgb)\nprint(\"RMSE on the testing data:\")\nprint(rmse_test_xgb)\n\nRMSE on the training data:\n3.056647692221936\nRMSE on the testing data:\n5.716658029989171\n\n\n\nimp_xgb = rfpimp.importances(model_xgb, X_test, y_test) # permutation\nprint(imp_xgb)\nviz_xgb = rfpimp.plot_importances(imp_xgb)\nviz_xgb.view()\n\n                                      Importance\nFeature                                         \nNDVI                                      0.5805\nSAR                                       0.0519\nPA                                        0.0441\nSMS                                       0.0246\nLST                                       0.0053\n\n\n\n\n\n\n\n4.4. E. Artificial Neural Network (ANN)\n\n#predictions\ny_pred_train_ann = model_ann.predict(X_train).flatten()\ny_pred_test_ann = model_ann.predict(X_test).flatten()\n\n#Compute R2 and RMSE\nr2_train_ann = np.round(r2_score(y_train, y_pred_train_ann),2)\nr2_test_ann = np.round(r2_score(y_test, y_pred_test_ann),2)\nrmse_train_ann = np.round(np.sqrt(mean_squared_error(y_train, y_pred_train_ann)),2)\nrmse_test_ann = np.round(np.sqrt(mean_squared_error(y_test, y_pred_test_ann)),2)\n\n#print the result\nprint(\"Train R2:\", r2_train_ann)\nprint(\"Test R2:\", r2_test_ann)\nprint(\"Train RMSE:\", rmse_train_ann)\nprint(\"Test RMSE:\", rmse_test_ann)\n\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 3ms/step \n3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \nTrain R2: 0.6\nTest R2: 0.45\nTrain RMSE: 4.84\nTest RMSE: 5.85\n\n\n\n#crosscheck the y value between real and predicted\ncrosscheck_y_dict = {\n    'y_test' : y_test,\n    'y_pred' : np.round(y_pred_test_ann,0),\n    'delta' : np.abs(np.round((y_test - y_pred_test_ann),0))\n}\n\n#plotting histogram\ncrosscheck_y_df = pd.DataFrame(crosscheck_y_dict)\nplt.hist(crosscheck_y_df['delta'], bins=10)\nplt.xlabel(f'Delta\\nR2_test = {r2_test_ann}, RMSE_test = {rmse_test_ann}, delta_max = {crosscheck_y_df.delta.max()}')\nplt.ylabel('Frequency (Number of Data)')\nplt.title(f\"Accuracy of ANN Model Based on Delta of Y Test and Y Pred)\")\nplt.show()\n\n\n\n\n\n\n4.5. Model Performance Comparison\n\n#please input your metrics in here\nmetrics_dict = {\n    'metrics': [\"Train R2\",\"Test R2\",\"Train RMSE\",\"Test RMSE\"],\n    'LR': [r2_train_lr, r2_test_lr, rmse_train_lr, rmse_test_lr],\n    'RF': [r2_train_rf, r2_test_rf, rmse_train_rf, rmse_test_rf],\n    'XGB': [r2_train_xgb, r2_test_xgb, rmse_train_xgb, rmse_test_xgb],\n    'ANN': [r2_train_ann, r2_test_ann, rmse_train_ann, rmse_test_ann]\n}\n\n#create dataframe\nmetrics_df = pd.DataFrame(metrics_dict)\nmetrics_df.set_index('metrics')\n\n\n\n\n\n\n\n\nLR\nRF\nXGB\nANN\n\n\nmetrics\n\n\n\n\n\n\n\n\nTrain R2\n0.5434\n0.6833\n0.8385\n0.6000\n\n\nTest R2\n0.4428\n0.4941\n0.4787\n0.4500\n\n\nTrain RMSE\n5.1400\n4.2809\n3.0566\n4.8400\n\n\nTest RMSE\n5.9105\n5.6321\n5.7167\n5.8500\n\n\n\n\n\n\n\nBased on the comparison, it can be said that the best model that can be used for estimating crop yield is []. From this point, [] model would be used for estimation phase (step 5)."
  },
  {
    "objectID": "Modeling/old_supervised_learning.html#crop-yield-estimation-and-export-result",
    "href": "Modeling/old_supervised_learning.html#crop-yield-estimation-and-export-result",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "5. Crop Yield Estimation and Export Result",
    "text": "5. Crop Yield Estimation and Export Result"
  },
  {
    "objectID": "Modeling/Wheat2_supervised_learning.html",
    "href": "Modeling/Wheat2_supervised_learning.html",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "",
    "text": "This jupyter notebook is addressed for creating model that can estimate crop yield in North Dakota, United States. The detail workflow of this project can be seen in here."
  },
  {
    "objectID": "Modeling/Wheat2_supervised_learning.html#install-load-packages",
    "href": "Modeling/Wheat2_supervised_learning.html#install-load-packages",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "0. Install & Load Packages",
    "text": "0. Install & Load Packages\n\n#install packages\n!pip install tensorflow\n\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.11/site-packages (2.16.1)\nRequirement already satisfied: absl-py&gt;=1.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.1.0)\nRequirement already satisfied: astunparse&gt;=1.6.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers&gt;=23.5.26 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,&gt;=0.2.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta&gt;=0.1.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py&gt;=3.10.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.10.0)\nRequirement already satisfied: libclang&gt;=13.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: ml-dtypes~=0.3.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.3.2)\nRequirement already satisfied: opt-einsum&gt;=2.3.2 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (23.1)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,&lt;5.0.0dev,&gt;=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.23.3)\nRequirement already satisfied: requests&lt;3,&gt;=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.31.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (68.1.2)\nRequirement already satisfied: six&gt;=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor&gt;=1.1.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions&gt;=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.7.1)\nRequirement already satisfied: wrapt&gt;=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.15.0)\nRequirement already satisfied: grpcio&lt;2.0,&gt;=1.24.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.62.1)\nRequirement already satisfied: tensorboard&lt;2.17,&gt;=2.16 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.16.2)\nRequirement already satisfied: keras&gt;=3.0.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.2.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem&gt;=0.23.1 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (0.36.0)\nRequirement already satisfied: numpy&lt;2.0.0,&gt;=1.23.5 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.24.4)\nRequirement already satisfied: wheel&lt;1.0,&gt;=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse&gt;=1.6.0-&gt;tensorflow) (0.41.2)\nRequirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from keras&gt;=3.0.0-&gt;tensorflow) (13.5.3)\nRequirement already satisfied: namex in /opt/conda/lib/python3.11/site-packages (from keras&gt;=3.0.0-&gt;tensorflow) (0.0.7)\nRequirement already satisfied: optree in /opt/conda/lib/python3.11/site-packages (from keras&gt;=3.0.0-&gt;tensorflow) (0.11.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/conda/lib/python3.11/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (3.2.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.11/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (2.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests&lt;3,&gt;=2.21.0-&gt;tensorflow) (2023.7.22)\nRequirement already satisfied: markdown&gt;=2.6.8 in /opt/conda/lib/python3.11/site-packages (from tensorboard&lt;2.17,&gt;=2.16-&gt;tensorflow) (3.6)\nRequirement already satisfied: tensorboard-data-server&lt;0.8.0,&gt;=0.7.0 in /opt/conda/lib/python3.11/site-packages (from tensorboard&lt;2.17,&gt;=2.16-&gt;tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug&gt;=1.0.1 in /opt/conda/lib/python3.11/site-packages (from tensorboard&lt;2.17,&gt;=2.16-&gt;tensorflow) (3.0.2)\nRequirement already satisfied: MarkupSafe&gt;=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard&lt;2.17,&gt;=2.16-&gt;tensorflow) (2.1.3)\nRequirement already satisfied: markdown-it-py&gt;=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich-&gt;keras&gt;=3.0.0-&gt;tensorflow) (3.0.0)\nRequirement already satisfied: pygments&lt;3.0.0,&gt;=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich-&gt;keras&gt;=3.0.0-&gt;tensorflow) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py&gt;=2.2.0-&gt;rich-&gt;keras&gt;=3.0.0-&gt;tensorflow) (0.1.0)\n\n\n\n#load packages\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n#packages for manipulating dataframe\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom shapely.geometry import Point\nimport sklearn\n\n#packages for machine learning\n##train-test-split\nfrom sklearn.model_selection import train_test_split, GridSearchCV, validation_curve\n\n##method 1: Linear Regression (LR)\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import cross_val_score\n\n##method 2: Random Forest Regressor (RF)\nimport rfpimp\nfrom sklearn.ensemble import RandomForestRegressor\n\n##method 3: Gradient Boosting Regressor (XGB)\nimport xgboost\nfrom xgboost import XGBRegressor\n\n##method 4: Artificial Neural Network (ANN)\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n##cross validation\n\n##evaluation metrics (R2 and RMSE)\nfrom sklearn.metrics import r2_score, mean_squared_error\n\n#data visualization\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_rows', 300) # specifies number of rows to show\npd.options.display.float_format = '{:40,.4f}'.format # specifies default number format to 4 decimal places\nplt.style.use('ggplot') # specifies that graphs should use ggplot styling\n%matplotlib inline"
  },
  {
    "objectID": "Modeling/Wheat2_supervised_learning.html#load-cleaning-data",
    "href": "Modeling/Wheat2_supervised_learning.html#load-cleaning-data",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "1. Load & Cleaning Data",
    "text": "1. Load & Cleaning Data\n\n#load data\nwheat_2018 = pd.read_csv('https://www.dropbox.com/scl/fi/qyk0rjcvay0azb6k3cxzf/wheat_2018.csv?rlkey=1vmvrf6o9jiik4tsjax68ps7b&dl=1')\nwheat_2019 = pd.read_csv('https://www.dropbox.com/scl/fi/g5vws0nge1drxu675fb92/wheat_2019.csv?rlkey=fq5h3fjuz5n9qgmcef9khlcan&dl=1')\nwheat_2020 = pd.read_csv('https://www.dropbox.com/scl/fi/xczuhmh4l66fcci6hdltu/wheat_2020.csv?rlkey=vkdy66mfbjd5dxr657uew2uxn&dl=1')\nwheat_2021 = pd.read_csv('https://www.dropbox.com/scl/fi/8swbr3pbosrx52tq5s1xr/wheat_2021.csv?rlkey=j74dzta87987vnaprsdr0fjtx&dl=1')\nwheat_2022 = pd.read_csv('https://www.dropbox.com/scl/fi/9m8c7iu1wc07ispw2vhy3/wheat_2022.csv?rlkey=101ujmr4w4v0tv2stm4kr8z5u&dl=1')\nwheat_2023 = pd.read_csv('https://www.dropbox.com/scl/fi/eq4rmh61kr1oolm1txe2p/wheat_2023.csv?rlkey=yldyhdickfvnpm7ssv3lx6d55&dl=1')\n\n\nwheat_list = [wheat_2018, wheat_2019, wheat_2020, wheat_2021, wheat_2022, wheat_2023]\nwheat_df = pd.concat(wheat_list)\n# wheat_df = wheat_df.drop(['NAME','GEOID'], axis=1)\n# Precipitation; LST_DAY ; Soil_Moisture_am\nwheat_df = wheat_df.drop(['NAME','GEOID','Precipitation','LST_DAY','Soil_Moisture_am'], axis=1)\nwheat_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 318 entries, 0 to 52\nData columns (total 7 columns):\n #   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n 0   Yield             318 non-null    float64\n 1   ET                318 non-null    float64\n 2   LST_NIGHT         318 non-null    float64\n 3   NDVI              318 non-null    float64\n 4   PAR               318 non-null    float64\n 5   SAR               318 non-null    float64\n 6   Soil_Moisture_pm  318 non-null    float64\ndtypes: float64(7)\nmemory usage: 19.9 KB\n\n\n\n# 导出 DataFrame 到 CSV 文件\nwheat_df.to_csv('wheat.csv', index=False)\n\n\nwheat_df.Yield.hist()\n\n&lt;Axes: &gt;\n\n\n\n\n\n\n# Correlation coefficients\ncorrelation_matrix = wheat_df.corr()\n\nplt.rcParams[\"axes.grid\"] = False\nf = plt.figure(figsize=(19, 15))\nplt.matshow(wheat_df.corr(), fignum=f.number,cmap='bwr')\nplt.xticks(range(wheat_df.shape[1]), wheat_df.columns, fontsize=14, rotation=90)\nplt.yticks(range(wheat_df.shape[1]), wheat_df.columns, fontsize=14)\n\nfor i in range(wheat_df.shape[1]):\n    for j in range(wheat_df.shape[1]):\n        plt.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}', ha='center', va='center', fontsize=12, color='black')\n\ncb = plt.colorbar()\ncb.ax.tick_params(labelsize=14)\n\nplt.savefig('correlation_matrix_soybean.png', bbox_inches='tight')\n\nplt.title('Correlation Matrix', fontsize=16)\n\nText(0.5, 1.0, 'Correlation Matrix')"
  },
  {
    "objectID": "Modeling/Wheat2_supervised_learning.html#train-test-data-split",
    "href": "Modeling/Wheat2_supervised_learning.html#train-test-data-split",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "2.1 Train & Test Data Split",
    "text": "2.1 Train & Test Data Split\n\n#split the dataset\nX = wheat_df.drop('Yield', axis=1)\ny = wheat_df['Yield']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=50)"
  },
  {
    "objectID": "Modeling/Wheat2_supervised_learning.html#train-test-data-standarderlized",
    "href": "Modeling/Wheat2_supervised_learning.html#train-test-data-standarderlized",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "2.2 Train & Test Data Standarderlized",
    "text": "2.2 Train & Test Data Standarderlized\n\ndef standardize_columns(file_name, columns_to_standardize):\n    scaler = StandardScaler()\n\n    df = file_name\n    df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n    return df\n\ndef standardize_series(series):\n    scaler = StandardScaler()\n    series = scaler.fit_transform(series.values.reshape(-1, 1)).flatten()\n    return series\n    \n# X_columns = ['ET','LST_DAY','LST_NIGHT','NDVI','Precipitation', 'PAR','SAR', 'Soil_Moisture_am','Soil_Moisture_pm']\n# Soil_Moisture_am\nX_columns = ['ET','LST_NIGHT','NDVI', 'PAR','SAR','Soil_Moisture_pm']\ny_columns = ['Yield']\n\nX_train = standardize_columns(X_train, X_columns)\nX_test = standardize_columns(X_test, X_columns)\n\n# y_train = standardize_series(y_train)\n# y_test = standardize_series(y_test)"
  },
  {
    "objectID": "Modeling/Wheat2_supervised_learning.html#model-training-and-parameter-tuning",
    "href": "Modeling/Wheat2_supervised_learning.html#model-training-and-parameter-tuning",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "3. Model Training and Parameter Tuning",
    "text": "3. Model Training and Parameter Tuning\n\n3.1. Linear Regression (LR)\n\nmodel_lr = LinearRegression()\n\n# Cross validation\nscores = cross_val_score(model_lr, X, y, cv=5, scoring='neg_mean_squared_error')\n\nmean_mse = np.mean(scores)\nstd_mse = np.std(scores)\n\nprint(f'Mean MSE: {mean_mse}')\nprint(f'Standard Deviation of MSE: {std_mse}')\n\nMean MSE: -34.518798672878525\nStandard Deviation of MSE: 9.445061469649747\n\n\n\nmodel_lr.fit(X_train, y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\n3.2. Random Forest Regressor (RF)\n\n# values of max_depth and min_samples_split\nhyperparameters = {'max_depth':[3,5,10,20,30], 'min_samples_split':[2,4,6,8,10]}\n\n\nrandomState_dt = 10000\nmodel_rf = RandomForestRegressor(random_state=randomState_dt)\n\n# cv=5 by default, which means 5-fold cross-validation\nclf = GridSearchCV(model_rf, hyperparameters)\n\nclf.fit(X_train, y_train)\n\n# we can query the best parameter value and its accuracy score\nprint (\"The best parameter value is: \")\nprint (clf.best_params_)\nprint (\"The best score is: \")\nprint (clf.best_score_)\n\nThe best parameter value is: \n{'max_depth': 10, 'min_samples_split': 10}\nThe best score is: \n0.6888061371401578\n\n\n\n# Train the final RF\nrf_final = RandomForestRegressor(max_depth=clf.best_params_['max_depth'], min_samples_split=clf.best_params_['min_samples_split'], random_state=randomState_dt)\nrf_final.fit(X_train, y_train)\n\nRandomForestRegressor(max_depth=10, min_samples_split=10, random_state=10000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestRegressorRandomForestRegressor(max_depth=10, min_samples_split=10, random_state=10000)\n\n\n\n\n3.3. Gradient Boosting Regressor (XGB)\n\nimport warnings\n\n# 设置忽略 FutureWarning 类型的警告\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\n# model_xgb =\n# soybean_df\n\n# values of max_depth and min_samples_split\nhyperparameters = {'max_depth':[2,4,6,8,10], 'n_estimators':[4,8,12,16,20]}\n\nrandomState_xgb = 125\nxgb = XGBRegressor(random_state=randomState_xgb)\n\n# cv=5 by default, which means 5-fold cross-validation\ngscv_xgb = GridSearchCV(xgb, hyperparameters)\n\ngscv_xgb.fit(X_train, y_train)\n\n# we can query the best parameter value and its accuracy score\nprint (\"The best parameter value is: \")\nprint (gscv_xgb.best_params_)\nprint (\"The best score is: \")\nprint (gscv_xgb.best_score_)\n\nThe best parameter value is: \n{'max_depth': 8, 'n_estimators': 20}\nThe best score is: \n0.9923195499625839\n\n\n\n\n3.4. Artificial Neural Network (ANN)\n\nmodel_ann = keras.Sequential([\n    layers.Input(shape=(9,)),  # Input layer\n    layers.Dense(128, activation='relu'),  # Hidden layer with ReLU activation\n    layers.Dropout(0.5),  # Dropout layer for regularization\n    layers.Dense(64, activation='relu'),  # Additional hidden layer\n    layers.Dropout(0.3),  # Another dropout layer\n    layers.Dense(1)  # Output layer\n])\n\n#measuring the training with certain metrics\nmodel_ann.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n\n\n#train the model\nmodel_ann.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n\nEpoch 1/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 1s 14ms/step - accuracy: 0.0000e+00 - loss: 2348.2444 - val_accuracy: 0.0000e+00 - val_loss: 2294.2520\nEpoch 2/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 2315.0215 - val_accuracy: 0.0000e+00 - val_loss: 2212.4778\nEpoch 3/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 2237.3042 - val_accuracy: 0.0000e+00 - val_loss: 2111.4783\nEpoch 4/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 2096.6111 - val_accuracy: 0.0000e+00 - val_loss: 1978.4889\nEpoch 5/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 1977.5494 - val_accuracy: 0.0000e+00 - val_loss: 1804.2450\nEpoch 6/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 1767.3147 - val_accuracy: 0.0000e+00 - val_loss: 1587.7260\nEpoch 7/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 1484.5956 - val_accuracy: 0.0000e+00 - val_loss: 1332.4340\nEpoch 8/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 1282.2721 - val_accuracy: 0.0000e+00 - val_loss: 1055.5273\nEpoch 9/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 967.6757 - val_accuracy: 0.0000e+00 - val_loss: 781.4720\nEpoch 10/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 725.3635 - val_accuracy: 0.0000e+00 - val_loss: 553.0772\nEpoch 11/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 511.4912 - val_accuracy: 0.0000e+00 - val_loss: 397.6915\nEpoch 12/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 371.6410 - val_accuracy: 0.0000e+00 - val_loss: 309.4381\nEpoch 13/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 381.1350 - val_accuracy: 0.0000e+00 - val_loss: 263.3522\nEpoch 14/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 306.6189 - val_accuracy: 0.0000e+00 - val_loss: 231.5027\nEpoch 15/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 321.3996 - val_accuracy: 0.0000e+00 - val_loss: 201.1707\nEpoch 16/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 308.0375 - val_accuracy: 0.0000e+00 - val_loss: 175.5851\nEpoch 17/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 220.9242 - val_accuracy: 0.0000e+00 - val_loss: 160.0740\nEpoch 18/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 254.7399 - val_accuracy: 0.0000e+00 - val_loss: 148.8470\nEpoch 19/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 208.1436 - val_accuracy: 0.0000e+00 - val_loss: 134.9997\nEpoch 20/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 188.5805 - val_accuracy: 0.0000e+00 - val_loss: 123.6581\nEpoch 21/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 259.6965 - val_accuracy: 0.0000e+00 - val_loss: 114.3191\nEpoch 22/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 225.0826 - val_accuracy: 0.0000e+00 - val_loss: 107.3627\nEpoch 23/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 174.4936 - val_accuracy: 0.0000e+00 - val_loss: 101.5680\nEpoch 24/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 194.6940 - val_accuracy: 0.0000e+00 - val_loss: 97.3500\nEpoch 25/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 164.3917 - val_accuracy: 0.0000e+00 - val_loss: 90.4988\nEpoch 26/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 164.2730 - val_accuracy: 0.0000e+00 - val_loss: 87.0279\nEpoch 27/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 163.8259 - val_accuracy: 0.0000e+00 - val_loss: 86.4885\nEpoch 28/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 177.2993 - val_accuracy: 0.0000e+00 - val_loss: 85.5943\nEpoch 29/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 188.3952 - val_accuracy: 0.0000e+00 - val_loss: 84.5504\nEpoch 30/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 180.8502 - val_accuracy: 0.0000e+00 - val_loss: 81.6582\nEpoch 31/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 147.1124 - val_accuracy: 0.0000e+00 - val_loss: 79.2850\nEpoch 32/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 137.2493 - val_accuracy: 0.0000e+00 - val_loss: 75.8591\nEpoch 33/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 146.1477 - val_accuracy: 0.0000e+00 - val_loss: 75.6277\nEpoch 34/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 156.7099 - val_accuracy: 0.0000e+00 - val_loss: 73.9673\nEpoch 35/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 136.9605 - val_accuracy: 0.0000e+00 - val_loss: 71.6457\nEpoch 36/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 118.4153 - val_accuracy: 0.0000e+00 - val_loss: 71.3647\nEpoch 37/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 131.1028 - val_accuracy: 0.0000e+00 - val_loss: 70.2476\nEpoch 38/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 116.8714 - val_accuracy: 0.0000e+00 - val_loss: 68.0001\nEpoch 39/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 145.7686 - val_accuracy: 0.0000e+00 - val_loss: 66.9965\nEpoch 40/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 146.6443 - val_accuracy: 0.0000e+00 - val_loss: 67.0357\nEpoch 41/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 143.9851 - val_accuracy: 0.0000e+00 - val_loss: 65.6160\nEpoch 42/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 124.8958 - val_accuracy: 0.0000e+00 - val_loss: 65.4467\nEpoch 43/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 131.7978 - val_accuracy: 0.0000e+00 - val_loss: 62.4357\nEpoch 44/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 138.7973 - val_accuracy: 0.0000e+00 - val_loss: 60.9795\nEpoch 45/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 98.3516 - val_accuracy: 0.0000e+00 - val_loss: 61.8930\nEpoch 46/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.0000e+00 - loss: 111.2170 - val_accuracy: 0.0000e+00 - val_loss: 63.2430\nEpoch 47/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 102.5599 - val_accuracy: 0.0000e+00 - val_loss: 59.5875\nEpoch 48/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 115.6318 - val_accuracy: 0.0000e+00 - val_loss: 56.4804\nEpoch 49/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 129.6184 - val_accuracy: 0.0000e+00 - val_loss: 56.1119\nEpoch 50/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 139.3834 - val_accuracy: 0.0000e+00 - val_loss: 59.0773\nEpoch 51/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 126.3853 - val_accuracy: 0.0000e+00 - val_loss: 56.2493\nEpoch 52/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 108.6963 - val_accuracy: 0.0000e+00 - val_loss: 54.6014\nEpoch 53/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 139.8010 - val_accuracy: 0.0000e+00 - val_loss: 54.5905\nEpoch 54/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 122.4168 - val_accuracy: 0.0000e+00 - val_loss: 55.1012\nEpoch 55/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 125.4076 - val_accuracy: 0.0000e+00 - val_loss: 52.9402\nEpoch 56/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 104.3056 - val_accuracy: 0.0000e+00 - val_loss: 49.8272\nEpoch 57/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 108.5015 - val_accuracy: 0.0000e+00 - val_loss: 50.1172\nEpoch 58/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 114.8999 - val_accuracy: 0.0000e+00 - val_loss: 51.1561\nEpoch 59/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 107.7712 - val_accuracy: 0.0000e+00 - val_loss: 49.9431\nEpoch 60/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 106.7005 - val_accuracy: 0.0000e+00 - val_loss: 49.4118\nEpoch 61/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 112.2914 - val_accuracy: 0.0000e+00 - val_loss: 51.0100\nEpoch 62/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 118.0214 - val_accuracy: 0.0000e+00 - val_loss: 51.2380\nEpoch 63/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 104.4262 - val_accuracy: 0.0000e+00 - val_loss: 50.8532\nEpoch 64/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 98.6371 - val_accuracy: 0.0000e+00 - val_loss: 48.0555\nEpoch 65/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 120.1868 - val_accuracy: 0.0000e+00 - val_loss: 48.6442\nEpoch 66/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 125.9646 - val_accuracy: 0.0000e+00 - val_loss: 49.5364\nEpoch 67/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 110.5141 - val_accuracy: 0.0000e+00 - val_loss: 49.9628\nEpoch 68/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 104.9525 - val_accuracy: 0.0000e+00 - val_loss: 48.6041\nEpoch 69/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 110.4036 - val_accuracy: 0.0000e+00 - val_loss: 46.5019\nEpoch 70/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 97.6758 - val_accuracy: 0.0000e+00 - val_loss: 44.4252\nEpoch 71/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 118.9777 - val_accuracy: 0.0000e+00 - val_loss: 42.5766\nEpoch 72/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 76.9044 - val_accuracy: 0.0000e+00 - val_loss: 43.1743\nEpoch 73/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 99.4426 - val_accuracy: 0.0000e+00 - val_loss: 43.7194\nEpoch 74/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 84.9709 - val_accuracy: 0.0000e+00 - val_loss: 42.0428\nEpoch 75/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 106.3163 - val_accuracy: 0.0000e+00 - val_loss: 40.8084\nEpoch 76/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 103.4784 - val_accuracy: 0.0000e+00 - val_loss: 42.2662\nEpoch 77/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 100.8092 - val_accuracy: 0.0000e+00 - val_loss: 41.5544\nEpoch 78/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 89.3237 - val_accuracy: 0.0000e+00 - val_loss: 38.9430\nEpoch 79/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 86.0734 - val_accuracy: 0.0000e+00 - val_loss: 36.9373\nEpoch 80/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 108.4399 - val_accuracy: 0.0000e+00 - val_loss: 37.1271\nEpoch 81/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 106.9971 - val_accuracy: 0.0000e+00 - val_loss: 40.8744\nEpoch 82/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 93.6218 - val_accuracy: 0.0000e+00 - val_loss: 42.6647\nEpoch 83/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 97.7197 - val_accuracy: 0.0000e+00 - val_loss: 41.9427\nEpoch 84/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 96.9543 - val_accuracy: 0.0000e+00 - val_loss: 42.8357\nEpoch 85/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 88.3149 - val_accuracy: 0.0000e+00 - val_loss: 42.7525\nEpoch 86/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 84.8006 - val_accuracy: 0.0000e+00 - val_loss: 40.6215\nEpoch 87/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 93.7265 - val_accuracy: 0.0000e+00 - val_loss: 38.5561\nEpoch 88/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 7ms/step - accuracy: 0.0000e+00 - loss: 80.5097 - val_accuracy: 0.0000e+00 - val_loss: 36.6440\nEpoch 89/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 102.4726 - val_accuracy: 0.0000e+00 - val_loss: 34.5292\nEpoch 90/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 100.9845 - val_accuracy: 0.0000e+00 - val_loss: 34.0241\nEpoch 91/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 87.2559 - val_accuracy: 0.0000e+00 - val_loss: 33.8655\nEpoch 92/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step - accuracy: 0.0000e+00 - loss: 99.6406 - val_accuracy: 0.0000e+00 - val_loss: 33.0473\nEpoch 93/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 105.7855 - val_accuracy: 0.0000e+00 - val_loss: 34.1316\nEpoch 94/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 94.2116 - val_accuracy: 0.0000e+00 - val_loss: 36.4268\nEpoch 95/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - accuracy: 0.0000e+00 - loss: 89.5221 - val_accuracy: 0.0000e+00 - val_loss: 34.6093\nEpoch 96/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 94.2593 - val_accuracy: 0.0000e+00 - val_loss: 31.7085\nEpoch 97/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 85.8353 - val_accuracy: 0.0000e+00 - val_loss: 32.9171\nEpoch 98/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 82.7186 - val_accuracy: 0.0000e+00 - val_loss: 35.9330\nEpoch 99/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 79.3137 - val_accuracy: 0.0000e+00 - val_loss: 36.1082\nEpoch 100/100\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step - accuracy: 0.0000e+00 - loss: 96.1318 - val_accuracy: 0.0000e+00 - val_loss: 34.4059\n\n\n&lt;keras.src.callbacks.history.History at 0x7fe5d45a8b50&gt;"
  },
  {
    "objectID": "Modeling/Wheat2_supervised_learning.html#model-evaluation-and-performance-comparison",
    "href": "Modeling/Wheat2_supervised_learning.html#model-evaluation-and-performance-comparison",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "4. Model Evaluation and Performance Comparison",
    "text": "4. Model Evaluation and Performance Comparison\nComparing the performance with using R2 and Root Mean Squared Error (RMSE).\n\n4.1. E. Linear Regression (LR)\n\n# 获取回归模型的参数\nintercept = model_lr.intercept_  # 截距\ncoefficients = model_lr.coef_     # 系数，一个数组，每个元素对应一个自变量的系数\n\nprint(\"截距:\", intercept)\nprint(\"系数:\", coefficients)\n\n截距: 45.905462184873954\n系数: [ 5.32238802 -2.01864622  1.53505427  2.1144597   1.69548912 -1.08028928]\n\n\n\n# 构建模型表达式 Building intercept\nexpression = f\"Y = {intercept}\"\nfor i, (coef, var) in enumerate(zip(coefficients, X_columns)):\n    expression += f\" + {coef} * {var}\"\nprint(expression)\n\nY = 45.905462184873954 + 5.322388021624736 * ET + -2.018646216101594 * LST_NIGHT + 1.535054270985625 * NDVI + 2.114459697962808 * PAR + 1.6954891221007127 * SAR + -1.080289281473687 * Soil_Moisture_pm\n\n\n\ntrain_predictions = model_lr.predict(X_train)\ntest_predictions = model_lr.predict(X_test)\n\nr2_train_lr = r2_score(y_train, train_predictions)\nr2_test_lr = r2_score(y_test, test_predictions)\n\nrmse_train_lr = mean_squared_error(y_train, train_predictions, squared=False)\nrmse_test_lr = mean_squared_error(y_test, test_predictions, squared=False)\n\nprint(f\"Training R^2: {r2_train_lr:.4f}\")\nprint(f\"Test R^2: {r2_test_lr:.4f}\")\nprint(f\"Training RMSE: {rmse_train_lr:.4f}\")\nprint(f\"Test RMSE: {rmse_test_lr:.4f}\")\n\nTraining R^2: 0.7082\nTest R^2: 0.7042\nTraining RMSE: 5.5595\nTest RMSE: 5.2984\n\n\n\n\n4.2. E. Random Forest Regressor (RF)\n\nr2_train_rf = rf_final.score(X=X_train, y=y_train)\nr2_test_rf = rf_final.score(X=X_test, y=y_test)\n\nprint(\"R2 on the training data:\")\nprint(r2_train_rf)\nprint(\"R2 on the testing data:\")\nprint(r2_test_rf)\n\nR2 on the training data:\n0.9113651707234036\nR2 on the testing data:\n0.7361749954189063\n\n\n\nrmse_train_rf = mean_squared_error(y_train, rf_final.predict(X_train), squared=False)\nrmse_test_rf = mean_squared_error(y_test, rf_final.predict(X_test), squared=False)\n\nprint(\"RMSE on the training data:\")\nprint(rmse_train_rf)\nprint(\"RMSE on the testing data:\")\nprint(rmse_test_rf)\n\nRMSE on the training data:\n3.0640939019361295\nRMSE on the testing data:\n5.003729396744494\n\n\n\n# Calculate and plot the feature importance of the RF model\nimp = rfpimp.importances(rf_final, X_test, y_test)\nprint(imp)\nviz = rfpimp.plot_importances(imp)\nviz.view()\n\n                                               Importance\nFeature                                                  \nET                                                 0.3649\nNDVI                                               0.1915\nPAR                                                0.0944\nSAR                                                0.0794\nLST_NIGHT                                         -0.0082\nSoil_Moisture_pm                                  -0.0130\n\n\n\n\n\n\n\n4.3. E. Gradient Boosting Regressor (XGB)\n\nmodel_xgb = XGBRegressor(max_depth=gscv_xgb.best_params_['max_depth'], n_estimators=gscv_xgb.best_params_['n_estimators'], random_state=randomState_xgb)\nmodel_xgb.fit(X_train, y_train)\n\nXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=8, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=20, n_jobs=None,\n             num_parallel_tree=None, random_state=125, ...)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.XGBRegressorXGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=8, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             multi_strategy=None, n_estimators=20, n_jobs=None,\n             num_parallel_tree=None, random_state=125, ...)\n\n\n\n# r2_train_xgb, r2_test_xgb, rmse_train_xgb, rmse_test_xgb\nr2_train_xgb = model_xgb.score(X=X_train, y=y_train)\nr2_test_xgb = model_xgb.score(X=X_test, y=y_test)\nrmse_train_xgb = mean_squared_error(y_train, model_xgb.predict(X_train), squared=False)\nrmse_test_xgb = mean_squared_error(y_test, model_xgb.predict(X_test), squared=False)\n\n\nprint(\"R2 on the training data:\")\nprint(r2_train_xgb)\nprint(\"R2 on the testing data:\")\nprint(r2_test_xgb)\n\nR2 on the training data:\n0.9999906705962772\nR2 on the testing data:\n0.6211699630764378\n\n\n\nprint(\"RMSE on the training data:\")\nprint(rmse_train_xgb)\nprint(\"RMSE on the testing data:\")\nprint(rmse_test_xgb)\n\nRMSE on the training data:\n0.020644560160876076\nRMSE on the testing data:\n4.23536556078444\n\n\n\nimp_xgb = rfpimp.importances(model_xgb, X_test, y_test) # permutation\nprint(imp_xgb)\nviz_xgb = rfpimp.plot_importances(imp_xgb)\nviz_xgb.view()\n\n                                               Importance\nFeature                                                  \nPAR                                                0.8349\nSAR                                                0.1441\nNDVI                                               0.1151\nSoil_Moisture_am                                   0.1071\nLST_DAY                                            0.0284\nPrecipitation                                      0.0018\nSoil_Moisture_pm                                  -0.0020\nET                                                -0.0084\nLST_NIGHT                                         -0.0136\n\n\n\n\n\n\n\n4.4. E. Artificial Neural Network (ANN)\n\n#predictions\ny_pred_train_ann = model_ann.predict(X_train).flatten()\ny_pred_test_ann = model_ann.predict(X_test).flatten()\n\n#Compute R2 and RMSE\nr2_train_ann = np.round(r2_score(y_train, y_pred_train_ann),2)\nr2_test_ann = np.round(r2_score(y_test, y_pred_test_ann),2)\nrmse_train_ann = np.round(np.sqrt(mean_squared_error(y_train, y_pred_train_ann)),2)\nrmse_test_ann = np.round(np.sqrt(mean_squared_error(y_test, y_pred_test_ann)),2)\n\n#print the result\nprint(\"Train R2:\", r2_train_ann)\nprint(\"Test R2:\", r2_test_ann)\nprint(\"Train RMSE:\", rmse_train_ann)\nprint(\"Test RMSE:\", rmse_test_ann)\n\n8/8 ━━━━━━━━━━━━━━━━━━━━ 0s 737us/step\n3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \nTrain R2: 0.34\nTest R2: 0.32\nTrain RMSE: 5.51\nTest RMSE: 5.69\n\n\n\n#crosscheck the y value between real and predicted\ncrosscheck_y_dict = {\n    'y_test' : y_test,\n    'y_pred' : np.round(y_pred_test_ann,0),\n    'delta' : np.abs(np.round((y_test - y_pred_test_ann),0))\n}\n\n#plotting histogram\ncrosscheck_y_df = pd.DataFrame(crosscheck_y_dict)\nplt.hist(crosscheck_y_df['delta'], bins=10)\nplt.xlabel(f'Delta\\nR2_test = {r2_test_ann}, RMSE_test = {rmse_test_ann}, delta_max = {crosscheck_y_df.delta.max()}')\nplt.ylabel('Frequency (Number of Data)')\nplt.title(f\"Accuracy of ANN Model Based on Delta of Y Test and Y Pred)\")\nplt.show()\n\n\n\n\n\n\n4.5. Model Performance Comparison\n\n#please input your metrics in here\nmetrics_dict = {\n    'metrics': [\"Train R2\",\"Test R2\",\"Train RMSE\",\"Test RMSE\"],\n    'LR': [r2_train_lr, r2_test_lr, rmse_train_lr, rmse_test_lr],\n    'RF': [r2_train_rf, r2_test_rf, rmse_train_rf, rmse_test_rf],\n#    'XGB': [r2_train_xgb, r2_test_xgb, rmse_train_xgb, rmse_test_xgb],\n#    'ANN': [r2_train_ann, r2_test_ann, rmse_train_ann, rmse_test_ann]\n}\n\n#create dataframe\nmetrics_df = pd.DataFrame(metrics_dict)\nmetrics_df.set_index('metrics')\n\n\n\n\n\n\n\n\nLR\nRF\n\n\nmetrics\n\n\n\n\n\n\nTrain R2\n0.7082\n0.9114\n\n\nTest R2\n0.7042\n0.7362\n\n\nTrain RMSE\n5.5595\n3.0641\n\n\nTest RMSE\n5.2984\n5.0037\n\n\n\n\n\n\n\nBased on the comparison, it can be said that the best model that can be used for estimating crop yield is []. From this point, [] model would be used for estimation phase (step 5)."
  },
  {
    "objectID": "Modeling/Wheat2_supervised_learning.html#crop-yield-estimation-and-export-result",
    "href": "Modeling/Wheat2_supervised_learning.html#crop-yield-estimation-and-export-result",
    "title": "Big Data Application: Modeling for Crop Yield Estimation",
    "section": "5. Crop Yield Estimation and Export Result",
    "text": "5. Crop Yield Estimation and Export Result"
  },
  {
    "objectID": "other/sarimax.html",
    "href": "other/sarimax.html",
    "title": "1. Install & Load Packages",
    "section": "",
    "text": "pip install pandas-datareader\n\nRequirement already satisfied: pandas-datareader in /opt/conda/lib/python3.11/site-packages (0.10.0)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.11/site-packages (from pandas-datareader) (4.9.3)\nRequirement already satisfied: pandas&gt;=0.23 in /opt/conda/lib/python3.11/site-packages (from pandas-datareader) (2.1.0)\nRequirement already satisfied: requests&gt;=2.19.0 in /opt/conda/lib/python3.11/site-packages (from pandas-datareader) (2.31.0)\nRequirement already satisfied: numpy&gt;=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas&gt;=0.23-&gt;pandas-datareader) (1.24.4)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas&gt;=0.23-&gt;pandas-datareader) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas&gt;=0.23-&gt;pandas-datareader) (2023.3.post1)\nRequirement already satisfied: tzdata&gt;=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas&gt;=0.23-&gt;pandas-datareader) (2023.3)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/conda/lib/python3.11/site-packages (from requests&gt;=2.19.0-&gt;pandas-datareader) (3.2.0)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.11/site-packages (from requests&gt;=2.19.0-&gt;pandas-datareader) (3.4)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests&gt;=2.19.0-&gt;pandas-datareader) (2.0.4)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests&gt;=2.19.0-&gt;pandas-datareader) (2023.7.22)\nRequirement already satisfied: six&gt;=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas&gt;=0.23-&gt;pandas-datareader) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\nimport numpy as np\nimport pandas_datareader.data as web\nimport pandas_datareader as web \n\nimport pandas as pd \npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nimport datetime\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.arima.model import ARIMA\n\nfrom sklearn.metrics import mean_squared_error\n\nimport re"
  },
  {
    "objectID": "other/sarimax.html#load-data",
    "href": "other/sarimax.html#load-data",
    "title": "1. Install & Load Packages",
    "section": "2. Load Data",
    "text": "2. Load Data\n\nnp.random.seed(0)\ndate_rng = pd.date_range(start=\"2021-01-01\", periods=365, freq=\"D\")\ndata = np.sin(np.arange(365) * 2 * np.pi / 365) + np.random.normal(0, 0.5, 365)\nbtc = pd.DataFrame({'Date':date_rng,'BTC-USD':data})\n\nbtc.Date = pd.to_datetime(btc['Date'], format='%Y-%m-%d')\nbtc.set_index('Date', inplace=True)\nbtc.tail()\n\n\n\n\n\n\n\n\nBTC-USD\n\n\nDate\n\n\n\n\n\n2021-12-27\n0.263264\n\n\n2021-12-28\n-0.066917\n\n\n2021-12-29\n0.414305\n\n\n2021-12-30\n0.135561\n\n\n2021-12-31\n-0.025054\n\n\n\n\n\n\n\n\ndef extract_number(text):\n    # Regex pattern to match digits\n    pattern = r'\\d+'\n    # Find all sequences of digits in the text\n    matches = re.findall(pattern, text)\n    if matches:\n        # Return the first matched sequence of digits as an integer\n        return int(matches[0])\n    else:\n        # Return None if no digits are found\n        return None\n\ndef cleaning_data(df):\n\n    #drop geoid column\n    if 'GEOID' in df.columns.to_list():\n        df.drop(columns='GEOID', inplace=True)\n    #transpose data\n    df = df.T\n    #set first row as column name\n    new_columns = df[df.index == 'NAME'].iloc[0].to_list()\n    df.columns = new_columns\n    df = df[1:]\n    #datetime index\n    df.reset_index(inplace=True)\n    df['index'] = df['index'].apply(extract_number)\n    df['index'] = pd.to_datetime(df['index'], format='%Y')\n    df.set_index('index', inplace=True)\n    #interpolate missing value\n    for column in df.columns.to_list():\n        df[column] = pd.to_numeric(df[column], errors='coerce')\n        df[column] = df[column].interpolate()\n        df[column] = df[column].bfill()\n    \n    return(df)\n\n\n#soybean\nET_soybean = cleaning_data(pd.read_csv('data/soybean_past/2000-2023_Soybean_ET.csv'))\nLST_DAY_soybean = cleaning_data(pd.read_csv('data/soybean_past/2000-2023_Soybean_LST_DAY.csv'))\nLST_NIGHT_soybean = cleaning_data(pd.read_csv('data/soybean_past/2000-2023_Soybean_LST_NIGHT.csv'))\nNDVI_soybean = cleaning_data(pd.read_csv('data/soybean_past/2000-2023_Soybean_NDVI.csv'))\nPA_soybean = cleaning_data(pd.read_csv('data/soybean_past/2000-2023_Soybean_PA.csv'))\nPAR_soybean = cleaning_data(pd.read_csv('data/soybean_past/2000-2023_Soybean_PAR.csv'))\n\n#corn\n@ET_corn = cleaning_data(pd.read_csv('data/corn_past/ET_2000_2023.csv', sep=';'))\n@LST_DAY_corn = cleaning_data(pd.read_csv('data/corn_past/LST_DAY_2000_2023.csv', sep=';'))\nLST_NIGHT_corn = cleaning_data(pd.read_csv('data/corn_past/LST_NIGHT_2000_2023.csv', sep=';'))\n@NDVI_corn = cleaning_data(pd.read_csv('data/corn_past/NDVI_2000_2023.csv', sep=';'))\n@PA_corn = cleaning_data(pd.read_csv('data/corn_past/PA_2000_2023.csv', sep=';'))\n@PAR_corn = cleaning_data(pd.read_csv('data/corn_past/PAR_2000_2023.csv', sep=';'))\n\n#wheat\nET_wheat = cleaning_data(pd.read_csv('data/wheat_past/wheat_ET_2000-2023.csv'))\nSAR_wheat = cleaning_data(pd.read_csv('data/wheat_past/SAR_wheat_2015-2023.csv'))\nLST_NIGHT_corn = cleaning_data(pd.read_csv('data/wheat_past/wheat_LST_NIGHT_2000-2023.csv'))\n@NDVI_wheat = cleaning_data(pd.read_csv('data/wheat_past/wheat_NDVI_2000-2023.csv', sep=';'))\n@PAR_wheat = cleaning_data(pd.read_csv('data/wheat_past/wheat_PAR_2000-2023.csv', sep=';'))\nSM_wheat = cleaning_data(pd.read_csv('data/wheat_past/wheat_Soil_Moisture_pm_2015-2023.csv'))\n\n\nSM_wheat\n\n\n\n\n\n\n\n\nGrand Forks\nStutsman\nSioux\nMorton\nBurleigh\nOliver\nWard\nMcHenry\nRenville\nWilliams\nStark\nCass\nRichland\nPembina\nWalsh\nCavalier\nTowner\nRamsey\nNelson\nBenson\nEddy\nGriggs\nSteele\nFoster\nBarnes\nTraill\nSargent\nDickey\nLaMoure\nRansom\nLogan\nKidder\nMcIntosh\nGrant\nEmmons\nMcLean\nMercer\nMountrail\nSheridan\nWells\nPierce\nRolette\nBottineau\nBurke\nDivide\nMcKenzie\nBillings\nGolden Valley\nSlope\nDunn\nHettinger\nAdams\nBowman\n\n\nindex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2015-01-01\n0.205381\n0.252169\n0.174487\n0.183757\n0.221467\n0.192562\n0.238125\n0.228823\n0.218186\n0.161946\n0.163834\n0.224758\n0.222814\n0.199302\n0.209958\n0.182190\n0.220745\n0.261063\n0.251781\n0.259323\n0.223471\n0.234915\n0.202663\n0.227061\n0.234029\n0.197104\n0.242153\n0.254832\n0.241557\n0.233779\n0.238518\n0.258835\n0.231876\n0.169998\n0.195962\n0.220506\n0.190393\n0.222586\n0.244408\n0.203051\n0.244598\n0.235718\n0.225359\n0.232348\n0.225829\n0.151948\n0.159520\n0.159178\n0.166768\n0.169361\n0.168307\n0.165660\n0.168246\n\n\n2016-01-01\n0.216213\n0.233538\n0.145552\n0.161847\n0.209432\n0.172285\n0.204179\n0.199692\n0.182478\n0.161655\n0.142695\n0.171291\n0.162570\n0.235387\n0.236387\n0.209884\n0.228881\n0.258773\n0.250904\n0.256260\n0.219017\n0.225074\n0.196714\n0.208957\n0.210012\n0.180465\n0.209384\n0.221897\n0.215053\n0.190601\n0.233203\n0.248691\n0.225697\n0.142984\n0.185105\n0.197218\n0.168046\n0.198535\n0.225572\n0.197028\n0.234464\n0.236937\n0.188186\n0.200125\n0.204131\n0.140150\n0.147295\n0.141620\n0.136223\n0.153233\n0.140096\n0.133475\n0.132127\n\n\n2017-01-01\n0.196929\n0.235005\n0.140892\n0.147710\n0.195112\n0.162360\n0.193209\n0.187966\n0.183952\n0.138931\n0.125244\n0.163523\n0.192244\n0.177577\n0.194714\n0.186716\n0.213546\n0.245824\n0.240626\n0.252239\n0.224749\n0.228885\n0.199382\n0.222587\n0.203115\n0.177715\n0.219814\n0.221354\n0.207342\n0.198926\n0.222814\n0.243320\n0.214944\n0.128169\n0.173213\n0.184112\n0.148952\n0.195711\n0.218977\n0.198677\n0.229699\n0.222589\n0.188696\n0.198148\n0.190628\n0.127793\n0.130493\n0.113619\n0.116320\n0.135578\n0.118863\n0.112913\n0.112343\n\n\n2018-01-01\n0.187333\n0.238482\n0.151179\n0.162236\n0.202188\n0.166268\n0.185987\n0.180483\n0.166166\n0.161120\n0.141159\n0.177763\n0.194443\n0.146629\n0.168423\n0.165187\n0.201908\n0.230770\n0.225790\n0.242643\n0.210664\n0.218074\n0.187029\n0.207131\n0.222344\n0.180522\n0.227289\n0.215639\n0.227474\n0.211065\n0.240594\n0.247382\n0.227381\n0.147973\n0.185499\n0.185833\n0.163022\n0.193133\n0.220014\n0.190185\n0.222059\n0.215903\n0.168163\n0.194575\n0.187110\n0.152025\n0.140246\n0.135927\n0.148148\n0.141502\n0.145283\n0.142380\n0.146272\n\n\n2019-01-01\n0.217961\n0.279251\n0.195985\n0.186320\n0.241471\n0.194319\n0.205472\n0.193700\n0.178742\n0.173677\n0.169514\n0.231440\n0.232625\n0.159179\n0.191869\n0.174575\n0.203142\n0.241443\n0.254050\n0.255625\n0.242857\n0.248429\n0.230256\n0.239069\n0.269159\n0.227290\n0.290850\n0.290975\n0.286314\n0.264371\n0.266908\n0.276793\n0.262786\n0.171315\n0.214540\n0.204795\n0.184525\n0.212621\n0.241831\n0.223090\n0.229762\n0.225639\n0.179972\n0.203169\n0.192957\n0.168855\n0.169544\n0.161120\n0.173497\n0.171316\n0.170185\n0.168424\n0.173513\n\n\n2020-01-01\n0.241001\n0.288233\n0.151126\n0.152594\n0.217620\n0.163988\n0.227066\n0.219878\n0.215900\n0.152422\n0.143089\n0.240212\n0.218606\n0.213278\n0.226400\n0.211715\n0.248892\n0.284066\n0.274638\n0.281626\n0.275509\n0.270065\n0.244771\n0.272237\n0.275136\n0.249989\n0.265737\n0.269059\n0.273647\n0.255348\n0.239854\n0.284220\n0.243519\n0.138610\n0.193423\n0.203675\n0.156840\n0.211752\n0.260285\n0.247545\n0.250165\n0.253327\n0.225551\n0.198216\n0.192693\n0.138087\n0.144796\n0.142716\n0.146117\n0.146412\n0.142604\n0.137919\n0.143397\n\n\n2021-01-01\n0.152929\n0.231011\n0.151047\n0.158035\n0.193603\n0.162295\n0.186452\n0.173300\n0.166952\n0.140497\n0.153121\n0.128771\n0.147924\n0.140715\n0.154793\n0.163363\n0.198045\n0.216577\n0.209084\n0.237769\n0.213039\n0.189615\n0.154035\n0.188228\n0.187183\n0.124738\n0.198310\n0.218281\n0.220218\n0.178269\n0.248394\n0.257610\n0.214204\n0.150019\n0.177524\n0.169424\n0.154762\n0.182224\n0.220193\n0.182625\n0.210987\n0.205730\n0.161963\n0.184832\n0.166371\n0.135477\n0.150654\n0.126582\n0.137404\n0.152653\n0.152088\n0.141709\n0.132438\n\n\n2022-01-01\n0.261477\n0.292537\n0.162171\n0.172424\n0.206937\n0.177715\n0.240735\n0.229794\n0.258407\n0.206174\n0.185571\n0.241574\n0.233784\n0.272727\n0.270367\n0.282639\n0.295651\n0.331111\n0.304845\n0.301769\n0.285294\n0.281688\n0.254882\n0.282363\n0.287276\n0.239680\n0.279250\n0.276114\n0.288545\n0.269544\n0.258079\n0.280615\n0.251201\n0.168677\n0.196946\n0.199232\n0.173212\n0.229237\n0.255886\n0.255334\n0.264846\n0.275625\n0.248346\n0.268372\n0.254623\n0.174364\n0.189267\n0.178125\n0.176581\n0.178941\n0.184801\n0.171303\n0.164511\n\n\n2023-01-01\n0.175247\n0.264256\n0.179389\n0.188574\n0.229116\n0.192328\n0.215875\n0.208189\n0.198459\n0.166153\n0.173851\n0.179068\n0.169750\n0.169678\n0.176206\n0.167370\n0.202217\n0.228034\n0.231671\n0.245143\n0.251683\n0.225297\n0.189477\n0.237667\n0.230838\n0.167154\n0.237149\n0.258243\n0.246088\n0.231897\n0.264068\n0.280139\n0.246502\n0.176363\n0.199494\n0.198471\n0.174316\n0.202618\n0.245731\n0.227116\n0.222186\n0.209327\n0.192197\n0.207546\n0.199185\n0.148763\n0.169245\n0.165951\n0.173180\n0.170094\n0.175873\n0.167824\n0.168062\n\n\n\n\n\n\n\n\nsns.set()\n\nplt.plot(btc.index, btc['BTC-USD'], )\n\nplt.ylabel('BTC Price')\nplt.xlabel('Date')\nplt.xticks(rotation=45)\n\n(array([18628., 18687., 18748., 18809., 18871., 18932., 18993.]),\n [Text(18628.0, 0, '2021-01'),\n  Text(18687.0, 0, '2021-03'),\n  Text(18748.0, 0, '2021-05'),\n  Text(18809.0, 0, '2021-07'),\n  Text(18871.0, 0, '2021-09'),\n  Text(18932.0, 0, '2021-11'),\n  Text(18993.0, 0, '2022-01')])\n\n\n\n\n\n\ntrain = btc[btc.index &lt; pd.to_datetime(\"2021-11-01\", format='%Y-%m-%d')]\ntest = btc[btc.index &gt; pd.to_datetime(\"2021-11-01\", format='%Y-%m-%d')]\n\nplt.plot(train, color = \"black\")\nplt.plot(test, color = \"red\")\nplt.ylabel('BTC Price')\nplt.xlabel('Date')\nplt.xticks(rotation=45)\nplt.title(\"Train/Test split for BTC Data\")\nplt.show()\n\n\n\n\n\ny = train['BTC-USD']"
  },
  {
    "objectID": "other/sarimax.html#modeling-sarima",
    "href": "other/sarimax.html#modeling-sarima",
    "title": "1. Install & Load Packages",
    "section": "3. Modeling SARIMA",
    "text": "3. Modeling SARIMA\n\nSARIMAXmodel = SARIMAX(y, order = (2, 2, 2), seasonal_order=(2,2,2,12))\nSARIMAXmodel = SARIMAXmodel.fit()\n\n/opt/conda/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\n/opt/conda/lib/python3.11/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n  self._init_dates(dates, freq)\n/opt/conda/lib/python3.11/site-packages/statsmodels/tsa/statespace/sarimax.py:978: UserWarning: Non-invertible starting MA parameters found. Using zeros as starting parameters.\n  warn('Non-invertible starting MA parameters found.'\n/opt/conda/lib/python3.11/site-packages/statsmodels/tsa/statespace/sarimax.py:1009: UserWarning: Non-invertible starting seasonal moving average Using zeros as starting parameters.\n  warn('Non-invertible starting seasonal moving average'\n This problem is unconstrained.\n\n\nRUNNING THE L-BFGS-B CODE\n\n           * * *\n\nMachine precision = 2.220D-16\n N =            9     M =           10\n\nAt X0         0 variables are exactly at the bounds\n\nAt iterate    0    f=  1.59530D+00    |proj g|=  2.22140D-01\n\nAt iterate    5    f=  1.11840D+00    |proj g|=  4.60017D-02\n\nAt iterate   10    f=  1.10869D+00    |proj g|=  5.37039D-03\n\nAt iterate   15    f=  1.06654D+00    |proj g|=  2.30925D-01\n\nAt iterate   20    f=  9.83098D-01    |proj g|=  7.22321D-02\n\nAt iterate   25    f=  9.73724D-01    |proj g|=  2.16433D-03\n\nAt iterate   30    f=  9.71063D-01    |proj g|=  2.47353D-02\n\nAt iterate   35    f=  9.68698D-01    |proj g|=  3.31404D-03\n\nAt iterate   40    f=  9.68664D-01    |proj g|=  1.17843D-03\n\nAt iterate   45    f=  9.68600D-01    |proj g|=  2.66174D-03\n\n           * * *\n\nTit   = total number of iterations\nTnf   = total number of function evaluations\nTnint = total number of segments explored during Cauchy searches\nSkip  = number of BFGS updates skipped\nNact  = number of active bounds at final generalized Cauchy point\nProjg = norm of the final projected gradient\nF     = final function value\n\n           * * *\n\n   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n    9     49     68      1     0     0   5.366D-05   9.686D-01\n  F =  0.96859331650655422     \n\nCONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH             \n\n\n\nlen(test.index)\n\n60\n\n\n\ny_pred3 = SARIMAXmodel.get_forecast(len(test.index))\ny_pred3\n\n&lt;statsmodels.tsa.statespace.mlemodel.PredictionResultsWrapper at 0x7fcfacc30f90&gt;\n\n\n\ny_pred_df3 = y_pred3.conf_int(alpha = 0.05) \ny_pred_df3\n\n\n\n\n\n\n\n\nlower BTC-USD\nupper BTC-USD\n\n\n\n\n2021-11-01\n-1.342926\n1.127470\n\n\n2021-11-02\n-2.142490\n0.328053\n\n\n2021-11-03\n-2.033792\n0.444090\n\n\n2021-11-04\n-1.270100\n1.216526\n\n\n2021-11-05\n-1.855000\n0.640524\n\n\n2021-11-06\n-1.719218\n0.785582\n\n\n2021-11-07\n-2.283207\n0.231254\n\n\n2021-11-08\n-1.671446\n0.853104\n\n\n2021-11-09\n-2.113231\n0.416338\n\n\n2021-11-10\n-1.222144\n1.318303\n\n\n2021-11-11\n-2.390734\n0.160999\n\n\n2021-11-12\n-2.367081\n0.196349\n\n\n2021-11-13\n-1.294671\n1.733468\n\n\n2021-11-14\n-2.229487\n0.810624\n\n\n2021-11-15\n-2.378566\n0.683290\n\n\n2021-11-16\n-1.379682\n1.705530\n\n\n2021-11-17\n-1.954631\n1.154503\n\n\n2021-11-18\n-1.765427\n1.368371\n\n\n2021-11-19\n-2.463844\n0.695366\n\n\n2021-11-20\n-1.736218\n1.449228\n\n\n2021-11-21\n-2.553456\n0.655177\n\n\n2021-11-22\n-1.203308\n2.033052\n\n\n2021-11-23\n-2.156165\n1.108664\n\n\n2021-11-24\n-2.456826\n0.837213\n\n\n2021-11-25\n-1.518644\n2.529125\n\n\n2021-11-26\n-2.680510\n1.394892\n\n\n2021-11-27\n-2.368980\n1.751415\n\n\n2021-11-28\n-1.623173\n2.544870\n\n\n2021-11-29\n-2.225126\n1.991394\n\n\n2021-11-30\n-2.135272\n2.130847\n\n\n2021-12-01\n-2.681707\n1.635146\n\n\n2021-12-02\n-2.195978\n2.172994\n\n\n2021-12-03\n-2.778615\n1.629839\n\n\n2021-12-04\n-1.607311\n2.855551\n\n\n2021-12-05\n-2.903352\n1.615006\n\n\n2021-12-06\n-2.967177\n1.607751\n\n\n2021-12-07\n-1.969471\n3.302299\n\n\n2021-12-08\n-2.709343\n2.617004\n\n\n2021-12-09\n-3.053541\n2.347764\n\n\n2021-12-10\n-1.916017\n3.563334\n\n\n2021-12-11\n-2.544451\n3.013986\n\n\n2021-12-12\n-2.427976\n3.210904\n\n\n2021-12-13\n-3.224669\n2.496021\n\n\n2021-12-14\n-2.413794\n3.390364\n\n\n2021-12-15\n-3.268955\n2.608631\n\n\n2021-12-16\n-1.902404\n4.061344\n\n\n2021-12-17\n-2.995500\n3.055681\n\n\n2021-12-18\n-3.332879\n2.806986\n\n\n2021-12-19\n-2.213153\n4.574876\n\n\n2021-12-20\n-3.445779\n3.429153\n\n\n2021-12-21\n-3.192088\n3.791343\n\n\n2021-12-22\n-2.362135\n4.733033\n\n\n2021-12-23\n-3.006773\n4.201262\n\n\n2021-12-24\n-2.976848\n4.345501\n\n\n2021-12-25\n-3.591097\n3.847047\n\n\n2021-12-26\n-3.107659\n4.448334\n\n\n2021-12-27\n-3.816747\n3.837376\n\n\n2021-12-28\n-2.479058\n5.295918\n\n\n2021-12-29\n-3.886206\n4.010982\n\n\n2021-12-30\n-4.047953\n3.972782\n\n\n\n\n\n\n\n\ny_pred_df3[\"Predictions\"] = SARIMAXmodel.predict(start = y_pred_df3.index[0], end = y_pred_df3.index[-1])\ny_pred_df3\n\n\n\n\n\n\n\n\nlower BTC-USD\nupper BTC-USD\nPredictions\n\n\n\n\n2021-11-01\n-1.342926\n1.127470\n-0.107728\n\n\n2021-11-02\n-2.142490\n0.328053\n-0.907219\n\n\n2021-11-03\n-2.033792\n0.444090\n-0.794851\n\n\n2021-11-04\n-1.270100\n1.216526\n-0.026787\n\n\n2021-11-05\n-1.855000\n0.640524\n-0.607238\n\n\n2021-11-06\n-1.719218\n0.785582\n-0.466818\n\n\n2021-11-07\n-2.283207\n0.231254\n-1.025977\n\n\n2021-11-08\n-1.671446\n0.853104\n-0.409171\n\n\n2021-11-09\n-2.113231\n0.416338\n-0.848447\n\n\n2021-11-10\n-1.222144\n1.318303\n0.048079\n\n\n2021-11-11\n-2.390734\n0.160999\n-1.114868\n\n\n2021-11-12\n-2.367081\n0.196349\n-1.085366\n\n\n2021-11-13\n-1.294671\n1.733468\n0.219399\n\n\n2021-11-14\n-2.229487\n0.810624\n-0.709432\n\n\n2021-11-15\n-2.378566\n0.683290\n-0.847638\n\n\n2021-11-16\n-1.379682\n1.705530\n0.162924\n\n\n2021-11-17\n-1.954631\n1.154503\n-0.400064\n\n\n2021-11-18\n-1.765427\n1.368371\n-0.198528\n\n\n2021-11-19\n-2.463844\n0.695366\n-0.884239\n\n\n2021-11-20\n-1.736218\n1.449228\n-0.143495\n\n\n2021-11-21\n-2.553456\n0.655177\n-0.949140\n\n\n2021-11-22\n-1.203308\n2.033052\n0.414872\n\n\n2021-11-23\n-2.156165\n1.108664\n-0.523750\n\n\n2021-11-24\n-2.456826\n0.837213\n-0.809807\n\n\n2021-11-25\n-1.518644\n2.529125\n0.505241\n\n\n2021-11-26\n-2.680510\n1.394892\n-0.642809\n\n\n2021-11-27\n-2.368980\n1.751415\n-0.308782\n\n\n2021-11-28\n-1.623173\n2.544870\n0.460849\n\n\n2021-11-29\n-2.225126\n1.991394\n-0.116866\n\n\n2021-11-30\n-2.135272\n2.130847\n-0.002213\n\n\n2021-12-01\n-2.681707\n1.635146\n-0.523280\n\n\n2021-12-02\n-2.195978\n2.172994\n-0.011492\n\n\n2021-12-03\n-2.778615\n1.629839\n-0.574388\n\n\n2021-12-04\n-1.607311\n2.855551\n0.624120\n\n\n2021-12-05\n-2.903352\n1.615006\n-0.644173\n\n\n2021-12-06\n-2.967177\n1.607751\n-0.679713\n\n\n2021-12-07\n-1.969471\n3.302299\n0.666414\n\n\n2021-12-08\n-2.709343\n2.617004\n-0.046169\n\n\n2021-12-09\n-3.053541\n2.347764\n-0.352888\n\n\n2021-12-10\n-1.916017\n3.563334\n0.823659\n\n\n2021-12-11\n-2.544451\n3.013986\n0.234767\n\n\n2021-12-12\n-2.427976\n3.210904\n0.391464\n\n\n2021-12-13\n-3.224669\n2.496021\n-0.364324\n\n\n2021-12-14\n-2.413794\n3.390364\n0.488285\n\n\n2021-12-15\n-3.268955\n2.608631\n-0.330162\n\n\n2021-12-16\n-1.902404\n4.061344\n1.079470\n\n\n2021-12-17\n-2.995500\n3.055681\n0.030090\n\n\n2021-12-18\n-3.332879\n2.806986\n-0.262947\n\n\n2021-12-19\n-2.213153\n4.574876\n1.180862\n\n\n2021-12-20\n-3.445779\n3.429153\n-0.008313\n\n\n2021-12-21\n-3.192088\n3.791343\n0.299627\n\n\n2021-12-22\n-2.362135\n4.733033\n1.185449\n\n\n2021-12-23\n-3.006773\n4.201262\n0.597245\n\n\n2021-12-24\n-2.976848\n4.345501\n0.684326\n\n\n2021-12-25\n-3.591097\n3.847047\n0.127975\n\n\n2021-12-26\n-3.107659\n4.448334\n0.670337\n\n\n2021-12-27\n-3.816747\n3.837376\n0.010314\n\n\n2021-12-28\n-2.479058\n5.295918\n1.408430\n\n\n2021-12-29\n-3.886206\n4.010982\n0.062388\n\n\n2021-12-30\n-4.047953\n3.972782\n-0.037585\n\n\n\n\n\n\n\n\ny_pred_df3.index = test.index\ny_pred_df3\n\n\n\n\n\n\n\n\nlower BTC-USD\nupper BTC-USD\nPredictions\n\n\nDate\n\n\n\n\n\n\n\n2021-11-02\n-1.342926\n1.127470\n-0.107728\n\n\n2021-11-03\n-2.142490\n0.328053\n-0.907219\n\n\n2021-11-04\n-2.033792\n0.444090\n-0.794851\n\n\n2021-11-05\n-1.270100\n1.216526\n-0.026787\n\n\n2021-11-06\n-1.855000\n0.640524\n-0.607238\n\n\n2021-11-07\n-1.719218\n0.785582\n-0.466818\n\n\n2021-11-08\n-2.283207\n0.231254\n-1.025977\n\n\n2021-11-09\n-1.671446\n0.853104\n-0.409171\n\n\n2021-11-10\n-2.113231\n0.416338\n-0.848447\n\n\n2021-11-11\n-1.222144\n1.318303\n0.048079\n\n\n2021-11-12\n-2.390734\n0.160999\n-1.114868\n\n\n2021-11-13\n-2.367081\n0.196349\n-1.085366\n\n\n2021-11-14\n-1.294671\n1.733468\n0.219399\n\n\n2021-11-15\n-2.229487\n0.810624\n-0.709432\n\n\n2021-11-16\n-2.378566\n0.683290\n-0.847638\n\n\n2021-11-17\n-1.379682\n1.705530\n0.162924\n\n\n2021-11-18\n-1.954631\n1.154503\n-0.400064\n\n\n2021-11-19\n-1.765427\n1.368371\n-0.198528\n\n\n2021-11-20\n-2.463844\n0.695366\n-0.884239\n\n\n2021-11-21\n-1.736218\n1.449228\n-0.143495\n\n\n2021-11-22\n-2.553456\n0.655177\n-0.949140\n\n\n2021-11-23\n-1.203308\n2.033052\n0.414872\n\n\n2021-11-24\n-2.156165\n1.108664\n-0.523750\n\n\n2021-11-25\n-2.456826\n0.837213\n-0.809807\n\n\n2021-11-26\n-1.518644\n2.529125\n0.505241\n\n\n2021-11-27\n-2.680510\n1.394892\n-0.642809\n\n\n2021-11-28\n-2.368980\n1.751415\n-0.308782\n\n\n2021-11-29\n-1.623173\n2.544870\n0.460849\n\n\n2021-11-30\n-2.225126\n1.991394\n-0.116866\n\n\n2021-12-01\n-2.135272\n2.130847\n-0.002213\n\n\n2021-12-02\n-2.681707\n1.635146\n-0.523280\n\n\n2021-12-03\n-2.195978\n2.172994\n-0.011492\n\n\n2021-12-04\n-2.778615\n1.629839\n-0.574388\n\n\n2021-12-05\n-1.607311\n2.855551\n0.624120\n\n\n2021-12-06\n-2.903352\n1.615006\n-0.644173\n\n\n2021-12-07\n-2.967177\n1.607751\n-0.679713\n\n\n2021-12-08\n-1.969471\n3.302299\n0.666414\n\n\n2021-12-09\n-2.709343\n2.617004\n-0.046169\n\n\n2021-12-10\n-3.053541\n2.347764\n-0.352888\n\n\n2021-12-11\n-1.916017\n3.563334\n0.823659\n\n\n2021-12-12\n-2.544451\n3.013986\n0.234767\n\n\n2021-12-13\n-2.427976\n3.210904\n0.391464\n\n\n2021-12-14\n-3.224669\n2.496021\n-0.364324\n\n\n2021-12-15\n-2.413794\n3.390364\n0.488285\n\n\n2021-12-16\n-3.268955\n2.608631\n-0.330162\n\n\n2021-12-17\n-1.902404\n4.061344\n1.079470\n\n\n2021-12-18\n-2.995500\n3.055681\n0.030090\n\n\n2021-12-19\n-3.332879\n2.806986\n-0.262947\n\n\n2021-12-20\n-2.213153\n4.574876\n1.180862\n\n\n2021-12-21\n-3.445779\n3.429153\n-0.008313\n\n\n2021-12-22\n-3.192088\n3.791343\n0.299627\n\n\n2021-12-23\n-2.362135\n4.733033\n1.185449\n\n\n2021-12-24\n-3.006773\n4.201262\n0.597245\n\n\n2021-12-25\n-2.976848\n4.345501\n0.684326\n\n\n2021-12-26\n-3.591097\n3.847047\n0.127975\n\n\n2021-12-27\n-3.107659\n4.448334\n0.670337\n\n\n2021-12-28\n-3.816747\n3.837376\n0.010314\n\n\n2021-12-29\n-2.479058\n5.295918\n1.408430\n\n\n2021-12-30\n-3.886206\n4.010982\n0.062388\n\n\n2021-12-31\n-4.047953\n3.972782\n-0.037585\n\n\n\n\n\n\n\n\ny_pred_out3 = y_pred_df3[\"Predictions\"] \ny_pred_out3\n\nDate\n2021-11-02   -0.107728\n2021-11-03   -0.907219\n2021-11-04   -0.794851\n2021-11-05   -0.026787\n2021-11-06   -0.607238\n2021-11-07   -0.466818\n2021-11-08   -1.025977\n2021-11-09   -0.409171\n2021-11-10   -0.848447\n2021-11-11    0.048079\n2021-11-12   -1.114868\n2021-11-13   -1.085366\n2021-11-14    0.219399\n2021-11-15   -0.709432\n2021-11-16   -0.847638\n2021-11-17    0.162924\n2021-11-18   -0.400064\n2021-11-19   -0.198528\n2021-11-20   -0.884239\n2021-11-21   -0.143495\n2021-11-22   -0.949140\n2021-11-23    0.414872\n2021-11-24   -0.523750\n2021-11-25   -0.809807\n2021-11-26    0.505241\n2021-11-27   -0.642809\n2021-11-28   -0.308782\n2021-11-29    0.460849\n2021-11-30   -0.116866\n2021-12-01   -0.002213\n2021-12-02   -0.523280\n2021-12-03   -0.011492\n2021-12-04   -0.574388\n2021-12-05    0.624120\n2021-12-06   -0.644173\n2021-12-07   -0.679713\n2021-12-08    0.666414\n2021-12-09   -0.046169\n2021-12-10   -0.352888\n2021-12-11    0.823659\n2021-12-12    0.234767\n2021-12-13    0.391464\n2021-12-14   -0.364324\n2021-12-15    0.488285\n2021-12-16   -0.330162\n2021-12-17    1.079470\n2021-12-18    0.030090\n2021-12-19   -0.262947\n2021-12-20    1.180862\n2021-12-21   -0.008313\n2021-12-22    0.299627\n2021-12-23    1.185449\n2021-12-24    0.597245\n2021-12-25    0.684326\n2021-12-26    0.127975\n2021-12-27    0.670337\n2021-12-28    0.010314\n2021-12-29    1.408430\n2021-12-30    0.062388\n2021-12-31   -0.037585\nName: Predictions, dtype: float64\n\n\n\nplt.plot(train, color = \"black\")\n#plt.plot(y_pred_out1, color='green', label = 'ARMA')\n#plt.plot(y_pred_out2, color='Yellow', label = 'ARIMA')\nplt.plot(y_pred_out3, color='blue', label = 'SARIMA')\nplt.plot(test, color = \"red\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fcf9df09fd0&gt;\n\n\n\n\n\n\narma_rmse = np.sqrt(mean_squared_error(test[\"BTC-USD\"].values, y_pred_df3[\"Predictions\"]))\nprint(\"RMSE: \",arma_rmse)\n\nRMSE:  0.8804517649257403"
  }
]